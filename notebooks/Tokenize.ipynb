{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Overview<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from keras import backend as K\n",
    "from keras.models import Model \n",
    "from keras.models import Sequential as SequentialModel\n",
    "from keras.layers import Dense, Conv1D, LSTM, Dropout, Embedding, Layer, Input, Flatten, concatenate as Concatenate, Lambda\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "sys.path.insert(0, '../ct')\n",
    "\n",
    "import load\n",
    "from preprocess import preprocess\n",
    "from preprocess import Tokenizer\n",
    "from preprocess.preprocess import separator_samples\n",
    "\n",
    "from model.layers import LayerNormalization\n",
    "from model.layers import ContentBasedAttention_CT\n",
    "from model.layers import ScaledDotProductAttention\n",
    "from model.layers import MultiHeadAttention\n",
    "\n",
    "from model import CompressiveTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = ['..\\\\data\\\\wma-en-de\\\\input\\\\train-en.txt',\n",
    "              '..\\\\data\\\\wma-en-de\\\\input\\\\train-de.txt'\n",
    "              ]\n",
    "tokenizer_output_path = '..\\\\data\\\\wma-en-de\\\\tokenizer\\\\en-de-v0.tok'\n",
    "\n",
    "input_paths_tokenizer = ['..\\\\data\\\\wma-en-de\\\\input\\\\train-en-ascii.txt',\n",
    "                        # '..\\\\data\\\\wma-en-de\\\\input\\\\train-de-ascii.txt'\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_utf_to_ascii(input_path, output_path=None):\n",
    "    if output_path is None:\n",
    "        s = input_path.split('.')\n",
    "        output_path = '.'.join(s[:-1]) + '-ascii.' + s[-1]\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf8') as file:\n",
    "        content = file.read()\n",
    "    content = content.encode('ascii', 'xmlcharrefreplace')\n",
    "    content = content.decode('ascii')\n",
    "    with open(output_path, 'w', encoding='ascii') as file:\n",
    "        file.write(content)\n",
    "    print(f'converted utf->ascii for {input_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted utf->ascii for ..\\data\\wma-en-de\\input\\train-en.txt\n",
      "converted utf->ascii for ..\\data\\wma-en-de\\input\\train-de.txt\n"
     ]
    }
   ],
   "source": [
    "# for p in input_paths:\n",
    "#    file_utf_to_ascii(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(input_paths=input_paths_tokenizer, \n",
    "              tokenizer_output_path=tokenizer_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=2, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.encode_batch(['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Overview",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
