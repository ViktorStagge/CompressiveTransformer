{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Overview<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Spooky-Author-Classification\" data-toc-modified-id=\"Spooky-Author-Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Spooky Author Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-Tokenizer-Input-Data\" data-toc-modified-id=\"Generate-Tokenizer-Input-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Generate Tokenizer Input Data</a></span></li><li><span><a href=\"#Generate-Tokenizer\" data-toc-modified-id=\"Generate-Tokenizer-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Generate Tokenizer</a></span></li><li><span><a href=\"#Tokenize-texts\" data-toc-modified-id=\"Tokenize-texts-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Tokenize texts</a></span></li><li><span><a href=\"#Add-a-Regular-Tokenizer-to-compare\" data-toc-modified-id=\"Add-a-Regular-Tokenizer-to-compare-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Add a Regular Tokenizer to compare</a></span></li></ul></li><li><span><a href=\"#Estimate-a-reasonable-sequence's-max_length\" data-toc-modified-id=\"Estimate-a-reasonable-sequence's-max_length-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Estimate a reasonable sequence's max_length</a></span><ul class=\"toc-item\"><li><span><a href=\"#Enforce-a-sequence's-max_length\" data-toc-modified-id=\"Enforce-a-sequence's-max_length-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Enforce a sequence's max_length</a></span></li><li><span><a href=\"#Create-train-/test-sets\" data-toc-modified-id=\"Create-train-/test-sets-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Create train-/test sets</a></span></li><li><span><a href=\"#Create-Model\" data-toc-modified-id=\"Create-Model-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Create Model</a></span></li><li><span><a href=\"#Fit-Model\" data-toc-modified-id=\"Fit-Model-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Fit Model</a></span></li><li><span><a href=\"#Evaluate-Model\" data-toc-modified-id=\"Evaluate-Model-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Evaluate Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Conv1D, LSTM, Dropout, Embedding, Layer\n",
    "from keras.models import Sequential as SequentialModel\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.insert(0, '../ct')\n",
    "\n",
    "import load\n",
    "from preprocess import preprocess\n",
    "from preprocess import Tokenizer\n",
    "from preprocess.preprocess import separator_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(f'current dir: {os.getcwd()}')\n",
    "# df = pd.read_pickle('../data/processed/treebank.pickle')\n",
    "\n",
    "# if not os.path.exists('../data/processed/treebank'):\n",
    "#     os.mkdir('../data/processed/treebank')\n",
    "\n",
    "# for i, text in enumerate(df.text):\n",
    "#     with open(f'../data/processed/treebank/{i + 1}', 'w') as file:\n",
    "#         file.write(text)\n",
    "        \n",
    "# with open('../data/processed/treebank.txt', 'w', ) as file:\n",
    "#     file.write(text)\n",
    "\n",
    "# tokenizer = Tokenizer(input_paths=['../data/input/treebank/raw_txt/treebank-utf8.txt'],\n",
    "#                       tokens_output_dir='../data/processed/treebank/',\n",
    "#                       tokenizer_output_path='../data/tokenizer/treebank',\n",
    "#                       lowercase=True,\n",
    "#                       vocab_size=5000)\n",
    "# display(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate Tokenizer Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spooky_train = pd.read_csv('../data/input/spooky-author/train.csv')\n",
    "spooky_test = pd.read_csv('../data/input/spooky-author/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>id17718</td>\n",
       "      <td>I could have fancied, while I looked at it, th...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>id08973</td>\n",
       "      <td>The lids clenched themselves together as if in...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>id05267</td>\n",
       "      <td>Mais il faut agir that is to say, a Frenchman ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>id17513</td>\n",
       "      <td>For an item of news like this, it strikes us i...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>id00393</td>\n",
       "      <td>He laid a gnarled claw on my shoulder, and it ...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19579 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author\n",
       "0      id26305  This process, however, afforded me no means of...    EAP\n",
       "1      id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2      id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3      id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4      id12958  Finding nothing else, not even gold, the Super...    HPL\n",
       "...        ...                                                ...    ...\n",
       "19574  id17718  I could have fancied, while I looked at it, th...    EAP\n",
       "19575  id08973  The lids clenched themselves together as if in...    EAP\n",
       "19576  id05267  Mais il faut agir that is to say, a Frenchman ...    EAP\n",
       "19577  id17513  For an item of news like this, it strikes us i...    EAP\n",
       "19578  id00393  He laid a gnarled claw on my shoulder, and it ...    HPL\n",
       "\n",
       "[19579 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spooky_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/processed/spooky-author'):\n",
    "    os.mkdir('../data/processed/spooky-author')\n",
    "\n",
    "x_train = f'{separator_samples}'.join(spooky_train.text)\n",
    "x_train = x_train.encode('utf-8', 'backslashreplace').decode('utf-8', 'backslashreplace')\n",
    "\n",
    "with open('../data/input/spooky-author/train.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(input_paths=['../data/input/spooky-author/train.txt'],\n",
    "                      tokenizer_output_path='../data/tokenizer/spooky-author',\n",
    "                      lowercase=True,\n",
    "                      vocab_size=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_train['encoding'] = tokenizer.encode_batch(spooky_train.text.tolist())\n",
    "spooky_test['encoding'] = tokenizer.encode_batch(spooky_test.text.tolist())\n",
    "\n",
    "spooky_train['tokens'] = spooky_train.encoding.apply(lambda e: e.tokens)\n",
    "spooky_train['token_ids'] = spooky_train.encoding.apply(lambda e: e.ids)\n",
    "\n",
    "spooky_test['tokens'] = spooky_test.encoding.apply(lambda e: e.tokens)\n",
    "spooky_test['token_ids'] = spooky_test.encoding.apply(lambda e: e.ids)\n",
    "\n",
    "author_to_id = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
    "spooky_train['author_id'] = spooky_train.author.apply(lambda a: author_to_id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['th',\n",
       " 'is',\n",
       " 'Ä pro',\n",
       " 'c',\n",
       " 'ess',\n",
       " ',',\n",
       " 'Ä h',\n",
       " 'ow',\n",
       " 'e',\n",
       " 'ver',\n",
       " ',',\n",
       " 'Ä a',\n",
       " 'ff',\n",
       " 'or',\n",
       " 'd',\n",
       " 'ed',\n",
       " 'Ä me',\n",
       " 'Ä no',\n",
       " 'Ä me',\n",
       " 'an',\n",
       " 's',\n",
       " 'Ä of',\n",
       " 'Ä as',\n",
       " 'c',\n",
       " 'er',\n",
       " 't',\n",
       " 'ain',\n",
       " 'ing',\n",
       " 'Ä the',\n",
       " 'Ä d',\n",
       " 'im',\n",
       " 'en',\n",
       " 's',\n",
       " 'ion',\n",
       " 's',\n",
       " 'Ä of',\n",
       " 'Ä my',\n",
       " 'Ä d',\n",
       " 'u',\n",
       " 'n',\n",
       " 'ge',\n",
       " 'on',\n",
       " ';',\n",
       " 'Ä as',\n",
       " 'Ä i',\n",
       " 'Ä m',\n",
       " 'ight',\n",
       " 'Ä m',\n",
       " 'a',\n",
       " 'ke',\n",
       " 'Ä it',\n",
       " 's',\n",
       " 'Ä c',\n",
       " 'ir',\n",
       " 'c',\n",
       " 'u',\n",
       " 'it',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä re',\n",
       " 't',\n",
       " 'ur',\n",
       " 'n',\n",
       " 'Ä to',\n",
       " 'Ä the',\n",
       " 'Ä po',\n",
       " 'in',\n",
       " 't',\n",
       " 'Ä when',\n",
       " 'ce',\n",
       " 'Ä i',\n",
       " 'Ä s',\n",
       " 'et',\n",
       " 'Ä o',\n",
       " 'ut',\n",
       " ',',\n",
       " 'Ä with',\n",
       " 'out',\n",
       " 'Ä be',\n",
       " 'ing',\n",
       " 'Ä a',\n",
       " 'w',\n",
       " 'a',\n",
       " 're',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä f',\n",
       " 'a',\n",
       " 'ct',\n",
       " ';',\n",
       " 'Ä so',\n",
       " 'Ä per',\n",
       " 'f',\n",
       " 'ect',\n",
       " 'ly',\n",
       " 'Ä un',\n",
       " 'if',\n",
       " 'or',\n",
       " 'm',\n",
       " 'Ä se',\n",
       " 'em',\n",
       " 'ed',\n",
       " 'Ä the',\n",
       " 'Ä w',\n",
       " 'all',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It never once occurred to me that the fumbling might be a mere mistake.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'Ä ne',\n",
       " 'ver',\n",
       " 'Ä on',\n",
       " 'ce',\n",
       " 'Ä o',\n",
       " 'c',\n",
       " 'c',\n",
       " 'ur',\n",
       " 'red',\n",
       " 'Ä to',\n",
       " 'Ä me',\n",
       " 'Ä that',\n",
       " 'Ä the',\n",
       " 'Ä f',\n",
       " 'um',\n",
       " 'b',\n",
       " 'l',\n",
       " 'ing',\n",
       " 'Ä m',\n",
       " 'ight',\n",
       " 'Ä be',\n",
       " 'Ä a',\n",
       " 'Ä me',\n",
       " 're',\n",
       " 'Ä m',\n",
       " 'ist',\n",
       " 'a',\n",
       " 'ke',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'Ä his',\n",
       " 'Ä le',\n",
       " 'f',\n",
       " 't',\n",
       " 'Ä ha',\n",
       " 'nd',\n",
       " 'Ä was',\n",
       " 'Ä a',\n",
       " 'Ä g',\n",
       " 'o',\n",
       " 'ld',\n",
       " 'Ä s',\n",
       " 'n',\n",
       " 'u',\n",
       " 'ff',\n",
       " 'Ä b',\n",
       " 'o',\n",
       " 'x',\n",
       " ',',\n",
       " 'Ä from',\n",
       " 'Ä which',\n",
       " ',',\n",
       " 'Ä as',\n",
       " 'Ä he',\n",
       " 'Ä c',\n",
       " 'a',\n",
       " 'pe',\n",
       " 'red',\n",
       " 'Ä d',\n",
       " 'ow',\n",
       " 'n',\n",
       " 'Ä the',\n",
       " 'Ä h',\n",
       " 'ill',\n",
       " ',',\n",
       " 'Ä c',\n",
       " 'ut',\n",
       " 't',\n",
       " 'ing',\n",
       " 'Ä all',\n",
       " 'Ä man',\n",
       " 'ne',\n",
       " 'r',\n",
       " 'Ä of',\n",
       " 'Ä f',\n",
       " 'ant',\n",
       " 'ast',\n",
       " 'ic',\n",
       " 'Ä st',\n",
       " 'e',\n",
       " 'p',\n",
       " 's',\n",
       " ',',\n",
       " 'Ä he',\n",
       " 'Ä to',\n",
       " 'o',\n",
       " 'k',\n",
       " 'Ä s',\n",
       " 'n',\n",
       " 'u',\n",
       " 'ff',\n",
       " 'Ä in',\n",
       " 'c',\n",
       " 'ess',\n",
       " 'ant',\n",
       " 'ly',\n",
       " 'Ä with',\n",
       " 'Ä an',\n",
       " 'Ä a',\n",
       " 'ir',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä g',\n",
       " 'reat',\n",
       " 'est',\n",
       " 'Ä po',\n",
       " 'ss',\n",
       " 'i',\n",
       " 'ble',\n",
       " 'Ä se',\n",
       " 'lf',\n",
       " 'Ä s',\n",
       " 'at',\n",
       " 'is',\n",
       " 'f',\n",
       " 'a',\n",
       " 'ct',\n",
       " 'ion',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'ow',\n",
       " 'Ä lo',\n",
       " 've',\n",
       " 'ly',\n",
       " 'Ä is',\n",
       " 'Ä sp',\n",
       " 'r',\n",
       " 'ing',\n",
       " 'Ä as',\n",
       " 'Ä we',\n",
       " 'Ä lo',\n",
       " 'o',\n",
       " 'k',\n",
       " 'ed',\n",
       " 'Ä from',\n",
       " 'Ä w',\n",
       " 'ind',\n",
       " 's',\n",
       " 'or',\n",
       " 'Ä t',\n",
       " 'er',\n",
       " 'ra',\n",
       " 'ce',\n",
       " 'Ä on',\n",
       " 'Ä the',\n",
       " 'Ä s',\n",
       " 'i',\n",
       " 'x',\n",
       " 't',\n",
       " 'e',\n",
       " 'en',\n",
       " 'Ä f',\n",
       " 'er',\n",
       " 't',\n",
       " 'i',\n",
       " 'le',\n",
       " 'Ä c',\n",
       " 'ou',\n",
       " 'nt',\n",
       " 'i',\n",
       " 'es',\n",
       " 'Ä sp',\n",
       " 're',\n",
       " 'ad',\n",
       " 'Ä be',\n",
       " 'ne',\n",
       " 'at',\n",
       " 'h',\n",
       " ',',\n",
       " 'Ä s',\n",
       " 'pe',\n",
       " 'c',\n",
       " 'k',\n",
       " 'led',\n",
       " 'Ä by',\n",
       " 'Ä ha',\n",
       " 'pp',\n",
       " 'y',\n",
       " 'Ä c',\n",
       " 'ot',\n",
       " 't',\n",
       " 'ag',\n",
       " 'es',\n",
       " 'Ä and',\n",
       " 'Ä we',\n",
       " 'al',\n",
       " 'th',\n",
       " 'i',\n",
       " 'er',\n",
       " 'Ä to',\n",
       " 'w',\n",
       " 'n',\n",
       " 's',\n",
       " ',',\n",
       " 'Ä all',\n",
       " 'Ä lo',\n",
       " 'o',\n",
       " 'k',\n",
       " 'ed',\n",
       " 'Ä as',\n",
       " 'Ä in',\n",
       " 'Ä for',\n",
       " 'm',\n",
       " 'er',\n",
       " 'Ä y',\n",
       " 'e',\n",
       " 'ar',\n",
       " 's',\n",
       " ',',\n",
       " 'Ä he',\n",
       " 'art',\n",
       " 'Ä c',\n",
       " 'he',\n",
       " 'er',\n",
       " 'ing',\n",
       " 'Ä and',\n",
       " 'Ä f',\n",
       " 'a',\n",
       " 'ir',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f',\n",
       " 'ind',\n",
       " 'ing',\n",
       " 'Ä not',\n",
       " 'h',\n",
       " 'ing',\n",
       " 'Ä e',\n",
       " 'l',\n",
       " 'se',\n",
       " ',',\n",
       " 'Ä not',\n",
       " 'Ä e',\n",
       " 'ven',\n",
       " 'Ä g',\n",
       " 'o',\n",
       " 'ld',\n",
       " ',',\n",
       " 'Ä the',\n",
       " 'Ä su',\n",
       " 'per',\n",
       " 'in',\n",
       " 't',\n",
       " 'e',\n",
       " 'nd',\n",
       " 'ent',\n",
       " 'Ä ab',\n",
       " 'and',\n",
       " 'o',\n",
       " 'ned',\n",
       " 'Ä his',\n",
       " 'Ä at',\n",
       " 't',\n",
       " 'em',\n",
       " 'pt',\n",
       " 's',\n",
       " ';',\n",
       " 'Ä but',\n",
       " 'Ä a',\n",
       " 'Ä per',\n",
       " 'ple',\n",
       " 'x',\n",
       " 'ed',\n",
       " 'Ä lo',\n",
       " 'o',\n",
       " 'k',\n",
       " 'Ä o',\n",
       " 'c',\n",
       " 'c',\n",
       " 'as',\n",
       " 'ion',\n",
       " 'all',\n",
       " 'y',\n",
       " 'Ä st',\n",
       " 'e',\n",
       " 'al',\n",
       " 's',\n",
       " 'Ä o',\n",
       " 'ver',\n",
       " 'Ä his',\n",
       " 'Ä c',\n",
       " 'ou',\n",
       " 'nt',\n",
       " 'en',\n",
       " 'an',\n",
       " 'ce',\n",
       " 'Ä as',\n",
       " 'Ä he',\n",
       " 'Ä s',\n",
       " 'it',\n",
       " 's',\n",
       " 'Ä th',\n",
       " 'in',\n",
       " 'k',\n",
       " 'ing',\n",
       " 'Ä at',\n",
       " 'Ä his',\n",
       " 'Ä d',\n",
       " 'es',\n",
       " 'k',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'Ä you',\n",
       " 'th',\n",
       " 'Ä p',\n",
       " 'as',\n",
       " 's',\n",
       " 'ed',\n",
       " 'Ä in',\n",
       " 'Ä so',\n",
       " 'l',\n",
       " 'it',\n",
       " 'u',\n",
       " 'd',\n",
       " 'e',\n",
       " ',',\n",
       " 'Ä my',\n",
       " 'Ä b',\n",
       " 'est',\n",
       " 'Ä y',\n",
       " 'e',\n",
       " 'ar',\n",
       " 's',\n",
       " 'Ä sp',\n",
       " 'ent',\n",
       " 'Ä u',\n",
       " 'nd',\n",
       " 'er',\n",
       " 'Ä you',\n",
       " 'r',\n",
       " 'Ä g',\n",
       " 'ent',\n",
       " 'le',\n",
       " 'Ä and',\n",
       " 'Ä fe',\n",
       " 'm',\n",
       " 'in',\n",
       " 'in',\n",
       " 'e',\n",
       " 'Ä f',\n",
       " 'o',\n",
       " 'st',\n",
       " 'er',\n",
       " 'a',\n",
       " 'ge',\n",
       " ',',\n",
       " 'Ä ha',\n",
       " 's',\n",
       " 'Ä so',\n",
       " 'Ä re',\n",
       " 'f',\n",
       " 'in',\n",
       " 'ed',\n",
       " 'Ä the',\n",
       " 'Ä g',\n",
       " 'r',\n",
       " 'ound',\n",
       " 'w',\n",
       " 'or',\n",
       " 'k',\n",
       " 'Ä of',\n",
       " 'Ä my',\n",
       " 'Ä c',\n",
       " 'ha',\n",
       " 'ra',\n",
       " 'ct',\n",
       " 'er',\n",
       " 'Ä that',\n",
       " 'Ä i',\n",
       " 'Ä c',\n",
       " 'an',\n",
       " 'n',\n",
       " 'ot',\n",
       " 'Ä o',\n",
       " 'ver',\n",
       " 'c',\n",
       " 'ome',\n",
       " 'Ä an',\n",
       " 'Ä int',\n",
       " 'en',\n",
       " 'se',\n",
       " 'Ä dis',\n",
       " 't',\n",
       " 'ast',\n",
       " 'e',\n",
       " 'Ä to',\n",
       " 'Ä the',\n",
       " 'Ä u',\n",
       " 's',\n",
       " 'u',\n",
       " 'al',\n",
       " 'Ä b',\n",
       " 'r',\n",
       " 'ut',\n",
       " 'al',\n",
       " 'ity',\n",
       " 'Ä ex',\n",
       " 'er',\n",
       " 'c',\n",
       " 'is',\n",
       " 'ed',\n",
       " 'Ä on',\n",
       " 'Ä b',\n",
       " 'o',\n",
       " 'ard',\n",
       " 'Ä sh',\n",
       " 'i',\n",
       " 'p',\n",
       " ':',\n",
       " 'Ä i',\n",
       " 'Ä have',\n",
       " 'Ä ne',\n",
       " 'ver',\n",
       " 'Ä be',\n",
       " 'l',\n",
       " 'ie',\n",
       " 'ved',\n",
       " 'Ä it',\n",
       " 'Ä to',\n",
       " 'Ä be',\n",
       " 'Ä ne',\n",
       " 'c',\n",
       " 'ess',\n",
       " 'ar',\n",
       " 'y',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä when',\n",
       " 'Ä i',\n",
       " 'Ä he',\n",
       " 'ard',\n",
       " 'Ä of',\n",
       " 'Ä a',\n",
       " 'Ä m',\n",
       " 'ar',\n",
       " 'in',\n",
       " 'er',\n",
       " 'Ä e',\n",
       " 'qu',\n",
       " 'all',\n",
       " 'y',\n",
       " 'Ä not',\n",
       " 'ed',\n",
       " 'Ä for',\n",
       " 'Ä his',\n",
       " 'Ä k',\n",
       " 'ind',\n",
       " 'l',\n",
       " 'in',\n",
       " 'ess',\n",
       " 'Ä of',\n",
       " 'Ä he',\n",
       " 'art',\n",
       " 'Ä and',\n",
       " 'Ä the',\n",
       " 'Ä re',\n",
       " 's',\n",
       " 'pe',\n",
       " 'ct',\n",
       " 'Ä and',\n",
       " 'Ä o',\n",
       " 'b',\n",
       " 'ed',\n",
       " 'i',\n",
       " 'en',\n",
       " 'ce',\n",
       " 'Ä p',\n",
       " 'a',\n",
       " 'id',\n",
       " 'Ä to',\n",
       " 'Ä him',\n",
       " 'Ä by',\n",
       " 'Ä his',\n",
       " 'Ä c',\n",
       " 're',\n",
       " 'w',\n",
       " ',',\n",
       " 'Ä i',\n",
       " 'Ä fe',\n",
       " 'l',\n",
       " 't',\n",
       " 'Ä my',\n",
       " 'se',\n",
       " 'lf',\n",
       " 'Ä p',\n",
       " 'e',\n",
       " 'c',\n",
       " 'ul',\n",
       " 'i',\n",
       " 'ar',\n",
       " 'ly',\n",
       " 'Ä for',\n",
       " 't',\n",
       " 'u',\n",
       " 'n',\n",
       " 'ate',\n",
       " 'Ä in',\n",
       " 'Ä be',\n",
       " 'ing',\n",
       " 'Ä a',\n",
       " 'ble',\n",
       " 'Ä to',\n",
       " 'Ä se',\n",
       " 'c',\n",
       " 'ure',\n",
       " 'Ä his',\n",
       " 'Ä s',\n",
       " 'er',\n",
       " 'v',\n",
       " 'ic',\n",
       " 'es',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'Ä a',\n",
       " 'st',\n",
       " 'r',\n",
       " 'on',\n",
       " 'om',\n",
       " 'er',\n",
       " ',',\n",
       " 'Ä per',\n",
       " 'ha',\n",
       " 'p',\n",
       " 's',\n",
       " ',',\n",
       " 'Ä at',\n",
       " 'Ä this',\n",
       " 'Ä po',\n",
       " 'in',\n",
       " 't',\n",
       " ',',\n",
       " 'Ä to',\n",
       " 'o',\n",
       " 'k',\n",
       " 'Ä re',\n",
       " 'f',\n",
       " 'u',\n",
       " 'ge',\n",
       " 'Ä in',\n",
       " 'Ä the',\n",
       " 'Ä su',\n",
       " 'g',\n",
       " 'g',\n",
       " 'est',\n",
       " 'ion',\n",
       " 'Ä of',\n",
       " 'Ä n',\n",
       " 'on',\n",
       " 'Ä l',\n",
       " 'um',\n",
       " 'in',\n",
       " 'o',\n",
       " 's',\n",
       " 'ity',\n",
       " ';',\n",
       " 'Ä and',\n",
       " 'Ä he',\n",
       " 're',\n",
       " 'Ä an',\n",
       " 'al',\n",
       " 'o',\n",
       " 'g',\n",
       " 'y',\n",
       " 'Ä was',\n",
       " 'Ä su',\n",
       " 'd',\n",
       " 'd',\n",
       " 'en',\n",
       " 'ly',\n",
       " 'Ä le',\n",
       " 't',\n",
       " 'Ä f',\n",
       " 'all',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The surcingle hung in ribands from my body.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'Ä s',\n",
       " 'ur',\n",
       " 'c',\n",
       " 'ing',\n",
       " 'le',\n",
       " 'Ä h',\n",
       " 'u',\n",
       " 'n',\n",
       " 'g',\n",
       " 'Ä in',\n",
       " 'Ä r',\n",
       " 'i',\n",
       " 'b',\n",
       " 'and',\n",
       " 's',\n",
       " 'Ä from',\n",
       " 'Ä my',\n",
       " 'Ä b',\n",
       " 'od',\n",
       " 'y',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'Ä k',\n",
       " 'new',\n",
       " 'Ä that',\n",
       " 'Ä you',\n",
       " 'Ä could',\n",
       " 'Ä not',\n",
       " 'Ä s',\n",
       " 'ay',\n",
       " 'Ä to',\n",
       " 'Ä you',\n",
       " 'r',\n",
       " 'se',\n",
       " 'lf',\n",
       " 'Ä ',\n",
       " \"'\",\n",
       " 'st',\n",
       " 'e',\n",
       " 're',\n",
       " 'ot',\n",
       " 'om',\n",
       " 'y',\n",
       " \"'\",\n",
       " 'Ä with',\n",
       " 'out',\n",
       " 'Ä be',\n",
       " 'ing',\n",
       " 'Ä b',\n",
       " 'r',\n",
       " 'ou',\n",
       " 'ght',\n",
       " 'Ä to',\n",
       " 'Ä th',\n",
       " 'in',\n",
       " 'k',\n",
       " 'Ä of',\n",
       " 'Ä at',\n",
       " 'om',\n",
       " 'i',\n",
       " 'es',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä th',\n",
       " 'us',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä the',\n",
       " 'or',\n",
       " 'i',\n",
       " 'es',\n",
       " 'Ä of',\n",
       " 'Ä e',\n",
       " 'p',\n",
       " 'ic',\n",
       " 'ur',\n",
       " 'us',\n",
       " ';',\n",
       " 'Ä and',\n",
       " 'Ä s',\n",
       " 'in',\n",
       " 'ce',\n",
       " ',',\n",
       " 'Ä when',\n",
       " 'Ä we',\n",
       " 'Ä dis',\n",
       " 'c',\n",
       " 'u',\n",
       " 'ss',\n",
       " 'ed',\n",
       " 'Ä this',\n",
       " 'Ä su',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ect',\n",
       " 'Ä not',\n",
       " 'Ä ',\n",
       " 'very',\n",
       " 'Ä l',\n",
       " 'ong',\n",
       " 'Ä a',\n",
       " 'g',\n",
       " 'o',\n",
       " ',',\n",
       " 'Ä i',\n",
       " 'Ä m',\n",
       " 'ent',\n",
       " 'i',\n",
       " 'o',\n",
       " 'ned',\n",
       " 'Ä to',\n",
       " 'Ä you',\n",
       " 'Ä h',\n",
       " 'ow',\n",
       " 'Ä s',\n",
       " 'ing',\n",
       " 'ul',\n",
       " 'ar',\n",
       " 'ly',\n",
       " ',',\n",
       " 'Ä y',\n",
       " 'et',\n",
       " 'Ä with',\n",
       " 'Ä h',\n",
       " 'ow',\n",
       " 'Ä l',\n",
       " 'it',\n",
       " 't',\n",
       " 'le',\n",
       " 'Ä not',\n",
       " 'ic',\n",
       " 'e',\n",
       " ',',\n",
       " 'Ä the',\n",
       " 'Ä v',\n",
       " 'ag',\n",
       " 'u',\n",
       " 'e',\n",
       " 'Ä g',\n",
       " 'u',\n",
       " 'ess',\n",
       " 'es',\n",
       " 'Ä of',\n",
       " 'Ä that',\n",
       " 'Ä no',\n",
       " 'ble',\n",
       " 'Ä g',\n",
       " 're',\n",
       " 'e',\n",
       " 'k',\n",
       " 'Ä had',\n",
       " 'Ä me',\n",
       " 't',\n",
       " 'Ä with',\n",
       " 'Ä con',\n",
       " 'f',\n",
       " 'ir',\n",
       " 'm',\n",
       " 'ation',\n",
       " 'Ä in',\n",
       " 'Ä the',\n",
       " 'Ä l',\n",
       " 'ate',\n",
       " 'Ä ne',\n",
       " 'b',\n",
       " 'ul',\n",
       " 'ar',\n",
       " 'Ä c',\n",
       " 'o',\n",
       " 's',\n",
       " 'm',\n",
       " 'o',\n",
       " 'g',\n",
       " 'on',\n",
       " 'y',\n",
       " ',',\n",
       " 'Ä i',\n",
       " 'Ä fe',\n",
       " 'l',\n",
       " 't',\n",
       " 'Ä that',\n",
       " 'Ä you',\n",
       " 'Ä could',\n",
       " 'Ä not',\n",
       " 'Ä a',\n",
       " 'v',\n",
       " 'o',\n",
       " 'id',\n",
       " 'Ä c',\n",
       " 'ast',\n",
       " 'ing',\n",
       " 'Ä you',\n",
       " 'r',\n",
       " 'Ä e',\n",
       " 'y',\n",
       " 'es',\n",
       " 'Ä up',\n",
       " 'w',\n",
       " 'ard',\n",
       " 'Ä to',\n",
       " 'Ä the',\n",
       " 'Ä g',\n",
       " 'reat',\n",
       " 'Ä ne',\n",
       " 'b',\n",
       " 'ul',\n",
       " 'a',\n",
       " 'Ä in',\n",
       " 'Ä or',\n",
       " 'ion',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä i',\n",
       " 'Ä c',\n",
       " 'er',\n",
       " 't',\n",
       " 'ain',\n",
       " 'ly',\n",
       " 'Ä ex',\n",
       " 'pe',\n",
       " 'ct',\n",
       " 'ed',\n",
       " 'Ä that',\n",
       " 'Ä you',\n",
       " 'Ä would',\n",
       " 'Ä do',\n",
       " 'Ä so',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'Ä con',\n",
       " 'f',\n",
       " 'ess',\n",
       " 'Ä that',\n",
       " 'Ä ne',\n",
       " 'it',\n",
       " 'her',\n",
       " 'Ä the',\n",
       " 'Ä st',\n",
       " 'r',\n",
       " 'u',\n",
       " 'ct',\n",
       " 'ure',\n",
       " 'Ä of',\n",
       " 'Ä l',\n",
       " 'an',\n",
       " 'g',\n",
       " 'u',\n",
       " 'ag',\n",
       " 'es',\n",
       " ',',\n",
       " 'Ä n',\n",
       " 'or',\n",
       " 'Ä the',\n",
       " 'Ä c',\n",
       " 'od',\n",
       " 'e',\n",
       " 'Ä of',\n",
       " 'Ä g',\n",
       " 'o',\n",
       " 'ver',\n",
       " 'n',\n",
       " 'm',\n",
       " 'ent',\n",
       " 's',\n",
       " ',',\n",
       " 'Ä n',\n",
       " 'or',\n",
       " 'Ä the',\n",
       " 'Ä p',\n",
       " 'ol',\n",
       " 'it',\n",
       " 'ic',\n",
       " 's',\n",
       " 'Ä of',\n",
       " 'Ä v',\n",
       " 'ar',\n",
       " 'i',\n",
       " 'ous',\n",
       " 'Ä st',\n",
       " 'at',\n",
       " 'es',\n",
       " 'Ä po',\n",
       " 'ss',\n",
       " 'ess',\n",
       " 'ed',\n",
       " 'Ä at',\n",
       " 't',\n",
       " 'ra',\n",
       " 'ct',\n",
       " 'ion',\n",
       " 's',\n",
       " 'Ä for',\n",
       " 'Ä me',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He shall find that I can feel my injuries; he shall learn to dread my revenge\" A few days after he arrived.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'Ä s',\n",
       " 'ha',\n",
       " 'll',\n",
       " 'Ä f',\n",
       " 'ind',\n",
       " 'Ä that',\n",
       " 'Ä i',\n",
       " 'Ä c',\n",
       " 'an',\n",
       " 'Ä fe',\n",
       " 'e',\n",
       " 'l',\n",
       " 'Ä my',\n",
       " 'Ä in',\n",
       " 'j',\n",
       " 'ur',\n",
       " 'i',\n",
       " 'es',\n",
       " ';',\n",
       " 'Ä he',\n",
       " 'Ä s',\n",
       " 'ha',\n",
       " 'll',\n",
       " 'Ä le',\n",
       " 'ar',\n",
       " 'n',\n",
       " 'Ä to',\n",
       " 'Ä d',\n",
       " 're',\n",
       " 'ad',\n",
       " 'Ä my',\n",
       " 'Ä re',\n",
       " 'ven',\n",
       " 'ge',\n",
       " '\"',\n",
       " 'Ä a',\n",
       " 'Ä fe',\n",
       " 'w',\n",
       " 'Ä d',\n",
       " 'ay',\n",
       " 's',\n",
       " 'Ä a',\n",
       " 'f',\n",
       " 'ter',\n",
       " 'Ä he',\n",
       " 'Ä ar',\n",
       " 'ri',\n",
       " 'ved',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here we barricaded ourselves, and, for the present were secure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 're',\n",
       " 'Ä we',\n",
       " 'Ä b',\n",
       " 'ar',\n",
       " 'r',\n",
       " 'ic',\n",
       " 'ad',\n",
       " 'ed',\n",
       " 'Ä o',\n",
       " 'ur',\n",
       " 'se',\n",
       " 'l',\n",
       " 'v',\n",
       " 'es',\n",
       " ',',\n",
       " 'Ä and',\n",
       " ',',\n",
       " 'Ä for',\n",
       " 'Ä the',\n",
       " 'Ä pre',\n",
       " 's',\n",
       " 'ent',\n",
       " 'Ä were',\n",
       " 'Ä se',\n",
       " 'c',\n",
       " 'ure',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Herbert West needed fresh bodies because his life work was the reanimation of the dead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['her',\n",
       " 'b',\n",
       " 'er',\n",
       " 't',\n",
       " 'Ä w',\n",
       " 'est',\n",
       " 'Ä ne',\n",
       " 'ed',\n",
       " 'ed',\n",
       " 'Ä f',\n",
       " 're',\n",
       " 's',\n",
       " 'h',\n",
       " 'Ä b',\n",
       " 'od',\n",
       " 'i',\n",
       " 'es',\n",
       " 'Ä be',\n",
       " 'c',\n",
       " 'a',\n",
       " 'u',\n",
       " 'se',\n",
       " 'Ä his',\n",
       " 'Ä l',\n",
       " 'if',\n",
       " 'e',\n",
       " 'Ä wor',\n",
       " 'k',\n",
       " 'Ä was',\n",
       " 'Ä the',\n",
       " 'Ä re',\n",
       " 'an',\n",
       " 'im',\n",
       " 'ation',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä de',\n",
       " 'ad',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The farm like grounds extended back very deeply up the hill, almost to Wheaton Street.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'Ä f',\n",
       " 'ar',\n",
       " 'm',\n",
       " 'Ä l',\n",
       " 'i',\n",
       " 'ke',\n",
       " 'Ä g',\n",
       " 'r',\n",
       " 'ound',\n",
       " 's',\n",
       " 'Ä ex',\n",
       " 't',\n",
       " 'e',\n",
       " 'nd',\n",
       " 'ed',\n",
       " 'Ä b',\n",
       " 'ac',\n",
       " 'k',\n",
       " 'Ä ',\n",
       " 'very',\n",
       " 'Ä de',\n",
       " 'e',\n",
       " 'p',\n",
       " 'ly',\n",
       " 'Ä up',\n",
       " 'Ä the',\n",
       " 'Ä h',\n",
       " 'ill',\n",
       " ',',\n",
       " 'Ä al',\n",
       " 'm',\n",
       " 'o',\n",
       " 'st',\n",
       " 'Ä to',\n",
       " 'Ä whe',\n",
       " 'at',\n",
       " 'on',\n",
       " 'Ä st',\n",
       " 're',\n",
       " 'et',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But a glance will show the fallacy of this idea.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'ut',\n",
       " 'Ä a',\n",
       " 'Ä g',\n",
       " 'l',\n",
       " 'an',\n",
       " 'ce',\n",
       " 'Ä w',\n",
       " 'ill',\n",
       " 'Ä sh',\n",
       " 'ow',\n",
       " 'Ä the',\n",
       " 'Ä f',\n",
       " 'all',\n",
       " 'ac',\n",
       " 'y',\n",
       " 'Ä of',\n",
       " 'Ä this',\n",
       " 'Ä i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'a',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He had escaped me, and I must commence a destructive and almost endless journey across the mountainous ices of the ocean, amidst cold that few of the inhabitants could long endure and which I, the native of a genial and sunny climate, could not hope to survive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'Ä had',\n",
       " 'Ä ',\n",
       " 'es',\n",
       " 'c',\n",
       " 'a',\n",
       " 'p',\n",
       " 'ed',\n",
       " 'Ä me',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä i',\n",
       " 'Ä m',\n",
       " 'ust',\n",
       " 'Ä com',\n",
       " 'm',\n",
       " 'en',\n",
       " 'ce',\n",
       " 'Ä a',\n",
       " 'Ä d',\n",
       " 'est',\n",
       " 'r',\n",
       " 'u',\n",
       " 'ct',\n",
       " 'ive',\n",
       " 'Ä and',\n",
       " 'Ä al',\n",
       " 'm',\n",
       " 'o',\n",
       " 'st',\n",
       " 'Ä e',\n",
       " 'nd',\n",
       " 'le',\n",
       " 'ss',\n",
       " 'Ä j',\n",
       " 'our',\n",
       " 'ne',\n",
       " 'y',\n",
       " 'Ä a',\n",
       " 'c',\n",
       " 'ro',\n",
       " 'ss',\n",
       " 'Ä the',\n",
       " 'Ä m',\n",
       " 'ou',\n",
       " 'nt',\n",
       " 'ain',\n",
       " 'ous',\n",
       " 'Ä i',\n",
       " 'c',\n",
       " 'es',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä o',\n",
       " 'ce',\n",
       " 'an',\n",
       " ',',\n",
       " 'Ä a',\n",
       " 'm',\n",
       " 'id',\n",
       " 'st',\n",
       " 'Ä c',\n",
       " 'o',\n",
       " 'ld',\n",
       " 'Ä that',\n",
       " 'Ä fe',\n",
       " 'w',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä in',\n",
       " 'ha',\n",
       " 'b',\n",
       " 'it',\n",
       " 'ant',\n",
       " 's',\n",
       " 'Ä could',\n",
       " 'Ä l',\n",
       " 'ong',\n",
       " 'Ä e',\n",
       " 'nd',\n",
       " 'ure',\n",
       " 'Ä and',\n",
       " 'Ä which',\n",
       " 'Ä i',\n",
       " ',',\n",
       " 'Ä the',\n",
       " 'Ä n',\n",
       " 'at',\n",
       " 'ive',\n",
       " 'Ä of',\n",
       " 'Ä a',\n",
       " 'Ä g',\n",
       " 'en',\n",
       " 'i',\n",
       " 'al',\n",
       " 'Ä and',\n",
       " 'Ä su',\n",
       " 'n',\n",
       " 'n',\n",
       " 'y',\n",
       " 'Ä c',\n",
       " 'l',\n",
       " 'im',\n",
       " 'ate',\n",
       " ',',\n",
       " 'Ä could',\n",
       " 'Ä not',\n",
       " 'Ä h',\n",
       " 'o',\n",
       " 'pe',\n",
       " 'Ä to',\n",
       " 'Ä s',\n",
       " 'ur',\n",
       " 'v',\n",
       " 'ive',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To these speeches they gave, of course, their own interpretation; fancying, no doubt, that at all events I should come into possession of vast quantities of ready money; and provided I paid them all I owed, and a trifle more, in consideration of their services, I dare say they cared very little what became of either my soul or my carcass.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'o',\n",
       " 'Ä the',\n",
       " 'se',\n",
       " 'Ä s',\n",
       " 'pe',\n",
       " 'e',\n",
       " 'c',\n",
       " 'he',\n",
       " 's',\n",
       " 'Ä the',\n",
       " 'y',\n",
       " 'Ä g',\n",
       " 'a',\n",
       " 've',\n",
       " ',',\n",
       " 'Ä of',\n",
       " 'Ä c',\n",
       " 'our',\n",
       " 'se',\n",
       " ',',\n",
       " 'Ä the',\n",
       " 'ir',\n",
       " 'Ä o',\n",
       " 'w',\n",
       " 'n',\n",
       " 'Ä in',\n",
       " 'ter',\n",
       " 'p',\n",
       " 're',\n",
       " 't',\n",
       " 'ation',\n",
       " ';',\n",
       " 'Ä f',\n",
       " 'an',\n",
       " 'c',\n",
       " 'y',\n",
       " 'ing',\n",
       " ',',\n",
       " 'Ä no',\n",
       " 'Ä d',\n",
       " 'ou',\n",
       " 'b',\n",
       " 't',\n",
       " ',',\n",
       " 'Ä that',\n",
       " 'Ä at',\n",
       " 'Ä all',\n",
       " 'Ä e',\n",
       " 'v',\n",
       " 'ent',\n",
       " 's',\n",
       " 'Ä i',\n",
       " 'Ä sh',\n",
       " 'ould',\n",
       " 'Ä c',\n",
       " 'ome',\n",
       " 'Ä int',\n",
       " 'o',\n",
       " 'Ä po',\n",
       " 'ss',\n",
       " 'ess',\n",
       " 'ion',\n",
       " 'Ä of',\n",
       " 'Ä v',\n",
       " 'ast',\n",
       " 'Ä ',\n",
       " 'qu',\n",
       " 'ant',\n",
       " 'it',\n",
       " 'i',\n",
       " 'es',\n",
       " 'Ä of',\n",
       " 'Ä re',\n",
       " 'ad',\n",
       " 'y',\n",
       " 'Ä mo',\n",
       " 'ne',\n",
       " 'y',\n",
       " ';',\n",
       " 'Ä and',\n",
       " 'Ä pro',\n",
       " 'v',\n",
       " 'id',\n",
       " 'ed',\n",
       " 'Ä i',\n",
       " 'Ä p',\n",
       " 'a',\n",
       " 'id',\n",
       " 'Ä the',\n",
       " 'm',\n",
       " 'Ä all',\n",
       " 'Ä i',\n",
       " 'Ä o',\n",
       " 'w',\n",
       " 'ed',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä a',\n",
       " 'Ä t',\n",
       " 'ri',\n",
       " 'f',\n",
       " 'le',\n",
       " 'Ä m',\n",
       " 'ore',\n",
       " ',',\n",
       " 'Ä in',\n",
       " 'Ä con',\n",
       " 's',\n",
       " 'id',\n",
       " 'er',\n",
       " 'ation',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'ir',\n",
       " 'Ä s',\n",
       " 'er',\n",
       " 'v',\n",
       " 'ic',\n",
       " 'es',\n",
       " ',',\n",
       " 'Ä i',\n",
       " 'Ä d',\n",
       " 'a',\n",
       " 're',\n",
       " 'Ä s',\n",
       " 'ay',\n",
       " 'Ä the',\n",
       " 'y',\n",
       " 'Ä c',\n",
       " 'a',\n",
       " 'red',\n",
       " 'Ä ',\n",
       " 'very',\n",
       " 'Ä l',\n",
       " 'it',\n",
       " 't',\n",
       " 'le',\n",
       " 'Ä w',\n",
       " 'hat',\n",
       " 'Ä be',\n",
       " 'c',\n",
       " 'ame',\n",
       " 'Ä of',\n",
       " 'Ä e',\n",
       " 'it',\n",
       " 'her',\n",
       " 'Ä my',\n",
       " 'Ä s',\n",
       " 'ou',\n",
       " 'l',\n",
       " 'Ä or',\n",
       " 'Ä my',\n",
       " 'Ä c',\n",
       " 'ar',\n",
       " 'c',\n",
       " 'as',\n",
       " 's',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Her native sprightliness needed no undue excitement, and her placid heart reposed contented on my love, the well being of her children, and the beauty of surrounding nature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['her',\n",
       " 'Ä n',\n",
       " 'at',\n",
       " 'ive',\n",
       " 'Ä sp',\n",
       " 'r',\n",
       " 'ight',\n",
       " 'l',\n",
       " 'in',\n",
       " 'ess',\n",
       " 'Ä ne',\n",
       " 'ed',\n",
       " 'ed',\n",
       " 'Ä no',\n",
       " 'Ä u',\n",
       " 'nd',\n",
       " 'u',\n",
       " 'e',\n",
       " 'Ä ex',\n",
       " 'c',\n",
       " 'it',\n",
       " 'em',\n",
       " 'ent',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä her',\n",
       " 'Ä pl',\n",
       " 'ac',\n",
       " 'id',\n",
       " 'Ä he',\n",
       " 'art',\n",
       " 'Ä re',\n",
       " 'p',\n",
       " 'o',\n",
       " 's',\n",
       " 'ed',\n",
       " 'Ä con',\n",
       " 't',\n",
       " 'ent',\n",
       " 'ed',\n",
       " 'Ä on',\n",
       " 'Ä my',\n",
       " 'Ä lo',\n",
       " 've',\n",
       " ',',\n",
       " 'Ä the',\n",
       " 'Ä we',\n",
       " 'll',\n",
       " 'Ä be',\n",
       " 'ing',\n",
       " 'Ä of',\n",
       " 'Ä her',\n",
       " 'Ä c',\n",
       " 'h',\n",
       " 'i',\n",
       " 'ld',\n",
       " 're',\n",
       " 'n',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä the',\n",
       " 'Ä be',\n",
       " 'a',\n",
       " 'ut',\n",
       " 'y',\n",
       " 'Ä of',\n",
       " 'Ä s',\n",
       " 'ur',\n",
       " 'r',\n",
       " 'ound',\n",
       " 'ing',\n",
       " 'Ä n',\n",
       " 'at',\n",
       " 'ure',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I even went so far as to speak of a slightly hectic cough with which, at one time, I had been troubled of a chronic rheumatism of a twinge of hereditary gout and, in conclusion, of the disagreeable and inconvenient, but hitherto carefully concealed, weakness of my eyes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'Ä e',\n",
       " 'ven',\n",
       " 'Ä w',\n",
       " 'ent',\n",
       " 'Ä so',\n",
       " 'Ä f',\n",
       " 'ar',\n",
       " 'Ä as',\n",
       " 'Ä to',\n",
       " 'Ä s',\n",
       " 'pe',\n",
       " 'a',\n",
       " 'k',\n",
       " 'Ä of',\n",
       " 'Ä a',\n",
       " 'Ä s',\n",
       " 'l',\n",
       " 'ight',\n",
       " 'ly',\n",
       " 'Ä he',\n",
       " 'ct',\n",
       " 'ic',\n",
       " 'Ä c',\n",
       " 'ough',\n",
       " 'Ä with',\n",
       " 'Ä which',\n",
       " ',',\n",
       " 'Ä at',\n",
       " 'Ä one',\n",
       " 'Ä t',\n",
       " 'im',\n",
       " 'e',\n",
       " ',',\n",
       " 'Ä i',\n",
       " 'Ä had',\n",
       " 'Ä been',\n",
       " 'Ä t',\n",
       " 'r',\n",
       " 'ou',\n",
       " 'ble',\n",
       " 'd',\n",
       " 'Ä of',\n",
       " 'Ä a',\n",
       " 'Ä c',\n",
       " 'h',\n",
       " 'r',\n",
       " 'on',\n",
       " 'ic',\n",
       " 'Ä r',\n",
       " 'he',\n",
       " 'um',\n",
       " 'at',\n",
       " 'is',\n",
       " 'm',\n",
       " 'Ä of',\n",
       " 'Ä a',\n",
       " 'Ä t',\n",
       " 'w',\n",
       " 'ing',\n",
       " 'e',\n",
       " 'Ä of',\n",
       " 'Ä he',\n",
       " 'red',\n",
       " 'it',\n",
       " 'ar',\n",
       " 'y',\n",
       " 'Ä g',\n",
       " 'out',\n",
       " 'Ä and',\n",
       " ',',\n",
       " 'Ä in',\n",
       " 'Ä con',\n",
       " 'c',\n",
       " 'l',\n",
       " 'us',\n",
       " 'ion',\n",
       " ',',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä dis',\n",
       " 'ag',\n",
       " 're',\n",
       " 'e',\n",
       " 'able',\n",
       " 'Ä and',\n",
       " 'Ä in',\n",
       " 'c',\n",
       " 'on',\n",
       " 'ven',\n",
       " 'i',\n",
       " 'ent',\n",
       " ',',\n",
       " 'Ä but',\n",
       " 'Ä h',\n",
       " 'it',\n",
       " 'her',\n",
       " 't',\n",
       " 'o',\n",
       " 'Ä c',\n",
       " 'a',\n",
       " 're',\n",
       " 'f',\n",
       " 'u',\n",
       " 'll',\n",
       " 'y',\n",
       " 'Ä con',\n",
       " 'ce',\n",
       " 'a',\n",
       " 'led',\n",
       " ',',\n",
       " 'Ä we',\n",
       " 'a',\n",
       " 'k',\n",
       " 'ne',\n",
       " 'ss',\n",
       " 'Ä of',\n",
       " 'Ä my',\n",
       " 'Ä e',\n",
       " 'y',\n",
       " 'es',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "His facial aspect, too, was remarkable for its maturity; for though he shared his mother's and grandfather's chinlessness, his firm and precociously shaped nose united with the expression of his large, dark, almost Latin eyes to give him an air of quasi adulthood and well nigh preternatural intelligence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'is',\n",
       " 'Ä f',\n",
       " 'ac',\n",
       " 'i',\n",
       " 'al',\n",
       " 'Ä as',\n",
       " 'pe',\n",
       " 'ct',\n",
       " ',',\n",
       " 'Ä to',\n",
       " 'o',\n",
       " ',',\n",
       " 'Ä was',\n",
       " 'Ä re',\n",
       " 'm',\n",
       " 'ar',\n",
       " 'k',\n",
       " 'able',\n",
       " 'Ä for',\n",
       " 'Ä it',\n",
       " 's',\n",
       " 'Ä m',\n",
       " 'at',\n",
       " 'ur',\n",
       " 'ity',\n",
       " ';',\n",
       " 'Ä for',\n",
       " 'Ä th',\n",
       " 'ough',\n",
       " 'Ä he',\n",
       " 'Ä s',\n",
       " 'ha',\n",
       " 'red',\n",
       " 'Ä his',\n",
       " 'Ä m',\n",
       " 'ot',\n",
       " 'her',\n",
       " \"'s\",\n",
       " 'Ä and',\n",
       " 'Ä g',\n",
       " 'ra',\n",
       " 'nd',\n",
       " 'f',\n",
       " 'at',\n",
       " 'her',\n",
       " \"'s\",\n",
       " 'Ä c',\n",
       " 'h',\n",
       " 'in',\n",
       " 'le',\n",
       " 'ss',\n",
       " 'ne',\n",
       " 'ss',\n",
       " ',',\n",
       " 'Ä his',\n",
       " 'Ä f',\n",
       " 'ir',\n",
       " 'm',\n",
       " 'Ä and',\n",
       " 'Ä pre',\n",
       " 'c',\n",
       " 'o',\n",
       " 'c',\n",
       " 'i',\n",
       " 'ous',\n",
       " 'ly',\n",
       " 'Ä s',\n",
       " 'ha',\n",
       " 'p',\n",
       " 'ed',\n",
       " 'Ä no',\n",
       " 'se',\n",
       " 'Ä un',\n",
       " 'it',\n",
       " 'ed',\n",
       " 'Ä with',\n",
       " 'Ä the',\n",
       " 'Ä ex',\n",
       " 'p',\n",
       " 're',\n",
       " 'ss',\n",
       " 'ion',\n",
       " 'Ä of',\n",
       " 'Ä his',\n",
       " 'Ä l',\n",
       " 'ar',\n",
       " 'ge',\n",
       " ',',\n",
       " 'Ä d',\n",
       " 'ar',\n",
       " 'k',\n",
       " ',',\n",
       " 'Ä al',\n",
       " 'm',\n",
       " 'o',\n",
       " 'st',\n",
       " 'Ä l',\n",
       " 'at',\n",
       " 'in',\n",
       " 'Ä e',\n",
       " 'y',\n",
       " 'es',\n",
       " 'Ä to',\n",
       " 'Ä g',\n",
       " 'ive',\n",
       " 'Ä him',\n",
       " 'Ä an',\n",
       " 'Ä a',\n",
       " 'ir',\n",
       " 'Ä of',\n",
       " 'Ä ',\n",
       " 'qu',\n",
       " 'as',\n",
       " 'i',\n",
       " 'Ä a',\n",
       " 'd',\n",
       " 'ul',\n",
       " 'th',\n",
       " 'o',\n",
       " 'od',\n",
       " 'Ä and',\n",
       " 'Ä we',\n",
       " 'll',\n",
       " 'Ä n',\n",
       " 'i',\n",
       " 'gh',\n",
       " 'Ä pre',\n",
       " 'ter',\n",
       " 'n',\n",
       " 'at',\n",
       " 'ur',\n",
       " 'al',\n",
       " 'Ä int',\n",
       " 'e',\n",
       " 'll',\n",
       " 'ig',\n",
       " 'en',\n",
       " 'ce',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in spooky_train.head(20).iterrows():\n",
    "    print(row.text)\n",
    "    display(row.tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [tokenizer.encode(f'{separator_samples}{t}').tokens for t in spooky_train.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['############<',\n",
       " 'new',\n",
       " '_',\n",
       " 'sample',\n",
       " '>############',\n",
       " 'this',\n",
       " 'Ä pro',\n",
       " 'cess',\n",
       " ',',\n",
       " 'Ä however',\n",
       " ',',\n",
       " 'Ä afforded',\n",
       " 'Ä me',\n",
       " 'Ä no',\n",
       " 'Ä means',\n",
       " 'Ä of',\n",
       " 'Ä asc',\n",
       " 'ertain',\n",
       " 'ing',\n",
       " 'Ä the',\n",
       " 'Ä dimens',\n",
       " 'ions',\n",
       " 'Ä of',\n",
       " 'Ä my',\n",
       " 'Ä dun',\n",
       " 'ge',\n",
       " 'on',\n",
       " ';',\n",
       " 'Ä as',\n",
       " 'Ä i',\n",
       " 'Ä might',\n",
       " 'Ä make',\n",
       " 'Ä its',\n",
       " 'Ä circ',\n",
       " 'uit',\n",
       " ',',\n",
       " 'Ä and',\n",
       " 'Ä return',\n",
       " 'Ä to',\n",
       " 'Ä the',\n",
       " 'Ä point',\n",
       " 'Ä when',\n",
       " 'ce',\n",
       " 'Ä i',\n",
       " 'Ä set',\n",
       " 'Ä out',\n",
       " ',',\n",
       " 'Ä without',\n",
       " 'Ä being',\n",
       " 'Ä aware',\n",
       " 'Ä of',\n",
       " 'Ä the',\n",
       " 'Ä fact',\n",
       " ';',\n",
       " 'Ä so',\n",
       " 'Ä perfect',\n",
       " 'ly',\n",
       " 'Ä un',\n",
       " 'if',\n",
       " 'orm',\n",
       " 'Ä seemed',\n",
       " 'Ä the',\n",
       " 'Ä wall',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Regular Tokenizer to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KerasTokenizer(num_words=15000,\n",
    "                          lower=True,\n",
    "                          char_level=False)\n",
    "tokenizer.fit_on_texts(spooky_train.text)\n",
    "\n",
    "spooky_train['keras_token_ids'] = tokenizer.texts_to_sequences(spooky_train.text)\n",
    "spooky_train.keras_token_ids = [[min(s, tokenizer.num_words) for s in seq] for seq in spooky_train.keras_token_ids]\n",
    "\n",
    "spooky_test['keras_token_ids'] = tokenizer.texts_to_sequences(spooky_test.text)\n",
    "spooky_test.keras_token_ids = [[min(s, tokenizer.num_words) for s in seq] for seq in spooky_test.keras_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate a reasonable sequence's max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000       5.0\n",
       "0.111111      14.0\n",
       "0.222222      20.0\n",
       "0.333333      25.0\n",
       "0.444444      31.0\n",
       "0.555556      37.0\n",
       "0.666667      43.0\n",
       "0.777778      53.0\n",
       "0.888889      68.0\n",
       "1.000000    1081.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = pd.Series([len(t) for t in spooky_train.token_ids])\n",
    "\n",
    "lengths.quantile(np.linspace(0, 1, num=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforce a sequence's max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tokenizer` tokenizer\n",
    "max_length = 50\n",
    "\n",
    "spooky_train.token_ids = spooky_train.token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "spooky_test.token_ids = spooky_test.token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras tokenizer\n",
    "max_length = 50\n",
    "\n",
    "lengths = pd.Series([len(tokens) for tokens in spooky_train.keras_token_ids])\n",
    "spooky_train.keras_token_ids = spooky_train.keras_token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "\n",
    "spooky_test.token_ids = spooky_test.keras_token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_column = 'token_ids'\n",
    "\n",
    "assert token_column in ['token_ids', 'keras_token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(spooky_train[token_column].tolist())\n",
    "y_train = to_categorical(np.array(spooky_train.author_id.tolist()))\n",
    "\n",
    "x_test = np.array(spooky_test[token_column].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(len(x_train))\n",
    "x_train = x_train[permutation]\n",
    "y_train = y_train[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = tokenizer.get_vocab_size() if not isinstance(tokenizer, KerasTokenizer) else tokenizer.num_words\n",
    "\n",
    "# create model\n",
    "model = SequentialModel()\n",
    "model.add(Embedding(input_dim=num_words, output_dim=200))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sgt-Peppers\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13705 samples, validate on 5874 samples\n",
      "Epoch 1/10\n",
      "13705/13705 [==============================] - 34s 2ms/step - loss: 0.8769 - accuracy: 0.5731 - val_loss: 0.7038 - val_accuracy: 0.7433\n",
      "Epoch 2/10\n",
      "13705/13705 [==============================] - 33s 2ms/step - loss: 0.5751 - accuracy: 0.7780 - val_loss: 0.5932 - val_accuracy: 0.7622\n",
      "Epoch 3/10\n",
      "13705/13705 [==============================] - 35s 3ms/step - loss: 0.4477 - accuracy: 0.8336 - val_loss: 0.5662 - val_accuracy: 0.7828\n",
      "Epoch 4/10\n",
      "13705/13705 [==============================] - 33s 2ms/step - loss: 0.3798 - accuracy: 0.8698 - val_loss: 0.5904 - val_accuracy: 0.7802\n",
      "Epoch 5/10\n",
      "13705/13705 [==============================] - 32s 2ms/step - loss: 0.3238 - accuracy: 0.8900 - val_loss: 0.6532 - val_accuracy: 0.7702\n",
      "Epoch 6/10\n",
      "13705/13705 [==============================] - 44s 3ms/step - loss: 0.2872 - accuracy: 0.9036 - val_loss: 0.6719 - val_accuracy: 0.7659\n",
      "Epoch 7/10\n",
      "13705/13705 [==============================] - 47s 3ms/step - loss: 0.2426 - accuracy: 0.9183 - val_loss: 0.6755 - val_accuracy: 0.7775\n",
      "Epoch 8/10\n",
      "13705/13705 [==============================] - 40s 3ms/step - loss: 0.2144 - accuracy: 0.9298 - val_loss: 0.7403 - val_accuracy: 0.7782\n",
      "Epoch 9/10\n",
      "13705/13705 [==============================] - 34s 3ms/step - loss: 0.1825 - accuracy: 0.9429 - val_loss: 0.7782 - val_accuracy: 0.7712\n",
      "Epoch 10/10\n",
      "13705/13705 [==============================] - 34s 3ms/step - loss: 0.1562 - accuracy: 0.9510 - val_loss: 0.7822 - val_accuracy: 0.7683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23330860cc0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(x=x_train, \n",
    "          y=y_train,\n",
    "          validation_split=0.3,\n",
    "          batch_size=32,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Overview",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
