{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Overview<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Spooky-Author-Classification\" data-toc-modified-id=\"Spooky-Author-Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Spooky Author Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-Tokenizer-Input-Data\" data-toc-modified-id=\"Generate-Tokenizer-Input-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Generate Tokenizer Input Data</a></span></li><li><span><a href=\"#Generate-Tokenizer\" data-toc-modified-id=\"Generate-Tokenizer-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Generate Tokenizer</a></span></li><li><span><a href=\"#Tokenize-texts\" data-toc-modified-id=\"Tokenize-texts-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Tokenize texts</a></span></li><li><span><a href=\"#Add-a-Regular-Tokenizer-to-compare\" data-toc-modified-id=\"Add-a-Regular-Tokenizer-to-compare-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Add a Regular Tokenizer to compare</a></span></li></ul></li><li><span><a href=\"#Estimate-a-reasonable-sequence's-max_length\" data-toc-modified-id=\"Estimate-a-reasonable-sequence's-max_length-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Estimate a reasonable sequence's max_length</a></span><ul class=\"toc-item\"><li><span><a href=\"#Enforce-a-sequence's-max_length\" data-toc-modified-id=\"Enforce-a-sequence's-max_length-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Enforce a sequence's max_length</a></span></li><li><span><a href=\"#Create-train-/test-sets\" data-toc-modified-id=\"Create-train-/test-sets-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Create train-/test sets</a></span></li><li><span><a href=\"#Create-Model\" data-toc-modified-id=\"Create-Model-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Create Model</a></span></li><li><span><a href=\"#Fit-Model\" data-toc-modified-id=\"Fit-Model-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Fit Model</a></span></li><li><span><a href=\"#Evaluate-Model\" data-toc-modified-id=\"Evaluate-Model-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Evaluate Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Conv1D, LSTM, Dropout, Embedding, Layer\n",
    "from keras.models import Sequential as SequentialModel\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.insert(0, '../ct')\n",
    "\n",
    "import load\n",
    "from preprocess import preprocess\n",
    "from preprocess import Tokenizer\n",
    "from preprocess.preprocess import separator_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(f'current dir: {os.getcwd()}')\n",
    "# df = pd.read_pickle('../data/processed/treebank.pickle')\n",
    "\n",
    "# if not os.path.exists('../data/processed/treebank'):\n",
    "#     os.mkdir('../data/processed/treebank')\n",
    "\n",
    "# for i, text in enumerate(df.text):\n",
    "#     with open(f'../data/processed/treebank/{i + 1}', 'w') as file:\n",
    "#         file.write(text)\n",
    "        \n",
    "# with open('../data/processed/treebank.txt', 'w', ) as file:\n",
    "#     file.write(text)\n",
    "\n",
    "# tokenizer = Tokenizer(input_paths=['../data/input/treebank/raw_txt/treebank-utf8.txt'],\n",
    "#                       tokens_output_dir='../data/processed/treebank/',\n",
    "#                       tokenizer_output_path='../data/tokenizer/treebank',\n",
    "#                       lowercase=True,\n",
    "#                       vocab_size=5000)\n",
    "# display(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate Tokenizer Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spooky_train = pd.read_csv('../data/input/spooky-author/train.csv')\n",
    "spooky_test = pd.read_csv('../data/input/spooky-author/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>id17718</td>\n",
       "      <td>I could have fancied, while I looked at it, th...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>id08973</td>\n",
       "      <td>The lids clenched themselves together as if in...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>id05267</td>\n",
       "      <td>Mais il faut agir that is to say, a Frenchman ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>id17513</td>\n",
       "      <td>For an item of news like this, it strikes us i...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>id00393</td>\n",
       "      <td>He laid a gnarled claw on my shoulder, and it ...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19579 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author\n",
       "0      id26305  This process, however, afforded me no means of...    EAP\n",
       "1      id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2      id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3      id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4      id12958  Finding nothing else, not even gold, the Super...    HPL\n",
       "...        ...                                                ...    ...\n",
       "19574  id17718  I could have fancied, while I looked at it, th...    EAP\n",
       "19575  id08973  The lids clenched themselves together as if in...    EAP\n",
       "19576  id05267  Mais il faut agir that is to say, a Frenchman ...    EAP\n",
       "19577  id17513  For an item of news like this, it strikes us i...    EAP\n",
       "19578  id00393  He laid a gnarled claw on my shoulder, and it ...    HPL\n",
       "\n",
       "[19579 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spooky_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/processed/spooky-author'):\n",
    "    os.mkdir('../data/processed/spooky-author')\n",
    "\n",
    "x_train = f'{separator_samples}'.join(spooky_train.text)\n",
    "x_train = x_train.encode('utf-8', 'backslashreplace').decode('utf-8', 'backslashreplace')\n",
    "\n",
    "with open('../data/input/spooky-author/train.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(input_paths=['../data/input/spooky-author/train.txt'],\n",
    "                      tokenizer_output_path='../data/tokenizer/spooky-author',\n",
    "                      lowercase=True,\n",
    "                      vocab_size=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_train['encoding'] = tokenizer.encode_batch(spooky_train.text.tolist())\n",
    "spooky_test['encoding'] = tokenizer.encode_batch(spooky_test.text.tolist())\n",
    "\n",
    "spooky_train['tokens'] = spooky_train.encoding.apply(lambda e: e.tokens)\n",
    "spooky_train['token_ids'] = spooky_train.encoding.apply(lambda e: e.ids)\n",
    "\n",
    "spooky_test['tokens'] = spooky_test.encoding.apply(lambda e: e.tokens)\n",
    "spooky_test['token_ids'] = spooky_test.encoding.apply(lambda e: e.ids)\n",
    "\n",
    "author_to_id = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
    "spooky_train['author_id'] = spooky_train.author.apply(lambda a: author_to_id[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['th',\n",
       " 'is',\n",
       " 'Ġpro',\n",
       " 'c',\n",
       " 'ess',\n",
       " ',',\n",
       " 'Ġh',\n",
       " 'ow',\n",
       " 'e',\n",
       " 'ver',\n",
       " ',',\n",
       " 'Ġa',\n",
       " 'ff',\n",
       " 'or',\n",
       " 'd',\n",
       " 'ed',\n",
       " 'Ġme',\n",
       " 'Ġno',\n",
       " 'Ġme',\n",
       " 'an',\n",
       " 's',\n",
       " 'Ġof',\n",
       " 'Ġas',\n",
       " 'c',\n",
       " 'er',\n",
       " 't',\n",
       " 'ain',\n",
       " 'ing',\n",
       " 'Ġthe',\n",
       " 'Ġd',\n",
       " 'im',\n",
       " 'en',\n",
       " 's',\n",
       " 'ion',\n",
       " 's',\n",
       " 'Ġof',\n",
       " 'Ġmy',\n",
       " 'Ġd',\n",
       " 'u',\n",
       " 'n',\n",
       " 'ge',\n",
       " 'on',\n",
       " ';',\n",
       " 'Ġas',\n",
       " 'Ġi',\n",
       " 'Ġm',\n",
       " 'ight',\n",
       " 'Ġm',\n",
       " 'a',\n",
       " 'ke',\n",
       " 'Ġit',\n",
       " 's',\n",
       " 'Ġc',\n",
       " 'ir',\n",
       " 'c',\n",
       " 'u',\n",
       " 'it',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġre',\n",
       " 't',\n",
       " 'ur',\n",
       " 'n',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġpo',\n",
       " 'in',\n",
       " 't',\n",
       " 'Ġwhen',\n",
       " 'ce',\n",
       " 'Ġi',\n",
       " 'Ġs',\n",
       " 'et',\n",
       " 'Ġo',\n",
       " 'ut',\n",
       " ',',\n",
       " 'Ġwith',\n",
       " 'out',\n",
       " 'Ġbe',\n",
       " 'ing',\n",
       " 'Ġa',\n",
       " 'w',\n",
       " 'a',\n",
       " 're',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġf',\n",
       " 'a',\n",
       " 'ct',\n",
       " ';',\n",
       " 'Ġso',\n",
       " 'Ġper',\n",
       " 'f',\n",
       " 'ect',\n",
       " 'ly',\n",
       " 'Ġun',\n",
       " 'if',\n",
       " 'or',\n",
       " 'm',\n",
       " 'Ġse',\n",
       " 'em',\n",
       " 'ed',\n",
       " 'Ġthe',\n",
       " 'Ġw',\n",
       " 'all',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It never once occurred to me that the fumbling might be a mere mistake.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'Ġne',\n",
       " 'ver',\n",
       " 'Ġon',\n",
       " 'ce',\n",
       " 'Ġo',\n",
       " 'c',\n",
       " 'c',\n",
       " 'ur',\n",
       " 'red',\n",
       " 'Ġto',\n",
       " 'Ġme',\n",
       " 'Ġthat',\n",
       " 'Ġthe',\n",
       " 'Ġf',\n",
       " 'um',\n",
       " 'b',\n",
       " 'l',\n",
       " 'ing',\n",
       " 'Ġm',\n",
       " 'ight',\n",
       " 'Ġbe',\n",
       " 'Ġa',\n",
       " 'Ġme',\n",
       " 're',\n",
       " 'Ġm',\n",
       " 'ist',\n",
       " 'a',\n",
       " 'ke',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'Ġhis',\n",
       " 'Ġle',\n",
       " 'f',\n",
       " 't',\n",
       " 'Ġha',\n",
       " 'nd',\n",
       " 'Ġwas',\n",
       " 'Ġa',\n",
       " 'Ġg',\n",
       " 'o',\n",
       " 'ld',\n",
       " 'Ġs',\n",
       " 'n',\n",
       " 'u',\n",
       " 'ff',\n",
       " 'Ġb',\n",
       " 'o',\n",
       " 'x',\n",
       " ',',\n",
       " 'Ġfrom',\n",
       " 'Ġwhich',\n",
       " ',',\n",
       " 'Ġas',\n",
       " 'Ġhe',\n",
       " 'Ġc',\n",
       " 'a',\n",
       " 'pe',\n",
       " 'red',\n",
       " 'Ġd',\n",
       " 'ow',\n",
       " 'n',\n",
       " 'Ġthe',\n",
       " 'Ġh',\n",
       " 'ill',\n",
       " ',',\n",
       " 'Ġc',\n",
       " 'ut',\n",
       " 't',\n",
       " 'ing',\n",
       " 'Ġall',\n",
       " 'Ġman',\n",
       " 'ne',\n",
       " 'r',\n",
       " 'Ġof',\n",
       " 'Ġf',\n",
       " 'ant',\n",
       " 'ast',\n",
       " 'ic',\n",
       " 'Ġst',\n",
       " 'e',\n",
       " 'p',\n",
       " 's',\n",
       " ',',\n",
       " 'Ġhe',\n",
       " 'Ġto',\n",
       " 'o',\n",
       " 'k',\n",
       " 'Ġs',\n",
       " 'n',\n",
       " 'u',\n",
       " 'ff',\n",
       " 'Ġin',\n",
       " 'c',\n",
       " 'ess',\n",
       " 'ant',\n",
       " 'ly',\n",
       " 'Ġwith',\n",
       " 'Ġan',\n",
       " 'Ġa',\n",
       " 'ir',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġg',\n",
       " 'reat',\n",
       " 'est',\n",
       " 'Ġpo',\n",
       " 'ss',\n",
       " 'i',\n",
       " 'ble',\n",
       " 'Ġse',\n",
       " 'lf',\n",
       " 'Ġs',\n",
       " 'at',\n",
       " 'is',\n",
       " 'f',\n",
       " 'a',\n",
       " 'ct',\n",
       " 'ion',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'ow',\n",
       " 'Ġlo',\n",
       " 've',\n",
       " 'ly',\n",
       " 'Ġis',\n",
       " 'Ġsp',\n",
       " 'r',\n",
       " 'ing',\n",
       " 'Ġas',\n",
       " 'Ġwe',\n",
       " 'Ġlo',\n",
       " 'o',\n",
       " 'k',\n",
       " 'ed',\n",
       " 'Ġfrom',\n",
       " 'Ġw',\n",
       " 'ind',\n",
       " 's',\n",
       " 'or',\n",
       " 'Ġt',\n",
       " 'er',\n",
       " 'ra',\n",
       " 'ce',\n",
       " 'Ġon',\n",
       " 'Ġthe',\n",
       " 'Ġs',\n",
       " 'i',\n",
       " 'x',\n",
       " 't',\n",
       " 'e',\n",
       " 'en',\n",
       " 'Ġf',\n",
       " 'er',\n",
       " 't',\n",
       " 'i',\n",
       " 'le',\n",
       " 'Ġc',\n",
       " 'ou',\n",
       " 'nt',\n",
       " 'i',\n",
       " 'es',\n",
       " 'Ġsp',\n",
       " 're',\n",
       " 'ad',\n",
       " 'Ġbe',\n",
       " 'ne',\n",
       " 'at',\n",
       " 'h',\n",
       " ',',\n",
       " 'Ġs',\n",
       " 'pe',\n",
       " 'c',\n",
       " 'k',\n",
       " 'led',\n",
       " 'Ġby',\n",
       " 'Ġha',\n",
       " 'pp',\n",
       " 'y',\n",
       " 'Ġc',\n",
       " 'ot',\n",
       " 't',\n",
       " 'ag',\n",
       " 'es',\n",
       " 'Ġand',\n",
       " 'Ġwe',\n",
       " 'al',\n",
       " 'th',\n",
       " 'i',\n",
       " 'er',\n",
       " 'Ġto',\n",
       " 'w',\n",
       " 'n',\n",
       " 's',\n",
       " ',',\n",
       " 'Ġall',\n",
       " 'Ġlo',\n",
       " 'o',\n",
       " 'k',\n",
       " 'ed',\n",
       " 'Ġas',\n",
       " 'Ġin',\n",
       " 'Ġfor',\n",
       " 'm',\n",
       " 'er',\n",
       " 'Ġy',\n",
       " 'e',\n",
       " 'ar',\n",
       " 's',\n",
       " ',',\n",
       " 'Ġhe',\n",
       " 'art',\n",
       " 'Ġc',\n",
       " 'he',\n",
       " 'er',\n",
       " 'ing',\n",
       " 'Ġand',\n",
       " 'Ġf',\n",
       " 'a',\n",
       " 'ir',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f',\n",
       " 'ind',\n",
       " 'ing',\n",
       " 'Ġnot',\n",
       " 'h',\n",
       " 'ing',\n",
       " 'Ġe',\n",
       " 'l',\n",
       " 'se',\n",
       " ',',\n",
       " 'Ġnot',\n",
       " 'Ġe',\n",
       " 'ven',\n",
       " 'Ġg',\n",
       " 'o',\n",
       " 'ld',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'Ġsu',\n",
       " 'per',\n",
       " 'in',\n",
       " 't',\n",
       " 'e',\n",
       " 'nd',\n",
       " 'ent',\n",
       " 'Ġab',\n",
       " 'and',\n",
       " 'o',\n",
       " 'ned',\n",
       " 'Ġhis',\n",
       " 'Ġat',\n",
       " 't',\n",
       " 'em',\n",
       " 'pt',\n",
       " 's',\n",
       " ';',\n",
       " 'Ġbut',\n",
       " 'Ġa',\n",
       " 'Ġper',\n",
       " 'ple',\n",
       " 'x',\n",
       " 'ed',\n",
       " 'Ġlo',\n",
       " 'o',\n",
       " 'k',\n",
       " 'Ġo',\n",
       " 'c',\n",
       " 'c',\n",
       " 'as',\n",
       " 'ion',\n",
       " 'all',\n",
       " 'y',\n",
       " 'Ġst',\n",
       " 'e',\n",
       " 'al',\n",
       " 's',\n",
       " 'Ġo',\n",
       " 'ver',\n",
       " 'Ġhis',\n",
       " 'Ġc',\n",
       " 'ou',\n",
       " 'nt',\n",
       " 'en',\n",
       " 'an',\n",
       " 'ce',\n",
       " 'Ġas',\n",
       " 'Ġhe',\n",
       " 'Ġs',\n",
       " 'it',\n",
       " 's',\n",
       " 'Ġth',\n",
       " 'in',\n",
       " 'k',\n",
       " 'ing',\n",
       " 'Ġat',\n",
       " 'Ġhis',\n",
       " 'Ġd',\n",
       " 'es',\n",
       " 'k',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'Ġyou',\n",
       " 'th',\n",
       " 'Ġp',\n",
       " 'as',\n",
       " 's',\n",
       " 'ed',\n",
       " 'Ġin',\n",
       " 'Ġso',\n",
       " 'l',\n",
       " 'it',\n",
       " 'u',\n",
       " 'd',\n",
       " 'e',\n",
       " ',',\n",
       " 'Ġmy',\n",
       " 'Ġb',\n",
       " 'est',\n",
       " 'Ġy',\n",
       " 'e',\n",
       " 'ar',\n",
       " 's',\n",
       " 'Ġsp',\n",
       " 'ent',\n",
       " 'Ġu',\n",
       " 'nd',\n",
       " 'er',\n",
       " 'Ġyou',\n",
       " 'r',\n",
       " 'Ġg',\n",
       " 'ent',\n",
       " 'le',\n",
       " 'Ġand',\n",
       " 'Ġfe',\n",
       " 'm',\n",
       " 'in',\n",
       " 'in',\n",
       " 'e',\n",
       " 'Ġf',\n",
       " 'o',\n",
       " 'st',\n",
       " 'er',\n",
       " 'a',\n",
       " 'ge',\n",
       " ',',\n",
       " 'Ġha',\n",
       " 's',\n",
       " 'Ġso',\n",
       " 'Ġre',\n",
       " 'f',\n",
       " 'in',\n",
       " 'ed',\n",
       " 'Ġthe',\n",
       " 'Ġg',\n",
       " 'r',\n",
       " 'ound',\n",
       " 'w',\n",
       " 'or',\n",
       " 'k',\n",
       " 'Ġof',\n",
       " 'Ġmy',\n",
       " 'Ġc',\n",
       " 'ha',\n",
       " 'ra',\n",
       " 'ct',\n",
       " 'er',\n",
       " 'Ġthat',\n",
       " 'Ġi',\n",
       " 'Ġc',\n",
       " 'an',\n",
       " 'n',\n",
       " 'ot',\n",
       " 'Ġo',\n",
       " 'ver',\n",
       " 'c',\n",
       " 'ome',\n",
       " 'Ġan',\n",
       " 'Ġint',\n",
       " 'en',\n",
       " 'se',\n",
       " 'Ġdis',\n",
       " 't',\n",
       " 'ast',\n",
       " 'e',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġu',\n",
       " 's',\n",
       " 'u',\n",
       " 'al',\n",
       " 'Ġb',\n",
       " 'r',\n",
       " 'ut',\n",
       " 'al',\n",
       " 'ity',\n",
       " 'Ġex',\n",
       " 'er',\n",
       " 'c',\n",
       " 'is',\n",
       " 'ed',\n",
       " 'Ġon',\n",
       " 'Ġb',\n",
       " 'o',\n",
       " 'ard',\n",
       " 'Ġsh',\n",
       " 'i',\n",
       " 'p',\n",
       " ':',\n",
       " 'Ġi',\n",
       " 'Ġhave',\n",
       " 'Ġne',\n",
       " 'ver',\n",
       " 'Ġbe',\n",
       " 'l',\n",
       " 'ie',\n",
       " 'ved',\n",
       " 'Ġit',\n",
       " 'Ġto',\n",
       " 'Ġbe',\n",
       " 'Ġne',\n",
       " 'c',\n",
       " 'ess',\n",
       " 'ar',\n",
       " 'y',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġwhen',\n",
       " 'Ġi',\n",
       " 'Ġhe',\n",
       " 'ard',\n",
       " 'Ġof',\n",
       " 'Ġa',\n",
       " 'Ġm',\n",
       " 'ar',\n",
       " 'in',\n",
       " 'er',\n",
       " 'Ġe',\n",
       " 'qu',\n",
       " 'all',\n",
       " 'y',\n",
       " 'Ġnot',\n",
       " 'ed',\n",
       " 'Ġfor',\n",
       " 'Ġhis',\n",
       " 'Ġk',\n",
       " 'ind',\n",
       " 'l',\n",
       " 'in',\n",
       " 'ess',\n",
       " 'Ġof',\n",
       " 'Ġhe',\n",
       " 'art',\n",
       " 'Ġand',\n",
       " 'Ġthe',\n",
       " 'Ġre',\n",
       " 's',\n",
       " 'pe',\n",
       " 'ct',\n",
       " 'Ġand',\n",
       " 'Ġo',\n",
       " 'b',\n",
       " 'ed',\n",
       " 'i',\n",
       " 'en',\n",
       " 'ce',\n",
       " 'Ġp',\n",
       " 'a',\n",
       " 'id',\n",
       " 'Ġto',\n",
       " 'Ġhim',\n",
       " 'Ġby',\n",
       " 'Ġhis',\n",
       " 'Ġc',\n",
       " 're',\n",
       " 'w',\n",
       " ',',\n",
       " 'Ġi',\n",
       " 'Ġfe',\n",
       " 'l',\n",
       " 't',\n",
       " 'Ġmy',\n",
       " 'se',\n",
       " 'lf',\n",
       " 'Ġp',\n",
       " 'e',\n",
       " 'c',\n",
       " 'ul',\n",
       " 'i',\n",
       " 'ar',\n",
       " 'ly',\n",
       " 'Ġfor',\n",
       " 't',\n",
       " 'u',\n",
       " 'n',\n",
       " 'ate',\n",
       " 'Ġin',\n",
       " 'Ġbe',\n",
       " 'ing',\n",
       " 'Ġa',\n",
       " 'ble',\n",
       " 'Ġto',\n",
       " 'Ġse',\n",
       " 'c',\n",
       " 'ure',\n",
       " 'Ġhis',\n",
       " 'Ġs',\n",
       " 'er',\n",
       " 'v',\n",
       " 'ic',\n",
       " 'es',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'Ġa',\n",
       " 'st',\n",
       " 'r',\n",
       " 'on',\n",
       " 'om',\n",
       " 'er',\n",
       " ',',\n",
       " 'Ġper',\n",
       " 'ha',\n",
       " 'p',\n",
       " 's',\n",
       " ',',\n",
       " 'Ġat',\n",
       " 'Ġthis',\n",
       " 'Ġpo',\n",
       " 'in',\n",
       " 't',\n",
       " ',',\n",
       " 'Ġto',\n",
       " 'o',\n",
       " 'k',\n",
       " 'Ġre',\n",
       " 'f',\n",
       " 'u',\n",
       " 'ge',\n",
       " 'Ġin',\n",
       " 'Ġthe',\n",
       " 'Ġsu',\n",
       " 'g',\n",
       " 'g',\n",
       " 'est',\n",
       " 'ion',\n",
       " 'Ġof',\n",
       " 'Ġn',\n",
       " 'on',\n",
       " 'Ġl',\n",
       " 'um',\n",
       " 'in',\n",
       " 'o',\n",
       " 's',\n",
       " 'ity',\n",
       " ';',\n",
       " 'Ġand',\n",
       " 'Ġhe',\n",
       " 're',\n",
       " 'Ġan',\n",
       " 'al',\n",
       " 'o',\n",
       " 'g',\n",
       " 'y',\n",
       " 'Ġwas',\n",
       " 'Ġsu',\n",
       " 'd',\n",
       " 'd',\n",
       " 'en',\n",
       " 'ly',\n",
       " 'Ġle',\n",
       " 't',\n",
       " 'Ġf',\n",
       " 'all',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The surcingle hung in ribands from my body.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'Ġs',\n",
       " 'ur',\n",
       " 'c',\n",
       " 'ing',\n",
       " 'le',\n",
       " 'Ġh',\n",
       " 'u',\n",
       " 'n',\n",
       " 'g',\n",
       " 'Ġin',\n",
       " 'Ġr',\n",
       " 'i',\n",
       " 'b',\n",
       " 'and',\n",
       " 's',\n",
       " 'Ġfrom',\n",
       " 'Ġmy',\n",
       " 'Ġb',\n",
       " 'od',\n",
       " 'y',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'Ġk',\n",
       " 'new',\n",
       " 'Ġthat',\n",
       " 'Ġyou',\n",
       " 'Ġcould',\n",
       " 'Ġnot',\n",
       " 'Ġs',\n",
       " 'ay',\n",
       " 'Ġto',\n",
       " 'Ġyou',\n",
       " 'r',\n",
       " 'se',\n",
       " 'lf',\n",
       " 'Ġ',\n",
       " \"'\",\n",
       " 'st',\n",
       " 'e',\n",
       " 're',\n",
       " 'ot',\n",
       " 'om',\n",
       " 'y',\n",
       " \"'\",\n",
       " 'Ġwith',\n",
       " 'out',\n",
       " 'Ġbe',\n",
       " 'ing',\n",
       " 'Ġb',\n",
       " 'r',\n",
       " 'ou',\n",
       " 'ght',\n",
       " 'Ġto',\n",
       " 'Ġth',\n",
       " 'in',\n",
       " 'k',\n",
       " 'Ġof',\n",
       " 'Ġat',\n",
       " 'om',\n",
       " 'i',\n",
       " 'es',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġth',\n",
       " 'us',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġthe',\n",
       " 'or',\n",
       " 'i',\n",
       " 'es',\n",
       " 'Ġof',\n",
       " 'Ġe',\n",
       " 'p',\n",
       " 'ic',\n",
       " 'ur',\n",
       " 'us',\n",
       " ';',\n",
       " 'Ġand',\n",
       " 'Ġs',\n",
       " 'in',\n",
       " 'ce',\n",
       " ',',\n",
       " 'Ġwhen',\n",
       " 'Ġwe',\n",
       " 'Ġdis',\n",
       " 'c',\n",
       " 'u',\n",
       " 'ss',\n",
       " 'ed',\n",
       " 'Ġthis',\n",
       " 'Ġsu',\n",
       " 'b',\n",
       " 'j',\n",
       " 'ect',\n",
       " 'Ġnot',\n",
       " 'Ġ',\n",
       " 'very',\n",
       " 'Ġl',\n",
       " 'ong',\n",
       " 'Ġa',\n",
       " 'g',\n",
       " 'o',\n",
       " ',',\n",
       " 'Ġi',\n",
       " 'Ġm',\n",
       " 'ent',\n",
       " 'i',\n",
       " 'o',\n",
       " 'ned',\n",
       " 'Ġto',\n",
       " 'Ġyou',\n",
       " 'Ġh',\n",
       " 'ow',\n",
       " 'Ġs',\n",
       " 'ing',\n",
       " 'ul',\n",
       " 'ar',\n",
       " 'ly',\n",
       " ',',\n",
       " 'Ġy',\n",
       " 'et',\n",
       " 'Ġwith',\n",
       " 'Ġh',\n",
       " 'ow',\n",
       " 'Ġl',\n",
       " 'it',\n",
       " 't',\n",
       " 'le',\n",
       " 'Ġnot',\n",
       " 'ic',\n",
       " 'e',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'Ġv',\n",
       " 'ag',\n",
       " 'u',\n",
       " 'e',\n",
       " 'Ġg',\n",
       " 'u',\n",
       " 'ess',\n",
       " 'es',\n",
       " 'Ġof',\n",
       " 'Ġthat',\n",
       " 'Ġno',\n",
       " 'ble',\n",
       " 'Ġg',\n",
       " 're',\n",
       " 'e',\n",
       " 'k',\n",
       " 'Ġhad',\n",
       " 'Ġme',\n",
       " 't',\n",
       " 'Ġwith',\n",
       " 'Ġcon',\n",
       " 'f',\n",
       " 'ir',\n",
       " 'm',\n",
       " 'ation',\n",
       " 'Ġin',\n",
       " 'Ġthe',\n",
       " 'Ġl',\n",
       " 'ate',\n",
       " 'Ġne',\n",
       " 'b',\n",
       " 'ul',\n",
       " 'ar',\n",
       " 'Ġc',\n",
       " 'o',\n",
       " 's',\n",
       " 'm',\n",
       " 'o',\n",
       " 'g',\n",
       " 'on',\n",
       " 'y',\n",
       " ',',\n",
       " 'Ġi',\n",
       " 'Ġfe',\n",
       " 'l',\n",
       " 't',\n",
       " 'Ġthat',\n",
       " 'Ġyou',\n",
       " 'Ġcould',\n",
       " 'Ġnot',\n",
       " 'Ġa',\n",
       " 'v',\n",
       " 'o',\n",
       " 'id',\n",
       " 'Ġc',\n",
       " 'ast',\n",
       " 'ing',\n",
       " 'Ġyou',\n",
       " 'r',\n",
       " 'Ġe',\n",
       " 'y',\n",
       " 'es',\n",
       " 'Ġup',\n",
       " 'w',\n",
       " 'ard',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġg',\n",
       " 'reat',\n",
       " 'Ġne',\n",
       " 'b',\n",
       " 'ul',\n",
       " 'a',\n",
       " 'Ġin',\n",
       " 'Ġor',\n",
       " 'ion',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġi',\n",
       " 'Ġc',\n",
       " 'er',\n",
       " 't',\n",
       " 'ain',\n",
       " 'ly',\n",
       " 'Ġex',\n",
       " 'pe',\n",
       " 'ct',\n",
       " 'ed',\n",
       " 'Ġthat',\n",
       " 'Ġyou',\n",
       " 'Ġwould',\n",
       " 'Ġdo',\n",
       " 'Ġso',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'Ġcon',\n",
       " 'f',\n",
       " 'ess',\n",
       " 'Ġthat',\n",
       " 'Ġne',\n",
       " 'it',\n",
       " 'her',\n",
       " 'Ġthe',\n",
       " 'Ġst',\n",
       " 'r',\n",
       " 'u',\n",
       " 'ct',\n",
       " 'ure',\n",
       " 'Ġof',\n",
       " 'Ġl',\n",
       " 'an',\n",
       " 'g',\n",
       " 'u',\n",
       " 'ag',\n",
       " 'es',\n",
       " ',',\n",
       " 'Ġn',\n",
       " 'or',\n",
       " 'Ġthe',\n",
       " 'Ġc',\n",
       " 'od',\n",
       " 'e',\n",
       " 'Ġof',\n",
       " 'Ġg',\n",
       " 'o',\n",
       " 'ver',\n",
       " 'n',\n",
       " 'm',\n",
       " 'ent',\n",
       " 's',\n",
       " ',',\n",
       " 'Ġn',\n",
       " 'or',\n",
       " 'Ġthe',\n",
       " 'Ġp',\n",
       " 'ol',\n",
       " 'it',\n",
       " 'ic',\n",
       " 's',\n",
       " 'Ġof',\n",
       " 'Ġv',\n",
       " 'ar',\n",
       " 'i',\n",
       " 'ous',\n",
       " 'Ġst',\n",
       " 'at',\n",
       " 'es',\n",
       " 'Ġpo',\n",
       " 'ss',\n",
       " 'ess',\n",
       " 'ed',\n",
       " 'Ġat',\n",
       " 't',\n",
       " 'ra',\n",
       " 'ct',\n",
       " 'ion',\n",
       " 's',\n",
       " 'Ġfor',\n",
       " 'Ġme',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He shall find that I can feel my injuries; he shall learn to dread my revenge\" A few days after he arrived.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'Ġs',\n",
       " 'ha',\n",
       " 'll',\n",
       " 'Ġf',\n",
       " 'ind',\n",
       " 'Ġthat',\n",
       " 'Ġi',\n",
       " 'Ġc',\n",
       " 'an',\n",
       " 'Ġfe',\n",
       " 'e',\n",
       " 'l',\n",
       " 'Ġmy',\n",
       " 'Ġin',\n",
       " 'j',\n",
       " 'ur',\n",
       " 'i',\n",
       " 'es',\n",
       " ';',\n",
       " 'Ġhe',\n",
       " 'Ġs',\n",
       " 'ha',\n",
       " 'll',\n",
       " 'Ġle',\n",
       " 'ar',\n",
       " 'n',\n",
       " 'Ġto',\n",
       " 'Ġd',\n",
       " 're',\n",
       " 'ad',\n",
       " 'Ġmy',\n",
       " 'Ġre',\n",
       " 'ven',\n",
       " 'ge',\n",
       " '\"',\n",
       " 'Ġa',\n",
       " 'Ġfe',\n",
       " 'w',\n",
       " 'Ġd',\n",
       " 'ay',\n",
       " 's',\n",
       " 'Ġa',\n",
       " 'f',\n",
       " 'ter',\n",
       " 'Ġhe',\n",
       " 'Ġar',\n",
       " 'ri',\n",
       " 'ved',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here we barricaded ourselves, and, for the present were secure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 're',\n",
       " 'Ġwe',\n",
       " 'Ġb',\n",
       " 'ar',\n",
       " 'r',\n",
       " 'ic',\n",
       " 'ad',\n",
       " 'ed',\n",
       " 'Ġo',\n",
       " 'ur',\n",
       " 'se',\n",
       " 'l',\n",
       " 'v',\n",
       " 'es',\n",
       " ',',\n",
       " 'Ġand',\n",
       " ',',\n",
       " 'Ġfor',\n",
       " 'Ġthe',\n",
       " 'Ġpre',\n",
       " 's',\n",
       " 'ent',\n",
       " 'Ġwere',\n",
       " 'Ġse',\n",
       " 'c',\n",
       " 'ure',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Herbert West needed fresh bodies because his life work was the reanimation of the dead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['her',\n",
       " 'b',\n",
       " 'er',\n",
       " 't',\n",
       " 'Ġw',\n",
       " 'est',\n",
       " 'Ġne',\n",
       " 'ed',\n",
       " 'ed',\n",
       " 'Ġf',\n",
       " 're',\n",
       " 's',\n",
       " 'h',\n",
       " 'Ġb',\n",
       " 'od',\n",
       " 'i',\n",
       " 'es',\n",
       " 'Ġbe',\n",
       " 'c',\n",
       " 'a',\n",
       " 'u',\n",
       " 'se',\n",
       " 'Ġhis',\n",
       " 'Ġl',\n",
       " 'if',\n",
       " 'e',\n",
       " 'Ġwor',\n",
       " 'k',\n",
       " 'Ġwas',\n",
       " 'Ġthe',\n",
       " 'Ġre',\n",
       " 'an',\n",
       " 'im',\n",
       " 'ation',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġde',\n",
       " 'ad',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The farm like grounds extended back very deeply up the hill, almost to Wheaton Street.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'Ġf',\n",
       " 'ar',\n",
       " 'm',\n",
       " 'Ġl',\n",
       " 'i',\n",
       " 'ke',\n",
       " 'Ġg',\n",
       " 'r',\n",
       " 'ound',\n",
       " 's',\n",
       " 'Ġex',\n",
       " 't',\n",
       " 'e',\n",
       " 'nd',\n",
       " 'ed',\n",
       " 'Ġb',\n",
       " 'ac',\n",
       " 'k',\n",
       " 'Ġ',\n",
       " 'very',\n",
       " 'Ġde',\n",
       " 'e',\n",
       " 'p',\n",
       " 'ly',\n",
       " 'Ġup',\n",
       " 'Ġthe',\n",
       " 'Ġh',\n",
       " 'ill',\n",
       " ',',\n",
       " 'Ġal',\n",
       " 'm',\n",
       " 'o',\n",
       " 'st',\n",
       " 'Ġto',\n",
       " 'Ġwhe',\n",
       " 'at',\n",
       " 'on',\n",
       " 'Ġst',\n",
       " 're',\n",
       " 'et',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But a glance will show the fallacy of this idea.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'ut',\n",
       " 'Ġa',\n",
       " 'Ġg',\n",
       " 'l',\n",
       " 'an',\n",
       " 'ce',\n",
       " 'Ġw',\n",
       " 'ill',\n",
       " 'Ġsh',\n",
       " 'ow',\n",
       " 'Ġthe',\n",
       " 'Ġf',\n",
       " 'all',\n",
       " 'ac',\n",
       " 'y',\n",
       " 'Ġof',\n",
       " 'Ġthis',\n",
       " 'Ġi',\n",
       " 'd',\n",
       " 'e',\n",
       " 'a',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He had escaped me, and I must commence a destructive and almost endless journey across the mountainous ices of the ocean, amidst cold that few of the inhabitants could long endure and which I, the native of a genial and sunny climate, could not hope to survive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'Ġhad',\n",
       " 'Ġ',\n",
       " 'es',\n",
       " 'c',\n",
       " 'a',\n",
       " 'p',\n",
       " 'ed',\n",
       " 'Ġme',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġi',\n",
       " 'Ġm',\n",
       " 'ust',\n",
       " 'Ġcom',\n",
       " 'm',\n",
       " 'en',\n",
       " 'ce',\n",
       " 'Ġa',\n",
       " 'Ġd',\n",
       " 'est',\n",
       " 'r',\n",
       " 'u',\n",
       " 'ct',\n",
       " 'ive',\n",
       " 'Ġand',\n",
       " 'Ġal',\n",
       " 'm',\n",
       " 'o',\n",
       " 'st',\n",
       " 'Ġe',\n",
       " 'nd',\n",
       " 'le',\n",
       " 'ss',\n",
       " 'Ġj',\n",
       " 'our',\n",
       " 'ne',\n",
       " 'y',\n",
       " 'Ġa',\n",
       " 'c',\n",
       " 'ro',\n",
       " 'ss',\n",
       " 'Ġthe',\n",
       " 'Ġm',\n",
       " 'ou',\n",
       " 'nt',\n",
       " 'ain',\n",
       " 'ous',\n",
       " 'Ġi',\n",
       " 'c',\n",
       " 'es',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġo',\n",
       " 'ce',\n",
       " 'an',\n",
       " ',',\n",
       " 'Ġa',\n",
       " 'm',\n",
       " 'id',\n",
       " 'st',\n",
       " 'Ġc',\n",
       " 'o',\n",
       " 'ld',\n",
       " 'Ġthat',\n",
       " 'Ġfe',\n",
       " 'w',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġin',\n",
       " 'ha',\n",
       " 'b',\n",
       " 'it',\n",
       " 'ant',\n",
       " 's',\n",
       " 'Ġcould',\n",
       " 'Ġl',\n",
       " 'ong',\n",
       " 'Ġe',\n",
       " 'nd',\n",
       " 'ure',\n",
       " 'Ġand',\n",
       " 'Ġwhich',\n",
       " 'Ġi',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'Ġn',\n",
       " 'at',\n",
       " 'ive',\n",
       " 'Ġof',\n",
       " 'Ġa',\n",
       " 'Ġg',\n",
       " 'en',\n",
       " 'i',\n",
       " 'al',\n",
       " 'Ġand',\n",
       " 'Ġsu',\n",
       " 'n',\n",
       " 'n',\n",
       " 'y',\n",
       " 'Ġc',\n",
       " 'l',\n",
       " 'im',\n",
       " 'ate',\n",
       " ',',\n",
       " 'Ġcould',\n",
       " 'Ġnot',\n",
       " 'Ġh',\n",
       " 'o',\n",
       " 'pe',\n",
       " 'Ġto',\n",
       " 'Ġs',\n",
       " 'ur',\n",
       " 'v',\n",
       " 'ive',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To these speeches they gave, of course, their own interpretation; fancying, no doubt, that at all events I should come into possession of vast quantities of ready money; and provided I paid them all I owed, and a trifle more, in consideration of their services, I dare say they cared very little what became of either my soul or my carcass.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t',\n",
       " 'o',\n",
       " 'Ġthe',\n",
       " 'se',\n",
       " 'Ġs',\n",
       " 'pe',\n",
       " 'e',\n",
       " 'c',\n",
       " 'he',\n",
       " 's',\n",
       " 'Ġthe',\n",
       " 'y',\n",
       " 'Ġg',\n",
       " 'a',\n",
       " 've',\n",
       " ',',\n",
       " 'Ġof',\n",
       " 'Ġc',\n",
       " 'our',\n",
       " 'se',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'ir',\n",
       " 'Ġo',\n",
       " 'w',\n",
       " 'n',\n",
       " 'Ġin',\n",
       " 'ter',\n",
       " 'p',\n",
       " 're',\n",
       " 't',\n",
       " 'ation',\n",
       " ';',\n",
       " 'Ġf',\n",
       " 'an',\n",
       " 'c',\n",
       " 'y',\n",
       " 'ing',\n",
       " ',',\n",
       " 'Ġno',\n",
       " 'Ġd',\n",
       " 'ou',\n",
       " 'b',\n",
       " 't',\n",
       " ',',\n",
       " 'Ġthat',\n",
       " 'Ġat',\n",
       " 'Ġall',\n",
       " 'Ġe',\n",
       " 'v',\n",
       " 'ent',\n",
       " 's',\n",
       " 'Ġi',\n",
       " 'Ġsh',\n",
       " 'ould',\n",
       " 'Ġc',\n",
       " 'ome',\n",
       " 'Ġint',\n",
       " 'o',\n",
       " 'Ġpo',\n",
       " 'ss',\n",
       " 'ess',\n",
       " 'ion',\n",
       " 'Ġof',\n",
       " 'Ġv',\n",
       " 'ast',\n",
       " 'Ġ',\n",
       " 'qu',\n",
       " 'ant',\n",
       " 'it',\n",
       " 'i',\n",
       " 'es',\n",
       " 'Ġof',\n",
       " 'Ġre',\n",
       " 'ad',\n",
       " 'y',\n",
       " 'Ġmo',\n",
       " 'ne',\n",
       " 'y',\n",
       " ';',\n",
       " 'Ġand',\n",
       " 'Ġpro',\n",
       " 'v',\n",
       " 'id',\n",
       " 'ed',\n",
       " 'Ġi',\n",
       " 'Ġp',\n",
       " 'a',\n",
       " 'id',\n",
       " 'Ġthe',\n",
       " 'm',\n",
       " 'Ġall',\n",
       " 'Ġi',\n",
       " 'Ġo',\n",
       " 'w',\n",
       " 'ed',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġa',\n",
       " 'Ġt',\n",
       " 'ri',\n",
       " 'f',\n",
       " 'le',\n",
       " 'Ġm',\n",
       " 'ore',\n",
       " ',',\n",
       " 'Ġin',\n",
       " 'Ġcon',\n",
       " 's',\n",
       " 'id',\n",
       " 'er',\n",
       " 'ation',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'ir',\n",
       " 'Ġs',\n",
       " 'er',\n",
       " 'v',\n",
       " 'ic',\n",
       " 'es',\n",
       " ',',\n",
       " 'Ġi',\n",
       " 'Ġd',\n",
       " 'a',\n",
       " 're',\n",
       " 'Ġs',\n",
       " 'ay',\n",
       " 'Ġthe',\n",
       " 'y',\n",
       " 'Ġc',\n",
       " 'a',\n",
       " 'red',\n",
       " 'Ġ',\n",
       " 'very',\n",
       " 'Ġl',\n",
       " 'it',\n",
       " 't',\n",
       " 'le',\n",
       " 'Ġw',\n",
       " 'hat',\n",
       " 'Ġbe',\n",
       " 'c',\n",
       " 'ame',\n",
       " 'Ġof',\n",
       " 'Ġe',\n",
       " 'it',\n",
       " 'her',\n",
       " 'Ġmy',\n",
       " 'Ġs',\n",
       " 'ou',\n",
       " 'l',\n",
       " 'Ġor',\n",
       " 'Ġmy',\n",
       " 'Ġc',\n",
       " 'ar',\n",
       " 'c',\n",
       " 'as',\n",
       " 's',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Her native sprightliness needed no undue excitement, and her placid heart reposed contented on my love, the well being of her children, and the beauty of surrounding nature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['her',\n",
       " 'Ġn',\n",
       " 'at',\n",
       " 'ive',\n",
       " 'Ġsp',\n",
       " 'r',\n",
       " 'ight',\n",
       " 'l',\n",
       " 'in',\n",
       " 'ess',\n",
       " 'Ġne',\n",
       " 'ed',\n",
       " 'ed',\n",
       " 'Ġno',\n",
       " 'Ġu',\n",
       " 'nd',\n",
       " 'u',\n",
       " 'e',\n",
       " 'Ġex',\n",
       " 'c',\n",
       " 'it',\n",
       " 'em',\n",
       " 'ent',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġher',\n",
       " 'Ġpl',\n",
       " 'ac',\n",
       " 'id',\n",
       " 'Ġhe',\n",
       " 'art',\n",
       " 'Ġre',\n",
       " 'p',\n",
       " 'o',\n",
       " 's',\n",
       " 'ed',\n",
       " 'Ġcon',\n",
       " 't',\n",
       " 'ent',\n",
       " 'ed',\n",
       " 'Ġon',\n",
       " 'Ġmy',\n",
       " 'Ġlo',\n",
       " 've',\n",
       " ',',\n",
       " 'Ġthe',\n",
       " 'Ġwe',\n",
       " 'll',\n",
       " 'Ġbe',\n",
       " 'ing',\n",
       " 'Ġof',\n",
       " 'Ġher',\n",
       " 'Ġc',\n",
       " 'h',\n",
       " 'i',\n",
       " 'ld',\n",
       " 're',\n",
       " 'n',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġthe',\n",
       " 'Ġbe',\n",
       " 'a',\n",
       " 'ut',\n",
       " 'y',\n",
       " 'Ġof',\n",
       " 'Ġs',\n",
       " 'ur',\n",
       " 'r',\n",
       " 'ound',\n",
       " 'ing',\n",
       " 'Ġn',\n",
       " 'at',\n",
       " 'ure',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I even went so far as to speak of a slightly hectic cough with which, at one time, I had been troubled of a chronic rheumatism of a twinge of hereditary gout and, in conclusion, of the disagreeable and inconvenient, but hitherto carefully concealed, weakness of my eyes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'Ġe',\n",
       " 'ven',\n",
       " 'Ġw',\n",
       " 'ent',\n",
       " 'Ġso',\n",
       " 'Ġf',\n",
       " 'ar',\n",
       " 'Ġas',\n",
       " 'Ġto',\n",
       " 'Ġs',\n",
       " 'pe',\n",
       " 'a',\n",
       " 'k',\n",
       " 'Ġof',\n",
       " 'Ġa',\n",
       " 'Ġs',\n",
       " 'l',\n",
       " 'ight',\n",
       " 'ly',\n",
       " 'Ġhe',\n",
       " 'ct',\n",
       " 'ic',\n",
       " 'Ġc',\n",
       " 'ough',\n",
       " 'Ġwith',\n",
       " 'Ġwhich',\n",
       " ',',\n",
       " 'Ġat',\n",
       " 'Ġone',\n",
       " 'Ġt',\n",
       " 'im',\n",
       " 'e',\n",
       " ',',\n",
       " 'Ġi',\n",
       " 'Ġhad',\n",
       " 'Ġbeen',\n",
       " 'Ġt',\n",
       " 'r',\n",
       " 'ou',\n",
       " 'ble',\n",
       " 'd',\n",
       " 'Ġof',\n",
       " 'Ġa',\n",
       " 'Ġc',\n",
       " 'h',\n",
       " 'r',\n",
       " 'on',\n",
       " 'ic',\n",
       " 'Ġr',\n",
       " 'he',\n",
       " 'um',\n",
       " 'at',\n",
       " 'is',\n",
       " 'm',\n",
       " 'Ġof',\n",
       " 'Ġa',\n",
       " 'Ġt',\n",
       " 'w',\n",
       " 'ing',\n",
       " 'e',\n",
       " 'Ġof',\n",
       " 'Ġhe',\n",
       " 'red',\n",
       " 'it',\n",
       " 'ar',\n",
       " 'y',\n",
       " 'Ġg',\n",
       " 'out',\n",
       " 'Ġand',\n",
       " ',',\n",
       " 'Ġin',\n",
       " 'Ġcon',\n",
       " 'c',\n",
       " 'l',\n",
       " 'us',\n",
       " 'ion',\n",
       " ',',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġdis',\n",
       " 'ag',\n",
       " 're',\n",
       " 'e',\n",
       " 'able',\n",
       " 'Ġand',\n",
       " 'Ġin',\n",
       " 'c',\n",
       " 'on',\n",
       " 'ven',\n",
       " 'i',\n",
       " 'ent',\n",
       " ',',\n",
       " 'Ġbut',\n",
       " 'Ġh',\n",
       " 'it',\n",
       " 'her',\n",
       " 't',\n",
       " 'o',\n",
       " 'Ġc',\n",
       " 'a',\n",
       " 're',\n",
       " 'f',\n",
       " 'u',\n",
       " 'll',\n",
       " 'y',\n",
       " 'Ġcon',\n",
       " 'ce',\n",
       " 'a',\n",
       " 'led',\n",
       " ',',\n",
       " 'Ġwe',\n",
       " 'a',\n",
       " 'k',\n",
       " 'ne',\n",
       " 'ss',\n",
       " 'Ġof',\n",
       " 'Ġmy',\n",
       " 'Ġe',\n",
       " 'y',\n",
       " 'es',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "His facial aspect, too, was remarkable for its maturity; for though he shared his mother's and grandfather's chinlessness, his firm and precociously shaped nose united with the expression of his large, dark, almost Latin eyes to give him an air of quasi adulthood and well nigh preternatural intelligence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'is',\n",
       " 'Ġf',\n",
       " 'ac',\n",
       " 'i',\n",
       " 'al',\n",
       " 'Ġas',\n",
       " 'pe',\n",
       " 'ct',\n",
       " ',',\n",
       " 'Ġto',\n",
       " 'o',\n",
       " ',',\n",
       " 'Ġwas',\n",
       " 'Ġre',\n",
       " 'm',\n",
       " 'ar',\n",
       " 'k',\n",
       " 'able',\n",
       " 'Ġfor',\n",
       " 'Ġit',\n",
       " 's',\n",
       " 'Ġm',\n",
       " 'at',\n",
       " 'ur',\n",
       " 'ity',\n",
       " ';',\n",
       " 'Ġfor',\n",
       " 'Ġth',\n",
       " 'ough',\n",
       " 'Ġhe',\n",
       " 'Ġs',\n",
       " 'ha',\n",
       " 'red',\n",
       " 'Ġhis',\n",
       " 'Ġm',\n",
       " 'ot',\n",
       " 'her',\n",
       " \"'s\",\n",
       " 'Ġand',\n",
       " 'Ġg',\n",
       " 'ra',\n",
       " 'nd',\n",
       " 'f',\n",
       " 'at',\n",
       " 'her',\n",
       " \"'s\",\n",
       " 'Ġc',\n",
       " 'h',\n",
       " 'in',\n",
       " 'le',\n",
       " 'ss',\n",
       " 'ne',\n",
       " 'ss',\n",
       " ',',\n",
       " 'Ġhis',\n",
       " 'Ġf',\n",
       " 'ir',\n",
       " 'm',\n",
       " 'Ġand',\n",
       " 'Ġpre',\n",
       " 'c',\n",
       " 'o',\n",
       " 'c',\n",
       " 'i',\n",
       " 'ous',\n",
       " 'ly',\n",
       " 'Ġs',\n",
       " 'ha',\n",
       " 'p',\n",
       " 'ed',\n",
       " 'Ġno',\n",
       " 'se',\n",
       " 'Ġun',\n",
       " 'it',\n",
       " 'ed',\n",
       " 'Ġwith',\n",
       " 'Ġthe',\n",
       " 'Ġex',\n",
       " 'p',\n",
       " 're',\n",
       " 'ss',\n",
       " 'ion',\n",
       " 'Ġof',\n",
       " 'Ġhis',\n",
       " 'Ġl',\n",
       " 'ar',\n",
       " 'ge',\n",
       " ',',\n",
       " 'Ġd',\n",
       " 'ar',\n",
       " 'k',\n",
       " ',',\n",
       " 'Ġal',\n",
       " 'm',\n",
       " 'o',\n",
       " 'st',\n",
       " 'Ġl',\n",
       " 'at',\n",
       " 'in',\n",
       " 'Ġe',\n",
       " 'y',\n",
       " 'es',\n",
       " 'Ġto',\n",
       " 'Ġg',\n",
       " 'ive',\n",
       " 'Ġhim',\n",
       " 'Ġan',\n",
       " 'Ġa',\n",
       " 'ir',\n",
       " 'Ġof',\n",
       " 'Ġ',\n",
       " 'qu',\n",
       " 'as',\n",
       " 'i',\n",
       " 'Ġa',\n",
       " 'd',\n",
       " 'ul',\n",
       " 'th',\n",
       " 'o',\n",
       " 'od',\n",
       " 'Ġand',\n",
       " 'Ġwe',\n",
       " 'll',\n",
       " 'Ġn',\n",
       " 'i',\n",
       " 'gh',\n",
       " 'Ġpre',\n",
       " 'ter',\n",
       " 'n',\n",
       " 'at',\n",
       " 'ur',\n",
       " 'al',\n",
       " 'Ġint',\n",
       " 'e',\n",
       " 'll',\n",
       " 'ig',\n",
       " 'en',\n",
       " 'ce',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in spooky_train.head(20).iterrows():\n",
    "    print(row.text)\n",
    "    display(row.tokens)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [tokenizer.encode(f'{separator_samples}{t}').tokens for t in spooky_train.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['############<',\n",
       " 'new',\n",
       " '_',\n",
       " 'sample',\n",
       " '>############',\n",
       " 'this',\n",
       " 'Ġpro',\n",
       " 'cess',\n",
       " ',',\n",
       " 'Ġhowever',\n",
       " ',',\n",
       " 'Ġafforded',\n",
       " 'Ġme',\n",
       " 'Ġno',\n",
       " 'Ġmeans',\n",
       " 'Ġof',\n",
       " 'Ġasc',\n",
       " 'ertain',\n",
       " 'ing',\n",
       " 'Ġthe',\n",
       " 'Ġdimens',\n",
       " 'ions',\n",
       " 'Ġof',\n",
       " 'Ġmy',\n",
       " 'Ġdun',\n",
       " 'ge',\n",
       " 'on',\n",
       " ';',\n",
       " 'Ġas',\n",
       " 'Ġi',\n",
       " 'Ġmight',\n",
       " 'Ġmake',\n",
       " 'Ġits',\n",
       " 'Ġcirc',\n",
       " 'uit',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġreturn',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġpoint',\n",
       " 'Ġwhen',\n",
       " 'ce',\n",
       " 'Ġi',\n",
       " 'Ġset',\n",
       " 'Ġout',\n",
       " ',',\n",
       " 'Ġwithout',\n",
       " 'Ġbeing',\n",
       " 'Ġaware',\n",
       " 'Ġof',\n",
       " 'Ġthe',\n",
       " 'Ġfact',\n",
       " ';',\n",
       " 'Ġso',\n",
       " 'Ġperfect',\n",
       " 'ly',\n",
       " 'Ġun',\n",
       " 'if',\n",
       " 'orm',\n",
       " 'Ġseemed',\n",
       " 'Ġthe',\n",
       " 'Ġwall',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a Regular Tokenizer to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KerasTokenizer(num_words=15000,\n",
    "                          lower=True,\n",
    "                          char_level=False)\n",
    "tokenizer.fit_on_texts(spooky_train.text)\n",
    "\n",
    "spooky_train['keras_token_ids'] = tokenizer.texts_to_sequences(spooky_train.text)\n",
    "spooky_train.keras_token_ids = [[min(s, tokenizer.num_words) for s in seq] for seq in spooky_train.keras_token_ids]\n",
    "\n",
    "spooky_test['keras_token_ids'] = tokenizer.texts_to_sequences(spooky_test.text)\n",
    "spooky_test.keras_token_ids = [[min(s, tokenizer.num_words) for s in seq] for seq in spooky_test.keras_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate a reasonable sequence's max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000       5.0\n",
       "0.111111      14.0\n",
       "0.222222      20.0\n",
       "0.333333      25.0\n",
       "0.444444      31.0\n",
       "0.555556      37.0\n",
       "0.666667      43.0\n",
       "0.777778      53.0\n",
       "0.888889      68.0\n",
       "1.000000    1081.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = pd.Series([len(t) for t in spooky_train.token_ids])\n",
    "\n",
    "lengths.quantile(np.linspace(0, 1, num=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforce a sequence's max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tokenizer` tokenizer\n",
    "max_length = 50\n",
    "\n",
    "spooky_train.token_ids = spooky_train.token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "spooky_test.token_ids = spooky_test.token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras tokenizer\n",
    "max_length = 50\n",
    "\n",
    "lengths = pd.Series([len(tokens) for tokens in spooky_train.keras_token_ids])\n",
    "spooky_train.keras_token_ids = spooky_train.keras_token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "\n",
    "spooky_test.token_ids = spooky_test.keras_token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_column = 'token_ids'\n",
    "\n",
    "assert token_column in ['token_ids', 'keras_token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(spooky_train[token_column].tolist())\n",
    "y_train = to_categorical(np.array(spooky_train.author_id.tolist()))\n",
    "\n",
    "x_test = np.array(spooky_test[token_column].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(len(x_train))\n",
    "x_train = x_train[permutation]\n",
    "y_train = y_train[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = tokenizer.get_vocab_size() if not isinstance(tokenizer, KerasTokenizer) else tokenizer.num_words\n",
    "\n",
    "# create model\n",
    "model = SequentialModel()\n",
    "model.add(Embedding(input_dim=num_words, output_dim=200))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sgt-Peppers\\Anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13705 samples, validate on 5874 samples\n",
      "Epoch 1/10\n",
      "13705/13705 [==============================] - 34s 2ms/step - loss: 0.8769 - accuracy: 0.5731 - val_loss: 0.7038 - val_accuracy: 0.7433\n",
      "Epoch 2/10\n",
      "13705/13705 [==============================] - 33s 2ms/step - loss: 0.5751 - accuracy: 0.7780 - val_loss: 0.5932 - val_accuracy: 0.7622\n",
      "Epoch 3/10\n",
      "13705/13705 [==============================] - 35s 3ms/step - loss: 0.4477 - accuracy: 0.8336 - val_loss: 0.5662 - val_accuracy: 0.7828\n",
      "Epoch 4/10\n",
      "13705/13705 [==============================] - 33s 2ms/step - loss: 0.3798 - accuracy: 0.8698 - val_loss: 0.5904 - val_accuracy: 0.7802\n",
      "Epoch 5/10\n",
      "13705/13705 [==============================] - 32s 2ms/step - loss: 0.3238 - accuracy: 0.8900 - val_loss: 0.6532 - val_accuracy: 0.7702\n",
      "Epoch 6/10\n",
      "13705/13705 [==============================] - 44s 3ms/step - loss: 0.2872 - accuracy: 0.9036 - val_loss: 0.6719 - val_accuracy: 0.7659\n",
      "Epoch 7/10\n",
      "13705/13705 [==============================] - 47s 3ms/step - loss: 0.2426 - accuracy: 0.9183 - val_loss: 0.6755 - val_accuracy: 0.7775\n",
      "Epoch 8/10\n",
      "13705/13705 [==============================] - 40s 3ms/step - loss: 0.2144 - accuracy: 0.9298 - val_loss: 0.7403 - val_accuracy: 0.7782\n",
      "Epoch 9/10\n",
      "13705/13705 [==============================] - 34s 3ms/step - loss: 0.1825 - accuracy: 0.9429 - val_loss: 0.7782 - val_accuracy: 0.7712\n",
      "Epoch 10/10\n",
      "13705/13705 [==============================] - 34s 3ms/step - loss: 0.1562 - accuracy: 0.9510 - val_loss: 0.7822 - val_accuracy: 0.7683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23330860cc0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(x=x_train, \n",
    "          y=y_train,\n",
    "          validation_split=0.3,\n",
    "          batch_size=32,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Overview",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
