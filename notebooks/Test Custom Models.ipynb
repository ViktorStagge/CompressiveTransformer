{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Overview<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Load the Data</a></span></li><li><span><a href=\"#Create-Model\" data-toc-modified-id=\"Create-Model-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Create Model</a></span></li><li><span><a href=\"#Compile-Model\" data-toc-modified-id=\"Compile-Model-0.3\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>Compile Model</a></span></li><li><span><a href=\"#Fit-Model\" data-toc-modified-id=\"Fit-Model-0.4\"><span class=\"toc-item-num\">0.4&nbsp;&nbsp;</span>Fit Model</a></span></li></ul></li><li><span><a href=\"#Terminal\" data-toc-modified-id=\"Terminal-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Terminal</a></span></li><li><span><a href=\"#Test-on-English-next-char-prediction\" data-toc-modified-id=\"Test-on-English-next-char-prediction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Test on English next-char prediction</a></span></li><li><span><a href=\"#Detour-Get-everything-underneath-to-work-again\" data-toc-modified-id=\"Detour-Get-everything-underneath-to-work-again-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Detour Get everything underneath to work again</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#SDPA-attention-model\" data-toc-modified-id=\"SDPA-attention-model-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>SDPA attention model</a></span></li><li><span><a href=\"#MHA-model\" data-toc-modified-id=\"MHA-model-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>MHA model</a></span></li><li><span><a href=\"#Compression-model\" data-toc-modified-id=\"Compression-model-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Compression model</a></span></li></ul></li></ul></li><li><span><a href=\"#CT-model,-v.2\" data-toc-modified-id=\"CT-model,-v.2-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CT model, v.2</a></span></li><li><span><a href=\"#garbage\" data-toc-modified-id=\"garbage-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>garbage</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep\n",
    "from keras import backend as K\n",
    "from keras.models import Model \n",
    "from keras.models import Sequential as SequentialModel\n",
    "from keras.layers import Dense, Conv1D, LSTM, Dropout, Embedding, Layer, Input, Flatten, concatenate as Concatenate, Lambda, Add\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from sometimer import timer, time_this_method\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "sys.path.insert(0, '../ct')\n",
    "\n",
    "import load\n",
    "from preprocess import preprocess\n",
    "from preprocess import Tokenizer\n",
    "from preprocess.preprocess import separator_samples\n",
    "\n",
    "from model.layers import LayerNormalization\n",
    "from model.layers import ContentBasedAttention_CT\n",
    "from model.layers import ScaledDotProductAttention\n",
    "from model.layers import MultiHeadAttention\n",
    "from model.layers import content_based_attention\n",
    "\n",
    "from model import CompressiveTransformer\n",
    "from model import AttentionReconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('../data/processed/spooky-author/train.pkl')\n",
    "\n",
    "x_train = np.array(train_data.x.tolist())\n",
    "y_train = np.array(train_data.y.tolist())\n",
    "\n",
    "_x_train = np.zeros((x_train.shape[0], 128))  # samples, d_model\n",
    "_x_train[:,:x_train.shape[1]] = x_train\n",
    "x_train = _x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "sequence_length = 128\n",
    "\n",
    "model = CompressiveTransformer(d_model=d_model, sequence_length=sequence_length, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# redo everyth\n",
    "model = CompressiveTransformer(d_model=d_model, sequence_length=sequence_length, batch_size=1)\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "for batch in range(0, len(x_train), 1):\n",
    "    x_batch = x_train[batch:batch+1, :]\n",
    "    y_batch = y_train[batch:batch+1]\n",
    "    \n",
    "    model.train_on_batch(x_batch,\n",
    "                         y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = K.variable(np.array([i for i in range(200)]).reshape((2, 5, 20)))\n",
    "\n",
    "def call(x, units=None, gain=None, bias=None):\n",
    "    if units is None:\n",
    "        units = np.prod(x.shape[1:])\n",
    "        print(f'units={units}')\n",
    "    \n",
    "    mean = K.sum(x) / units\n",
    "    std_dev = K.sqrt(K.sum(K.square(x - mean)) / units)\n",
    "\n",
    "    y = (x - mean) / std_dev\n",
    "    if gain:\n",
    "        y *= gain\n",
    "    if bias:\n",
    "        y += bias\n",
    "    return y\n",
    "\n",
    "K.eval(call(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test on English next-char prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_paths_tokenizer = ['..\\\\data\\\\wma-en-de\\\\input\\\\train-en-ascii.txt',\n",
    "                        # '..\\\\data\\\\wma-en-de\\\\input\\\\train-de-ascii.txt'\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(input_paths=input_paths_tokenizer)\n",
    "\n",
    "data = []\n",
    "for path in input_paths_tokenizer:\n",
    "    with open(path) as file:\n",
    "        data.extend(file.readlines())\n",
    "df_ = pd.DataFrame(data={'text': data})\n",
    "df = df_[:10000]\n",
    "\n",
    "df['encoding'] = tokenizer.encode_batch(df.text.tolist())\n",
    "df['tokens'] = df.encoding.apply(lambda e: e.tokens)\n",
    "df['token_ids'] = df.encoding.apply(lambda e: e.ids)\n",
    "\n",
    "token_ids = [t for tokens in df.token_ids for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "sequence_length = 128\n",
    "\n",
    "model = CompressiveTransformer(d_model=d_model, sequence_length=sequence_length, batch_size=1)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(input_data, epochs=1, batch_size=1, d_model=128):\n",
    "    chunk_size = len(input_data) // batch_size\n",
    "    data = [input_data[i:i+chunk_size] for i in range(0, len(input_data), chunk_size)]\n",
    "    data = [[token for s in chunk for token in s] for chunk in data]\n",
    "    sample_size = len(data[0])\n",
    "    print(len(data))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(f'epoch: {e:5}')\n",
    "        for i in tqdm(range(chunk_size)):\n",
    "            print(f'       {e:5}-{i}')\n",
    "            x = [t[i:i+d_model] for t in data]\n",
    "            y = [t[i+d_model] for t in data]\n",
    "            \n",
    "            x = np.array(x)\n",
    "            y = np.array(y)\n",
    "            \n",
    "            y = keras.utils.to_categorical(y, num_classes=20000)\n",
    "            \n",
    "            print(y)\n",
    "            \n",
    "            model.train_on_batch(x=x,\n",
    "                                 y=y)\n",
    "            \n",
    "batch_generator(df.token_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detour Get everything underneath to work again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SDPA attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('../data/processed/spooky-author/train.pkl')\n",
    "\n",
    "x_train = np.array(train_data.x.tolist())\n",
    "y_train = np.array(train_data.y.tolist())\n",
    "\n",
    "_x_train = np.zeros((x_train.shape[0], 128))  # samples, d_model\n",
    "_x_train[:,:x_train.shape[1]] = x_train\n",
    "x_train = _x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_sdpa_model(d_model=128, compile=True):\n",
    "    x = Input(shape=(128,))\n",
    "    embed = Embedding(input_dim=20000, output_dim=128)(x)\n",
    "    sdpa = ScaledDotProductAttention(d_model=d_model, d_k=16, d_v=16, verbose=True)(embed)\n",
    "    flat = Flatten()(sdpa)\n",
    "    dense = Dense(units=10)(flat)\n",
    "    y = Dense(units=3, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[x],\n",
    "                  outputs=[y])\n",
    "    if compile:\n",
    "        model.compile(optimizer='Adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    return model, sdpa\n",
    "\n",
    "\n",
    "model, _ = create_sdpa_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Detour sample submission on Spooky-author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spooky_train = pd.read_csv('../data/input/spooky-author/train.csv')\n",
    "spooky_test = pd.read_csv('../data/input/spooky-author/test.csv')\n",
    "\n",
    "max_length = 128\n",
    "tokens = 'keras_token_ids'\n",
    "\n",
    "\n",
    "tokenizer = KerasTokenizer(num_words=15000,\n",
    "                          lower=True,\n",
    "                          char_level=False)\n",
    "tokenizer.fit_on_texts(spooky_train.text)\n",
    "\n",
    "spooky_train['keras_token_ids'] = tokenizer.texts_to_sequences(spooky_train.text)\n",
    "spooky_train.keras_token_ids = [[min(s, tokenizer.num_words) for s in seq] for seq in spooky_train.keras_token_ids]\n",
    "\n",
    "spooky_test['keras_token_ids'] = tokenizer.texts_to_sequences(spooky_test.text)\n",
    "spooky_test.keras_token_ids = [[min(s, tokenizer.num_words) for s in seq] for seq in spooky_test.keras_token_ids]\n",
    "\n",
    "tokenizer = Tokenizer(input_paths=['../data/input/spooky-author/train.txt'],\n",
    "                      lowercase=True,\n",
    "                      vocab_size=15000)\n",
    "\n",
    "spooky_train['encoding'] = tokenizer.encode_batch(spooky_train.text.tolist())\n",
    "spooky_test['encoding'] = tokenizer.encode_batch(spooky_test.text.tolist())\n",
    "\n",
    "spooky_train['tokens'] = spooky_train.encoding.apply(lambda e: e.tokens)\n",
    "spooky_test['tokens'] = spooky_test.encoding.apply(lambda e: e.tokens)\n",
    "\n",
    "spooky_train['token_ids'] = spooky_train.encoding.apply(lambda e: e.ids)\n",
    "spooky_test['token_ids'] = spooky_test.encoding.apply(lambda e: e.ids)\n",
    "\n",
    "author_to_id = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
    "spooky_train['author_id'] = spooky_train.author.apply(lambda a: author_to_id[a])\n",
    "\n",
    "spooky_train.token_ids = spooky_train.token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "spooky_test.token_ids = spooky_test.token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "spooky_train.keras_token_ids = spooky_train.keras_token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "spooky_test.keras_token_ids = spooky_test.keras_token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "\n",
    "x_train = np.array(spooky_train[tokens].tolist())\n",
    "y_train = to_categorical(np.array(spooky_train.author_id.tolist()))\n",
    "\n",
    "x_test = np.array(spooky_test[tokens].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = create_sdpa_model()\n",
    "\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          epochs=3,\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MHA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_mha_model(d_heads=4, d_model=128, sequence_length=128, compile=True):\n",
    "    x = Input(shape=(sequence_length,))\n",
    "    embed = Embedding(input_dim=15000, output_dim=d_model)(x)\n",
    "    sdpa = [ScaledDotProductAttention(d_model=d_model, d_k=16, d_v=16)(embed) for _ in range(d_heads)]\n",
    "    mha = MultiHeadAttention(d_heads=d_heads, \n",
    "                             d_model=d_model, \n",
    "                             d_k=16,\n",
    "                             d_v=16, \n",
    "                             sequence_length=sequence_length,\n",
    "                             verbose=True)(sdpa)\n",
    "    \n",
    "    flat = Flatten()(mha)\n",
    "    dense = Dense(units=100)(flat)\n",
    "    dropout = Dropout(rate=0.2)(dense)\n",
    "    y = Dense(units=3, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=[x],\n",
    "                  outputs=[y])\n",
    "    if compile:\n",
    "        model.compile(optimizer='Adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_mha_model(d_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          epochs=1,\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Failed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sequence_length = 128\n",
    "d_model = 128\n",
    "compression_rate = 3  # n_s/c=32\n",
    "\n",
    "h_shape = (batch_size, sequence_length, d_model)\n",
    "compressed_memory_shape = (batch_size, sequence_length // compression_rate, d_model)\n",
    "\n",
    "h = K.zeros(shape=h_shape)\n",
    "old_mem = K.zeros(shape=h_shape)\n",
    "new_cm = K.zeros(shape=compressed_memory_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "reconstruction_model = AttentionReconstruction(input_shape=[h_shape, h_shape], heads=[None], verbose=True)\n",
    "\n",
    "reconstruction_model.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# forward pass\n",
    "output = reconstruction_model([h, old_mem])\n",
    "print(output.shape)\n",
    "\n",
    "# train on batch\n",
    "loss = reconstruction_model.train_on_batch(x=[K.eval(h), K.eval(old_mem)],\n",
    "                                           y=K.eval(new_cm))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Detour really simple compression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DoubleInput(Model):\n",
    "    def __init__(self, input_shape, heads=None):\n",
    "        h_shape, old_mem_shape = input_shape\n",
    "        \n",
    "        h = Input(batch_shape=h_shape, name='h')\n",
    "        old_mem = Input(batch_shape=old_mem_shape, name='ar_old_mem')\n",
    "        \n",
    "        # zeros = Lambda(lambda _h: _h*0.00001, name='ar_pseudo_use_h')(h)\n",
    "        # pseudo_old_mem = Add(name='ar_add_zeros')([old_mem, zeros])\n",
    "        \n",
    "        output_layer = Conv1D(filters=128,\n",
    "                              kernel_size=3,\n",
    "                              strides=3,\n",
    "                              activation='relu',\n",
    "                              name='ar_conv1D')(old_mem)\n",
    "        \n",
    "        super().__init__(inputs=[h, old_mem], outputs=output_layer)\n",
    "        self.heads = heads\n",
    "        \n",
    "    def compile(self, *args, **kwargs):\n",
    "        print('compiling...')\n",
    "        return super().compile(*args, **kwargs)\n",
    "        \n",
    "    def train_on_batch(self, *args, **kwargs):\n",
    "        print('training on batch...')\n",
    "        return super().train_on_batch(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "di_model = DoubleInput(input_shape=[h_shape, h_shape], heads=[None])\n",
    "\n",
    "di_model.compile(optimizer='Adam',\n",
    "                 loss='mse')\n",
    "di_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = di_model([h, old_mem])\n",
    "print(output)\n",
    "\n",
    "loss = di_model.train_on_batch(x=[K.eval(h), K.eval(old_mem)],\n",
    "                               y=K.eval(new_cm))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Detour AR in notebook ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the head\n",
    "def create_sdpa_model(d_model=128, compile=True):\n",
    "    x = Input(shape=(128,))\n",
    "    embed = Embedding(input_dim=20000, output_dim=128)(x)\n",
    "    head = ScaledDotProductAttention(d_model=d_model, d_k=16, d_v=16, verbose=True)\n",
    "    sdpa = head(embed)\n",
    "    flat = Flatten()(sdpa)\n",
    "    dense = Dense(units=10)(flat)\n",
    "    y = Dense(units=3, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[x],\n",
    "                  outputs=[y])\n",
    "    if compile:\n",
    "        model.compile(optimizer='Adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    return model, head\n",
    "\n",
    "\n",
    "model, head = create_sdpa_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_max_pool = ['max-pool', 'max_pool', 'max pool', 'max']\n",
    "_1d_conv = ['1d-conv', '1d_conv', '1d conv', 'conv']\n",
    "_all_compressions = _max_pool[:1] + _1d_conv[:1]\n",
    "\n",
    "\n",
    "class AttentionReconstruction_notebook(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 heads,\n",
    "                 *args,\n",
    "                 compression='1d-conv',\n",
    "                 compression_rate=3,\n",
    "                 name='AttentionReconstruction',\n",
    "                 verbose=False,\n",
    "                 **kwargs):\n",
    "        assert isinstance(heads, list)\n",
    "        if len(heads) > 1:\n",
    "            raise NotImplementedError()\n",
    "        # heads\n",
    "\n",
    "        h_shape, old_mem_shape = input_shape\n",
    "        assert h_shape == old_mem_shape\n",
    "\n",
    "        h = Input(batch_shape=h_shape, name='ar_h')\n",
    "        old_mem = Input(batch_shape=old_mem_shape, name='ar_old_mem')\n",
    "\n",
    "        # zeros = Lambda(lambda _h: _h*0.00001, name='ar_pseudo_use_h')(h)\n",
    "        # pseudo_old_mem = Add(name='ar_add_zeros')([old_mem, zeros])\n",
    "\n",
    "        if compression in _max_pool:\n",
    "            raise NotImplementedError()\n",
    "        elif compression in _1d_conv:\n",
    "            filters = kwargs.get('conv_filters', 128)\n",
    "            activation = kwargs.get('conv_activation', 'relu')\n",
    "\n",
    "            output_layer = Conv1D(filters=filters,\n",
    "                                  kernel_size=compression_rate,\n",
    "                                  strides=compression_rate,\n",
    "                                  activation=activation,\n",
    "                                  name='ar_conv1D')\n",
    "            output = output_layer(old_mem)\n",
    "        else:\n",
    "            raise ValueError(f'unsupported compression: {compression}. '\n",
    "                             f'Select one from {_all_compressions}')\n",
    "\n",
    "        super().__init__(*args, inputs=[h, old_mem], outputs=output, name=name, **kwargs)\n",
    "        self.heads = heads\n",
    "        self.compression = compression\n",
    "        self.compression_rate = compression_rate\n",
    "        self._current_batch = dict(h=[h],\n",
    "                                   old_mem=[old_mem],\n",
    "                                   new_cm=[new_cm])\n",
    "        self.verbose = verbose\n",
    "        self._custom_layers = dict(output=output_layer)\n",
    "\n",
    "        if verbose:\n",
    "            print(self.summary())\n",
    "            \n",
    "    def compile(self,\n",
    "                optimizer,\n",
    "                loss='attention_reconstruction',\n",
    "                metrics=None,\n",
    "                loss_weights=None,\n",
    "                **kwargs):\n",
    "        if loss == 'attention_reconstruction':\n",
    "            loss = self.attention_reconstruction_loss()\n",
    "            print(loss)\n",
    "        else:\n",
    "            warnings.warn('using non-standard loss for AttentionReconstruction', RuntimeWarning)\n",
    "        \n",
    "        # self.add_loss(lambda: K.reduce_mean(self._current_batch['h']))\n",
    "        super().compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=metrics,\n",
    "                        loss_weights=loss_weights,\n",
    "                        **kwargs)\n",
    "    \n",
    "    def train_on_batch(self, x, y, sample_weight=None, class_weight=None, reset_metrics=True):\n",
    "        self._current_batch['h'] = [x[0]]\n",
    "        self._current_batch['old_mem'] = [x[1]]\n",
    "        self._current_batch['new_cm'] = [y]\n",
    "    \n",
    "        loss = super().train_on_batch(x=x,\n",
    "                                      y=y,\n",
    "                                      sample_weight=sample_weight,\n",
    "                                      class_weight=class_weight,\n",
    "                                      reset_metrics=reset_metrics)\n",
    "        return loss\n",
    "    \n",
    "    def attention_reconstruction_loss(self):\n",
    "    \n",
    "        def _attention_reconstruction_loss(y_true, y_pred):\n",
    "            # assert len(self.heads) == 1\n",
    "            # assert len(self._current_batch_old_mem) == 1\n",
    "            # assert len(self._current_batch_new_cm) == 1\n",
    "            print('   calculating loss...')\n",
    "            return K.sqrt((y_true - y_pred) ** 2)\n",
    "    \n",
    "#             for head, h, old_mem, new_cm in zip(self.heads, \n",
    "#                                                 self._current_batch['h'],\n",
    "#                                                 self._current_batch['old_mem'], \n",
    "#                                                 self._current_batch['new_cm']):\n",
    "#                 print(h, old_mem, head.w_q, head.w_k, head.w_v, sep='\\n')\n",
    "#                 old_attention = content_based_attention(h=h, m=old_mem, w_q=head.w_q, w_k=head.w_k, w_v=head.w_v)\n",
    "#                 new_attention = content_based_attention(h=h, m=new_cm, w_q=head.w_q, w_k=head.w_k, w_v=head.w_v)\n",
    "#                 loss_head = (old_attention - new_attention)\n",
    "    \n",
    "#                 loss += loss_head\n",
    "\n",
    "#             print((y_true - y_pred).shape)\n",
    "#             print(self._current_batch['h'][0])\n",
    "#             # # works\n",
    "#             # return y_true - y_pred\n",
    "#             return y_true - self._current_batch['new_cm'][0]\n",
    "            \n",
    "#             # # doesn't work\n",
    "#             # return K.zeros(shape=y_pred.shape)\n",
    "    \n",
    "        return _attention_reconstruction_loss\n",
    "\n",
    "reconstruction_model = AttentionReconstruction_notebook(input_shape=[h_shape, h_shape], heads=[head], verbose=True)\n",
    "\n",
    "reconstruction_model.compile(optimizer='Adam', loss=reconstruction_model.attention_reconstruction_loss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# FROM REPO\n",
    "reconstruction_model = AttentionReconstruction(input_shape=[h_shape, h_shape], heads=[head], verbose=True)\n",
    "\n",
    "reconstruction_model.compile(optimizer='Adam', loss='attention_reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# forward pass\n",
    "output = reconstruction_model([h, old_mem])\n",
    "print(output.shape)\n",
    "\n",
    "# train on batch\n",
    "loss = reconstruction_model.train_on_batch(x=[K.eval(h), K.eval(old_mem)],\n",
    "                                           y=K.eval(new_cm))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Potential solution to input-tracked-for-topology losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://keras.io/api/losses/#creating-custom-losses\n",
    "\n",
    "If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized.\n",
    "\n",
    "Example\n",
    "```python\n",
    "inputs = tf.keras.Input(shape=(10,))\n",
    "d = tf.keras.layers.Dense(10)\n",
    "x = d(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "# Weight regularization.\n",
    "model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstruction_model.train_on_batch(x=[K.eval(h), K.eval(old_mem)],\n",
    "                                    y=K.eval(new_cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT model, v.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.3\n",
    "\n",
    "train_data = pd.read_pickle('../data/processed/spooky-author/train.pkl')\n",
    "val_index = int(validation_split*len(train_data))\n",
    "\n",
    "x_train = np.array(train_data.x.tolist())\n",
    "y_train = np.array(train_data.y.tolist())\n",
    "\n",
    "_x_train = np.zeros((x_train.shape[0], 128))  # samples, d_model\n",
    "_x_train[:,:x_train.shape[1]] = x_train\n",
    "x_train = _x_train\n",
    "\n",
    "x_val = x_train[-val_index:]\n",
    "y_val = y_train[-val_index:]\n",
    "\n",
    "x_train = x_train[:-val_index]\n",
    "y_train = y_train[:-val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=32\n",
    "samples=len(x_train)\n",
    "validation_samples=len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct = CompressiveTransformer(d_layers=1,\n",
    "                            sequence_length=128, \n",
    "                            d_model=128,\n",
    "                            memory_size=256,\n",
    "                            compressed_memory_size=256,\n",
    "                            d_k=16, \n",
    "                            d_heads=2, \n",
    "                            output_size=3,\n",
    "                            batch_size=32,\n",
    "                            vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.compile(optimizer='Adam',\n",
    "           loss='categorical_crossentropy',\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train CT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ct, x_val):\n",
    "    y_pred = [ct.predict(x=[x_val[i:i+ct.batch_size], \n",
    "                            ct.memory, \n",
    "                            ct.compressed_memory]) \\\n",
    "                  for i in range(0, len(x_val) - len(x_val) % ct.batch_size, ct.batch_size)]\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    accuracy = (y_pred.argmax(axis=1) == y_val[:len(y_pred)].argmax(axis=1)).sum() / len(y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    \n",
    "    for i in range(0, samples - samples % batch_size, batch_size):\n",
    "            \n",
    "        (loss, acc), loss_ar = ct.train_on_batch(x=[x_train[i:i+batch_size], ct.memory, ct.compressed_memory],\n",
    "                                                 y=y_train[i:i+batch_size])\n",
    "        ct.memory *= 0\n",
    "        ct.compressed_memory *= 0\n",
    "        \n",
    "        epoch_loss.append(loss)\n",
    "        epoch_acc.append(acc)\n",
    "        \n",
    "        if (i // batch_size) % 20 == 0:\n",
    "            print(f'    i: {i:4d}    loss={np.mean(epoch_loss):.3f}, accuracy={np.mean(epoch_acc):.3f}')\n",
    "        # print(ct.memory[0])\n",
    "        # print('\\n\\n\\n')\n",
    "    val_acc = evaluate(ct, x_val)\n",
    "    print(f'val_acc={val_acc:.3f}        loss={np.mean(epoch_loss):.3f}, accuracy={np.mean(epoch_acc):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval CT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=dict(y_pred=y_pred.argmax(axis=1),\n",
    "                                 y_true=y_val[:validation_samples - validation_samples % batch_size].argmax(axis=1)))\n",
    "results['correct'] = results.y_pred == results.y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results.groupby('y_true')['correct'].agg(['count', 'sum'])\n",
    "r['accuracy'] = r['sum'] / r['count']\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: 40.033\n",
    "# 5: 55.682\n",
    "# 9: 89.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ct.get_layer(name='h_L0').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e.shape)\n",
    "print(np.prod(e.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = np.isnan(e).sum()\n",
    "nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ct.get_layer(name='output').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = np.isnan(np.array(e)).sum()\n",
    "nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Var blir outputten riktigt stor ? \n",
    "# nan - detector ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = [x_train[32:32+batch_size], \n",
    "           np.zeros_like(ct.memory), \n",
    "           np.zeros_like(ct.compressed_memory')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = [x_train[32:32+batch_size], \n",
    "           ct.memory, \n",
    "           ct.compressed_memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ct.memory)\n",
    "print(ct.compressed_memory)\n",
    "\n",
    "for layer in [ct.layers[1]] + ct.layers[4:]:\n",
    "    print(layer.name)\n",
    "    lv = K.function(ct.input, layer.output)(x_batch)\n",
    "    print(lv[0])\n",
    "    print()\n",
    "    print(f'{lv.min():.3f},   {lv.mean():.3f},   {lv.max():.3f}')\n",
    "    print(np.isnan(lv).sum())\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPRESSED MEMORY GETS NANS FOR SOME REASON.\n",
    "#     ->  fix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[layer.name for layer in ct.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Overview",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
