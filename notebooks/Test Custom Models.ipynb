{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Overview<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>Load the Data</a></span></li><li><span><a href=\"#Create-Model\" data-toc-modified-id=\"Create-Model-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>Create Model</a></span></li><li><span><a href=\"#Compile-Model\" data-toc-modified-id=\"Compile-Model-0.3\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>Compile Model</a></span></li><li><span><a href=\"#Fit-Model\" data-toc-modified-id=\"Fit-Model-0.4\"><span class=\"toc-item-num\">0.4&nbsp;&nbsp;</span>Fit Model</a></span></li></ul></li><li><span><a href=\"#Terminal\" data-toc-modified-id=\"Terminal-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Terminal</a></span></li><li><span><a href=\"#Test-on-English-next-char-prediction\" data-toc-modified-id=\"Test-on-English-next-char-prediction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Test on English next-char prediction</a></span></li><li><span><a href=\"#Detour-Get-everything-underneath-to-work-again\" data-toc-modified-id=\"Detour-Get-everything-underneath-to-work-again-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Detour Get everything underneath to work again</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#SDPA-attention-model\" data-toc-modified-id=\"SDPA-attention-model-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>SDPA attention model</a></span></li><li><span><a href=\"#MHA-model\" data-toc-modified-id=\"MHA-model-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>MHA model</a></span></li><li><span><a href=\"#Compression-model\" data-toc-modified-id=\"Compression-model-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Compression model</a></span></li></ul></li></ul></li><li><span><a href=\"#CT-model,-v.2\" data-toc-modified-id=\"CT-model,-v.2-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>CT model, v.2</a></span></li><li><span><a href=\"#CT:-WMA-en\" data-toc-modified-id=\"CT:-WMA-en-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CT: WMA en</a></span></li><li><span><a href=\"#garbage\" data-toc-modified-id=\"garbage-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>garbage</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep, time\n",
    "from keras import backend as K\n",
    "from keras.models import Model \n",
    "from keras.models import Sequential as SequentialModel\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Conv1D, LSTM, Dropout, Embedding, Layer, Input, Flatten, concatenate as Concatenate, Lambda, Add\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from sometimer import timer, time_this_method\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "sys.path.insert(0, '../ct')\n",
    "\n",
    "import load\n",
    "import preprocess\n",
    "\n",
    "from preprocess.tokenize import Tokenizer\n",
    "from preprocess.tokenize import separator_samples\n",
    "\n",
    "from model.layers import LayerNormalization\n",
    "from model.layers import ContentBasedAttention_CT\n",
    "from model.layers import ScaledDotProductAttention\n",
    "from model.layers import MultiHeadAttention\n",
    "from model.layers import content_based_attention\n",
    "\n",
    "from model import CompressiveTransformer\n",
    "from model import AttentionReconstruction\n",
    "\n",
    "from model.callbacks import ClearCompressedMemory, WriteLogsToFile\n",
    "from train.generators import next_token_batch_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('../data/processed/spooky-author/train.pkl')\n",
    "\n",
    "x_train = np.array(train_data.x.tolist())\n",
    "y_train = np.array(train_data.y.tolist())\n",
    "\n",
    "_x_train = np.zeros((x_train.shape[0], 128))  # samples, d_model\n",
    "_x_train[:,:x_train.shape[1]] = x_train\n",
    "x_train = _x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "sequence_length = 128\n",
    "\n",
    "model = CompressiveTransformer(d_model=d_model, sequence_length=sequence_length, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# redo everyth\n",
    "model = CompressiveTransformer(d_model=d_model, sequence_length=sequence_length, batch_size=1)\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "for batch in range(0, len(x_train), 1):\n",
    "    x_batch = x_train[batch:batch+1, :]\n",
    "    y_batch = y_train[batch:batch+1]\n",
    "    \n",
    "    model.train_on_batch(x_batch,\n",
    "                         y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = K.variable(np.array([i for i in range(200)]).reshape((2, 5, 20)))\n",
    "\n",
    "def call(x, units=None, gain=None, bias=None):\n",
    "    if units is None:\n",
    "        units = np.prod(x.shape[1:])\n",
    "        print(f'units={units}')\n",
    "    \n",
    "    mean = K.sum(x) / units\n",
    "    std_dev = K.sqrt(K.sum(K.square(x - mean)) / units)\n",
    "\n",
    "    y = (x - mean) / std_dev\n",
    "    if gain:\n",
    "        y *= gain\n",
    "    if bias:\n",
    "        y += bias\n",
    "    return y\n",
    "\n",
    "K.eval(call(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test on English next-char prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_paths_tokenizer = ['..\\\\data\\\\wma-en-de\\\\input\\\\train-en-ascii.txt',\n",
    "                        # '..\\\\data\\\\wma-en-de\\\\input\\\\train-de-ascii.txt'\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(input_paths=input_paths_tokenizer)\n",
    "\n",
    "data = []\n",
    "for path in input_paths_tokenizer:\n",
    "    with open(path) as file:\n",
    "        data.extend(file.readlines())\n",
    "df_ = pd.DataFrame(data={'text': data})\n",
    "df = df_[:10000]\n",
    "\n",
    "df['encoding'] = tokenizer.encode_batch(df.text.tolist())\n",
    "df['tokens'] = df.encoding.apply(lambda e: e.tokens)\n",
    "df['token_ids'] = df.encoding.apply(lambda e: e.ids)\n",
    "\n",
    "token_ids = [t for tokens in df.token_ids for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "sequence_length = 128\n",
    "\n",
    "model = CompressiveTransformer(d_model=d_model, sequence_length=sequence_length, batch_size=1)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(input_data, epochs=1, batch_size=1, d_model=128):\n",
    "    chunk_size = len(input_data) // batch_size\n",
    "    data = [input_data[i:i+chunk_size] for i in range(0, len(input_data), chunk_size)]\n",
    "    data = [[token for s in chunk for token in s] for chunk in data]\n",
    "    sample_size = len(data[0])\n",
    "    print(len(data))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        print(f'epoch: {e:5}')\n",
    "        for i in tqdm(range(chunk_size)):\n",
    "            print(f'       {e:5}-{i}')\n",
    "            x = [t[i:i+d_model] for t in data]\n",
    "            y = [t[i+d_model] for t in data]\n",
    "            \n",
    "            x = np.array(x)\n",
    "            y = np.array(y)\n",
    "            \n",
    "            y = keras.utils.to_categorical(y, num_classes=20000)\n",
    "            \n",
    "            print(y)\n",
    "            \n",
    "            model.train_on_batch(x=x,\n",
    "                                 y=y)\n",
    "            \n",
    "batch_generator(df.token_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detour Get everything underneath to work again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SDPA attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('../data/processed/spooky-author/train.pkl')\n",
    "\n",
    "x_train = np.array(train_data.x.tolist())\n",
    "y_train = np.array(train_data.y.tolist())\n",
    "\n",
    "_x_train = np.zeros((x_train.shape[0], 128))  # samples, d_model\n",
    "_x_train[:,:x_train.shape[1]] = x_train\n",
    "x_train = _x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_sdpa_model(d_model=128, compile=True):\n",
    "    x = Input(shape=(128,))\n",
    "    embed = Embedding(input_dim=20000, output_dim=128)(x)\n",
    "    sdpa = ScaledDotProductAttention(d_model=d_model, d_k=16, d_v=16, verbose=True)(embed)\n",
    "    flat = Flatten()(sdpa)\n",
    "    dense = Dense(units=10)(flat)\n",
    "    y = Dense(units=3, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[x],\n",
    "                  outputs=[y])\n",
    "    if compile:\n",
    "        model.compile(optimizer='Adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    return model, sdpa\n",
    "\n",
    "\n",
    "model, _ = create_sdpa_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Detour sample submission on Spooky-author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spooky_train = pd.read_csv('../data/input/spooky-author/train.csv')\n",
    "spooky_test = pd.read_csv('../data/input/spooky-author/test.csv')\n",
    "\n",
    "max_length = 128\n",
    "tokens = 'keras_token_ids'\n",
    "\n",
    "\n",
    "tokenizer = KerasTokenizer(num_words=15000,\n",
    "                          lower=True,\n",
    "                          char_level=False)\n",
    "tokenizer.fit_on_texts(spooky_train.text)\n",
    "\n",
    "spooky_train['keras_token_ids'] = tokenizer.texts_to_sequences(spooky_train.text)\n",
    "spooky_train.keras_token_ids = [[min(s, tokenizer.num_words) for s in seq] for seq in spooky_train.keras_token_ids]\n",
    "\n",
    "spooky_test['keras_token_ids'] = tokenizer.texts_to_sequences(spooky_test.text)\n",
    "spooky_test.keras_token_ids = [[min(s, tokenizer.num_words) for s in seq] for seq in spooky_test.keras_token_ids]\n",
    "\n",
    "tokenizer = Tokenizer(input_paths=['../data/input/spooky-author/train.txt'],\n",
    "                      lowercase=True,\n",
    "                      vocab_size=15000)\n",
    "\n",
    "spooky_train['encoding'] = tokenizer.encode_batch(spooky_train.text.tolist())\n",
    "spooky_test['encoding'] = tokenizer.encode_batch(spooky_test.text.tolist())\n",
    "\n",
    "spooky_train['tokens'] = spooky_train.encoding.apply(lambda e: e.tokens)\n",
    "spooky_test['tokens'] = spooky_test.encoding.apply(lambda e: e.tokens)\n",
    "\n",
    "spooky_train['token_ids'] = spooky_train.encoding.apply(lambda e: e.ids)\n",
    "spooky_test['token_ids'] = spooky_test.encoding.apply(lambda e: e.ids)\n",
    "\n",
    "author_to_id = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
    "spooky_train['author_id'] = spooky_train.author.apply(lambda a: author_to_id[a])\n",
    "\n",
    "spooky_train.token_ids = spooky_train.token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "spooky_test.token_ids = spooky_test.token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "spooky_train.keras_token_ids = spooky_train.keras_token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "spooky_test.keras_token_ids = spooky_test.keras_token_ids.apply(lambda a: (a + [0]*(max_length - len(a)))[:max_length])\n",
    "\n",
    "x_train = np.array(spooky_train[tokens].tolist())\n",
    "y_train = to_categorical(np.array(spooky_train.author_id.tolist()))\n",
    "\n",
    "x_test = np.array(spooky_test[tokens].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = create_sdpa_model()\n",
    "\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          epochs=3,\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MHA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00513655dd36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mha_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-00513655dd36>\u001b[0m in \u001b[0;36mcreate_mha_model\u001b[0;34m(d_heads, d_model, sequence_length, compile)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_mha_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msdpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mScaledDotProductAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     mha = MultiHeadAttention(d_heads=d_heads, \n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "def create_mha_model(d_heads=4, d_model=128, sequence_length=128, compile=True):\n",
    "    x = Input(shape=(sequence_length,))\n",
    "    embed = Embedding(input_dim=15000, output_dim=d_model)(x)\n",
    "    sdpa = [ScaledDotProductAttention(d_model=d_model, d_k=16, d_v=16)(embed) for _ in range(d_heads)]\n",
    "    mha = MultiHeadAttention(d_heads=d_heads, \n",
    "                             d_model=d_model, \n",
    "                             d_k=16,\n",
    "                             d_v=16, \n",
    "                             sequence_length=sequence_length,\n",
    "                             verbose=True)(sdpa)\n",
    "    \n",
    "    flat = Flatten()(mha)\n",
    "    dense = Dense(units=100)(flat)\n",
    "    dropout = Dropout(rate=0.2)(dense)\n",
    "    y = Dense(units=3, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=[x],\n",
    "                  outputs=[y])\n",
    "    if compile:\n",
    "        model.compile(optimizer='Adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_mha_model(d_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          epochs=1,\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Failed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sequence_length = 128\n",
    "d_model = 128\n",
    "compression_rate = 3  # n_s/c=32\n",
    "\n",
    "h_shape = (batch_size, sequence_length, d_model)\n",
    "compressed_memory_shape = (batch_size, sequence_length // compression_rate, d_model)\n",
    "\n",
    "h = K.zeros(shape=h_shape)\n",
    "old_mem = K.zeros(shape=h_shape)\n",
    "new_cm = K.zeros(shape=compressed_memory_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "reconstruction_model = AttentionReconstruction(input_shape=[h_shape, h_shape], heads=[None], verbose=True)\n",
    "\n",
    "reconstruction_model.compile(optimizer='Adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# forward pass\n",
    "output = reconstruction_model([h, old_mem])\n",
    "print(output.shape)\n",
    "\n",
    "# train on batch\n",
    "loss = reconstruction_model.train_on_batch(x=[K.eval(h), K.eval(old_mem)],\n",
    "                                           y=K.eval(new_cm))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Detour really simple compression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DoubleInput(Model):\n",
    "    def __init__(self, input_shape, heads=None):\n",
    "        h_shape, old_mem_shape = input_shape\n",
    "        \n",
    "        h = Input(batch_shape=h_shape, name='h')\n",
    "        old_mem = Input(batch_shape=old_mem_shape, name='ar_old_mem')\n",
    "        \n",
    "        # zeros = Lambda(lambda _h: _h*0.00001, name='ar_pseudo_use_h')(h)\n",
    "        # pseudo_old_mem = Add(name='ar_add_zeros')([old_mem, zeros])\n",
    "        \n",
    "        output_layer = Conv1D(filters=128,\n",
    "                              kernel_size=3,\n",
    "                              strides=3,\n",
    "                              activation='relu',\n",
    "                              name='ar_conv1D')(old_mem)\n",
    "        \n",
    "        super().__init__(inputs=[h, old_mem], outputs=output_layer)\n",
    "        self.heads = heads\n",
    "        \n",
    "    def compile(self, *args, **kwargs):\n",
    "        print('compiling...')\n",
    "        return super().compile(*args, **kwargs)\n",
    "        \n",
    "    def train_on_batch(self, *args, **kwargs):\n",
    "        print('training on batch...')\n",
    "        return super().train_on_batch(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "di_model = DoubleInput(input_shape=[h_shape, h_shape], heads=[None])\n",
    "\n",
    "di_model.compile(optimizer='Adam',\n",
    "                 loss='mse')\n",
    "di_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = di_model([h, old_mem])\n",
    "print(output)\n",
    "\n",
    "loss = di_model.train_on_batch(x=[K.eval(h), K.eval(old_mem)],\n",
    "                               y=K.eval(new_cm))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Detour AR in notebook ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the head\n",
    "def create_sdpa_model(d_model=128, compile=True):\n",
    "    x = Input(shape=(128,))\n",
    "    embed = Embedding(input_dim=20000, output_dim=128)(x)\n",
    "    head = ScaledDotProductAttention(d_model=d_model, d_k=16, d_v=16, verbose=True)\n",
    "    sdpa = head(embed)\n",
    "    flat = Flatten()(sdpa)\n",
    "    dense = Dense(units=10)(flat)\n",
    "    y = Dense(units=3, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[x],\n",
    "                  outputs=[y])\n",
    "    if compile:\n",
    "        model.compile(optimizer='Adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    return model, head\n",
    "\n",
    "\n",
    "model, head = create_sdpa_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_max_pool = ['max-pool', 'max_pool', 'max pool', 'max']\n",
    "_1d_conv = ['1d-conv', '1d_conv', '1d conv', 'conv']\n",
    "_all_compressions = _max_pool[:1] + _1d_conv[:1]\n",
    "\n",
    "\n",
    "class AttentionReconstruction_notebook(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 heads,\n",
    "                 *args,\n",
    "                 compression='1d-conv',\n",
    "                 compression_rate=3,\n",
    "                 name='AttentionReconstruction',\n",
    "                 verbose=False,\n",
    "                 **kwargs):\n",
    "        assert isinstance(heads, list)\n",
    "        if len(heads) > 1:\n",
    "            raise NotImplementedError()\n",
    "        # heads\n",
    "\n",
    "        h_shape, old_mem_shape = input_shape\n",
    "        assert h_shape == old_mem_shape\n",
    "\n",
    "        h = Input(batch_shape=h_shape, name='ar_h')\n",
    "        old_mem = Input(batch_shape=old_mem_shape, name='ar_old_mem')\n",
    "\n",
    "        # zeros = Lambda(lambda _h: _h*0.00001, name='ar_pseudo_use_h')(h)\n",
    "        # pseudo_old_mem = Add(name='ar_add_zeros')([old_mem, zeros])\n",
    "\n",
    "        if compression in _max_pool:\n",
    "            raise NotImplementedError()\n",
    "        elif compression in _1d_conv:\n",
    "            filters = kwargs.get('conv_filters', 128)\n",
    "            activation = kwargs.get('conv_activation', 'relu')\n",
    "\n",
    "            output_layer = Conv1D(filters=filters,\n",
    "                                  kernel_size=compression_rate,\n",
    "                                  strides=compression_rate,\n",
    "                                  activation=activation,\n",
    "                                  name='ar_conv1D')\n",
    "            output = output_layer(old_mem)\n",
    "        else:\n",
    "            raise ValueError(f'unsupported compression: {compression}. '\n",
    "                             f'Select one from {_all_compressions}')\n",
    "\n",
    "        super().__init__(*args, inputs=[h, old_mem], outputs=output, name=name, **kwargs)\n",
    "        self.heads = heads\n",
    "        self.compression = compression\n",
    "        self.compression_rate = compression_rate\n",
    "        self._current_batch = dict(h=[h],\n",
    "                                   old_mem=[old_mem],\n",
    "                                   new_cm=[new_cm])\n",
    "        self.verbose = verbose\n",
    "        self._custom_layers = dict(output=output_layer)\n",
    "\n",
    "        if verbose:\n",
    "            print(self.summary())\n",
    "            \n",
    "    def compile(self,\n",
    "                optimizer,\n",
    "                loss='attention_reconstruction',\n",
    "                metrics=None,\n",
    "                loss_weights=None,\n",
    "                **kwargs):\n",
    "        if loss == 'attention_reconstruction':\n",
    "            loss = self.attention_reconstruction_loss()\n",
    "            print(loss)\n",
    "        else:\n",
    "            warnings.warn('using non-standard loss for AttentionReconstruction', RuntimeWarning)\n",
    "        \n",
    "        # self.add_loss(lambda: K.reduce_mean(self._current_batch['h']))\n",
    "        super().compile(optimizer=optimizer,\n",
    "                        loss=loss,\n",
    "                        metrics=metrics,\n",
    "                        loss_weights=loss_weights,\n",
    "                        **kwargs)\n",
    "    \n",
    "    def train_on_batch(self, x, y, sample_weight=None, class_weight=None, reset_metrics=True):\n",
    "        self._current_batch['h'] = [x[0]]\n",
    "        self._current_batch['old_mem'] = [x[1]]\n",
    "        self._current_batch['new_cm'] = [y]\n",
    "    \n",
    "        loss = super().train_on_batch(x=x,\n",
    "                                      y=y,\n",
    "                                      sample_weight=sample_weight,\n",
    "                                      class_weight=class_weight,\n",
    "                                      reset_metrics=reset_metrics)\n",
    "        return loss\n",
    "    \n",
    "    def attention_reconstruction_loss(self):\n",
    "    \n",
    "        def _attention_reconstruction_loss(y_true, y_pred):\n",
    "            # assert len(self.heads) == 1\n",
    "            # assert len(self._current_batch_old_mem) == 1\n",
    "            # assert len(self._current_batch_new_cm) == 1\n",
    "            print('   calculating loss...')\n",
    "            return K.sqrt((y_true - y_pred) ** 2)\n",
    "    \n",
    "#             for head, h, old_mem, new_cm in zip(self.heads, \n",
    "#                                                 self._current_batch['h'],\n",
    "#                                                 self._current_batch['old_mem'], \n",
    "#                                                 self._current_batch['new_cm']):\n",
    "#                 print(h, old_mem, head.w_q, head.w_k, head.w_v, sep='\\n')\n",
    "#                 old_attention = content_based_attention(h=h, m=old_mem, w_q=head.w_q, w_k=head.w_k, w_v=head.w_v)\n",
    "#                 new_attention = content_based_attention(h=h, m=new_cm, w_q=head.w_q, w_k=head.w_k, w_v=head.w_v)\n",
    "#                 loss_head = (old_attention - new_attention)\n",
    "    \n",
    "#                 loss += loss_head\n",
    "\n",
    "#             print((y_true - y_pred).shape)\n",
    "#             print(self._current_batch['h'][0])\n",
    "#             # # works\n",
    "#             # return y_true - y_pred\n",
    "#             return y_true - self._current_batch['new_cm'][0]\n",
    "            \n",
    "#             # # doesn't work\n",
    "#             # return K.zeros(shape=y_pred.shape)\n",
    "    \n",
    "        return _attention_reconstruction_loss\n",
    "\n",
    "reconstruction_model = AttentionReconstruction_notebook(input_shape=[h_shape, h_shape], heads=[head], verbose=True)\n",
    "\n",
    "reconstruction_model.compile(optimizer='Adam', loss=reconstruction_model.attention_reconstruction_loss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# FROM REPO\n",
    "reconstruction_model = AttentionReconstruction(input_shape=[h_shape, h_shape], heads=[head], verbose=True)\n",
    "\n",
    "reconstruction_model.compile(optimizer='Adam', loss='attention_reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# forward pass\n",
    "output = reconstruction_model([h, old_mem])\n",
    "print(output.shape)\n",
    "\n",
    "# train on batch\n",
    "loss = reconstruction_model.train_on_batch(x=[K.eval(h), K.eval(old_mem)],\n",
    "                                           y=K.eval(new_cm))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Potential solution to input-tracked-for-topology losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://keras.io/api/losses/#creating-custom-losses\n",
    "\n",
    "If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized.\n",
    "\n",
    "Example\n",
    "```python\n",
    "inputs = tf.keras.Input(shape=(10,))\n",
    "d = tf.keras.layers.Dense(10)\n",
    "x = d(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "# Weight regularization.\n",
    "model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstruction_model.train_on_batch(x=[K.eval(h), K.eval(old_mem)],\n",
    "                                    y=K.eval(new_cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT model, v.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.3\n",
    "\n",
    "train_data = pd.read_pickle('../data/processed/spooky-author/train.pkl')\n",
    "val_index = int(validation_split*len(train_data))\n",
    "\n",
    "x_train = np.array(train_data.x.tolist())\n",
    "y_train = np.array(train_data.y.tolist())\n",
    "\n",
    "_x_train = np.zeros((x_train.shape[0], 128))  # samples, d_model\n",
    "_x_train[:,:x_train.shape[1]] = x_train\n",
    "x_train = _x_train\n",
    "\n",
    "x_val = x_train[-val_index:]\n",
    "y_val = y_train[-val_index:]\n",
    "\n",
    "x_train = x_train[:-val_index]\n",
    "y_train = y_train[:-val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=32\n",
    "samples=len(x_train)\n",
    "validation_samples=len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct = CompressiveTransformer(d_layers=1,\n",
    "                            sequence_length=128, \n",
    "                            d_model=128,\n",
    "                            memory_size=256,\n",
    "                            compressed_memory_size=256,\n",
    "                            d_k=16, \n",
    "                            d_heads=2, \n",
    "                            output_size=3,\n",
    "                            batch_size=32,\n",
    "                            vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.compile(optimizer='Adam',\n",
    "           loss='categorical_crossentropy',\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train CT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ct, x_val):\n",
    "    y_pred = [ct.predict(x=[x_val[i:i+ct.batch_size], \n",
    "                            ct.memory, \n",
    "                            ct.compressed_memory]) \\\n",
    "                  for i in range(0, len(x_val) - len(x_val) % ct.batch_size, ct.batch_size)]\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    accuracy = (y_pred.argmax(axis=1) == y_val[:len(y_pred)].argmax(axis=1)).sum() / len(y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "\n",
    "    for i in range(0, samples - samples % batch_size, batch_size):\n",
    "\n",
    "        (loss, acc), loss_ar = ct.train_on_batch(x=[x_train[i:i+batch_size], ct.memory, ct.compressed_memory],\n",
    "                                                 y=y_train[i:i+batch_size])\n",
    "        ct.memory *= 0\n",
    "        ct.compressed_memory *= 0\n",
    "\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_acc.append(acc)\n",
    "\n",
    "        if (i // batch_size) % 20 == 0:\n",
    "            print(f'    i: {i:4d}    loss={np.mean(epoch_loss):.3f}, accuracy={np.mean(epoch_acc):.3f}')\n",
    "        # print(ct.memory[0])\n",
    "        # print('\\n\\n\\n')\n",
    "    val_acc = evaluate(ct, x_val)\n",
    "    print(f'val_acc={val_acc:.3f}        loss={np.mean(epoch_loss):.3f}, accuracy={np.mean(epoch_acc):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval CT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=dict(y_pred=y_pred.argmax(axis=1),\n",
    "                                 y_true=y_val[:validation_samples - validation_samples % batch_size].argmax(axis=1)))\n",
    "results['correct'] = results.y_pred == results.y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results.groupby('y_true')['correct'].agg(['count', 'sum'])\n",
    "r['accuracy'] = r['sum'] / r['count']\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: 40.033\n",
    "# 5: 55.682\n",
    "# 9: 89.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ct.get_layer(name='h_L0').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ct.get_layer(name='output').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = [x_train[32:32+batch_size], \n",
    "           np.zeros_like(ct.memory), \n",
    "           np.zeros_like(ct.compressed_memory')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = [x_train[32:32+batch_size], \n",
    "           ct.memory, \n",
    "           ct.compressed_memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ct.memory)\n",
    "print(ct.compressed_memory)\n",
    "\n",
    "for layer in [ct.layers[1]] + ct.layers[4:]:\n",
    "    print(layer.name)\n",
    "    lv = K.function(ct.input, layer.output)(x_batch)\n",
    "    print(lv[0])\n",
    "    print()\n",
    "    print(f'{lv.min():.3f},   {lv.mean():.3f},   {lv.max():.3f}')\n",
    "    print(np.isnan(lv).sum())\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT: WMA en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape \n",
    "def _preprocess(series, batch_size, output_path=None, use_existing=True):\n",
    "    if use_existing and os.path.exists(output_path):\n",
    "        with open(output_path, 'rb') as file:\n",
    "            processed = np.load(file)\n",
    "        return processed\n",
    "    \n",
    "    train = [list() for _ in range(batch_size)] \n",
    "    for i, t in enumerate(series):\n",
    "        train[i % batch_size].extend(t)\n",
    "\n",
    "    sample_length = min(len(t) for t in train)\n",
    "    train = [t[:sample_length] for t in train]\n",
    "    train = np.array(train)\n",
    "    \n",
    "    if output_path is not None:\n",
    "        with open(output_path, 'wb') as file:\n",
    "            np.save(file, train)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2\n",
    "batch_size=128\n",
    "d_layers=1\n",
    "sequence_length=128\n",
    "d_model=128\n",
    "vocab_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('../data/wma-en-de/processed/train-en.pkl')\n",
    "val_data = pd.read_pickle('../data/wma-en-de/processed/val-en.pkl') \n",
    "\n",
    "# train = _preprocess(train_data.english_ids,\n",
    "#                     batch_size=batch_size,\n",
    "#                     output_path='../data/wma-en-de/processed/train.np', \n",
    "#                     use_existing=False)\n",
    "# val = _preprocess(val_data.english_ids, \n",
    "#                   batch_size=batch_size,\n",
    "#                   output_path='../data/wma-en-de/processed/val.np', \n",
    "#                   use_existing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array([ids for english_ids in train_data.english_ids for ids in english_ids])\n",
    "val = np.array([ids for english_ids in val_data.english_ids for ids in english_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_steps=len(train)  # epoch_steps=train.shape[1]\n",
    "validation_steps=len(val)  # validation_steps=val.shape[1]\n",
    "steps_per_epoch=epoch_steps//sequence_length\n",
    "steps_per_validation=validation_steps//sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct = CompressiveTransformer(d_layers=d_layers,\n",
    "                            sequence_length=sequence_length, \n",
    "                            d_model=d_model,\n",
    "                            memory_size=256,\n",
    "                            compressed_memory_size=256,\n",
    "                            d_k=16, \n",
    "                            d_heads=2, \n",
    "                            output_size=vocab_size,\n",
    "                            batch_size=batch_size,\n",
    "                            vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function AttentionReconstruction.attention_reconstruction_loss.<locals>._attention_reconstruction_loss at 0x7f6c0ededef0>\n",
      "   calculating loss...\n"
     ]
    }
   ],
   "source": [
    "ct.compile(optimizer='Adam',\n",
    "           loss='categorical_crossentropy',\n",
    "           metrics=['accuracy']\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CompressiveTransformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "compressed_memory (InputLayer)  (None, 1, 256, 128)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "memory (InputLayer)             (None, 1, 256, 128)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h_L0 (Embedding)                (None, 128, 128)     131072      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "select_compressed_memory_L0 (La (None, 256, 128)     0           compressed_memory[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "select_memory_L0 (Lambda)       (None, 256, 128)     0           memory[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "h_tilde_L0 (Concatenate)        (None, 640, 128)     0           select_compressed_memory_L0[0][0]\n",
      "                                                                 select_memory_L0[0][0]           \n",
      "                                                                 h_L0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "scaled_dot_product_attention_3  (None, 128, 128)     20480       h_L0[0][0]                       \n",
      "                                                                 h_tilde_L0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "scaled_dot_product_attention_4  (None, 128, 128)     20480       h_L0[0][0]                       \n",
      "                                                                 h_tilde_L0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multihead_attention_L0 (MultiHe (None, 128, 128)     32768       scaled_dot_product_attention_3[0]\n",
      "                                                                 scaled_dot_product_attention_4[0]\n",
      "__________________________________________________________________________________________________\n",
      "mha_skip_L0 (Add)               (None, 128, 128)     0           h_L0[0][0]                       \n",
      "                                                                 multihead_attention_L0[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mha_layer_norm_L0 (LayerNormali (None, 128, 128)     32768       mha_skip_L0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mlp_hidden_0_L0 (Dense)         (None, 128, 128)     16512       mha_layer_norm_L0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "mlp_no_activation_L0 (Dense)    (None, 128, 128)     16512       mlp_hidden_0_L0[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_skip_L0 (Add)               (None, 128, 128)     0           mlp_no_activation_L0[0][0]       \n",
      "                                                                 mha_layer_norm_L0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "h_L1 (LayerNormalization)       (None, 128, 128)     32768       mlp_skip_L0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (ReverseEmbedding)       (None, 1024)         131072      h_L1[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 434,432\n",
      "Trainable params: 303,360\n",
      "Non-trainable params: 131,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ct.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ClearCompressedMemory(),\n",
    "             WriteLogsToFile(filepath='training-logs/ct.txt', overwrite_old_file=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = next_token_batch_generator(ct=ct,\n",
    "                                       epochs=epochs, \n",
    "                                       data=train,\n",
    "                                       data_path=None,\n",
    "                                       epoch_steps=epoch_steps, \n",
    "                                       sequence_length=sequence_length, \n",
    "                                       batch_size=sequence_length,\n",
    "                                       stride=1,\n",
    "                                       vocab_size=vocab_size)\n",
    "validation_generator = next_token_batch_generator(ct=ct,\n",
    "                                                  epochs=epochs, \n",
    "                                                  data=val,\n",
    "                                                  data_path=None,\n",
    "                                                  epoch_steps=validation_steps, \n",
    "                                                  sequence_length=sequence_length, \n",
    "                                                  batch_size=sequence_length,\n",
    "                                                  stride=1, \n",
    "                                                  vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vs/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 110265/1626084 [=>............................] - ETA: 43:44:06 - loss: 5.7942 - accuracy: 0.0217"
     ]
    }
   ],
   "source": [
    "ct.fit_generator(generator(), \n",
    "                 steps_per_epoch=steps_per_epoch,\n",
    "                 epochs=epochs,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_data=validation_generator(),\n",
    "                 validation_steps=steps_per_validation,\n",
    "                 shuffle=False\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=40\n",
    "# batch_size=100\n",
    "# epoch_steps=2000000\n",
    "# validation_steps=800000\n",
    "# sequence_length=128\n",
    "# vocab_size=1024\n",
    "# steps_per_epoch=epoch_steps//sequence_length\n",
    "\n",
    "# >>> \n",
    "# Epoch 40/40\n",
    "# 15625/15625 [==============================] - 1216s 78ms/step - loss: 2.2174 - accuracy: 0.4947\n",
    "\n",
    "# >>>>\n",
    "# Epoch 40/40\n",
    "# 15625/15625 [==============================] - 1306s 84ms/step - loss: 2.6349 - accuracy: 0.4131 - \n",
    "#                                                val_loss: 4.8582 - val_accuracy: 0.2171\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_train = pd.read_pickle('../data/wma-en-de/input/train.pickle')\n",
    "_df_train = _df_train.head(1000)\n",
    "_df_train['english_ids'] =train_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iron cement is a ready for use paste which is ...</td>\n",
       "      <td>iron cement ist eine gebrauchs ##AT##-##AT## f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iron cement protects the ingot against the hot...</td>\n",
       "      <td>Nach der Aushrtung schtzt iron cement die Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fire restant repair cement for fire places ,...</td>\n",
       "      <td>feuerfester Reparaturkitt fr Feuerungsanlagen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Construction and repair of highways and ...\\n</td>\n",
       "      <td>Der Bau und die Reparatur der Autostraen ...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An announcement must be commercial character .\\n</td>\n",
       "      <td>die Mitteilungen sollen den geschftlichen kom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>5 For I , the Lord , have decreed in mine ange...</td>\n",
       "      <td>5 denn ich , der Herr , habe in meinem Zorn vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>6 Nevertheless , all flesh is in mine hand , a...</td>\n",
       "      <td>6 Doch ist alles Fleisch in meiner Hand , und ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>7 Wherefore , it is expedient that my servant ...</td>\n",
       "      <td>7 Darum ist es ratsam , da mein Knecht Sidney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9 But now , verily I say , it behooveth me tha...</td>\n",
       "      <td>9 aber jetzt , wahrlich , ich sage : Mir ersch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>10 And inasmuch as they are a faithful they sh...</td>\n",
       "      <td>10 und insofern sie treu sind , werden sie bew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               english  \\\n",
       "0    iron cement is a ready for use paste which is ...   \n",
       "1    iron cement protects the ingot against the hot...   \n",
       "2    a fire restant repair cement for fire places ,...   \n",
       "3        Construction and repair of highways and ...\\n   \n",
       "4     An announcement must be commercial character .\\n   \n",
       "..                                                 ...   \n",
       "995  5 For I , the Lord , have decreed in mine ange...   \n",
       "996  6 Nevertheless , all flesh is in mine hand , a...   \n",
       "997  7 Wherefore , it is expedient that my servant ...   \n",
       "998  9 But now , verily I say , it behooveth me tha...   \n",
       "999  10 And inasmuch as they are a faithful they sh...   \n",
       "\n",
       "                                                german  \n",
       "0    iron cement ist eine gebrauchs ##AT##-##AT## f...  \n",
       "1    Nach der Aushrtung schtzt iron cement die Ko...  \n",
       "2    feuerfester Reparaturkitt fr Feuerungsanlagen...  \n",
       "3      Der Bau und die Reparatur der Autostraen ...\\n  \n",
       "4    die Mitteilungen sollen den geschftlichen kom...  \n",
       "..                                                 ...  \n",
       "995  5 denn ich , der Herr , habe in meinem Zorn vi...  \n",
       "996  6 Doch ist alles Fleisch in meiner Hand , und ...  \n",
       "997  7 Darum ist es ratsam , da mein Knecht Sidney...  \n",
       "998  9 aber jetzt , wahrlich , ich sage : Mir ersch...  \n",
       "999  10 und insofern sie treu sind , werden sie bew...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "      <th>english_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iron cement is a ready for use paste which is ...</td>\n",
       "      <td>iron cement ist eine gebrauchs ##AT##-##AT## f...</td>\n",
       "      <td>[330, 265, 296, 339, 301, 326, 259, 353, 445, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iron cement protects the ingot against the hot...</td>\n",
       "      <td>Nach der Aushrtung schtzt iron cement die Ko...</td>\n",
       "      <td>[330, 265, 296, 339, 301, 406, 288, 364, 82, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a fire restant repair cement for fire places ,...</td>\n",
       "      <td>feuerfester Reparaturkitt fr Feuerungsanlagen...</td>\n",
       "      <td>[64, 285, 651, 409, 397, 490, 353, 79, 64, 330...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Construction and repair of highways and ...\\n</td>\n",
       "      <td>Der Bau und die Reparatur der Autostraen ...\\n</td>\n",
       "      <td>[34, 265, 323, 722, 682, 312, 353, 79, 64, 330...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An announcement must be commercial character .\\n</td>\n",
       "      <td>die Mitteilungen sollen den geschftlichen kom...</td>\n",
       "      <td>[32, 77, 291, 77, 749, 66, 339, 301, 926, 324,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>5 For I , the Lord , have decreed in mine ange...</td>\n",
       "      <td>5 denn ich , der Herr , habe in meinem Zorn vi...</td>\n",
       "      <td>[20, 355, 275, 350, 263, 277, 399, 715, 263, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>6 Nevertheless , all flesh is in mine hand , a...</td>\n",
       "      <td>6 Doch ist alles Fleisch in meiner Hand , und ...</td>\n",
       "      <td>[21, 452, 68, 434, 475, 335, 396, 263, 520, 28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>7 Wherefore , it is expedient that my servant ...</td>\n",
       "      <td>7 Darum ist es ratsam , da mein Knecht Sidney...</td>\n",
       "      <td>[22, 388, 71, 393, 494, 68, 263, 441, 326, 487...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>9 But now , verily I say , it behooveth me tha...</td>\n",
       "      <td>9 aber jetzt , wahrlich , ich sage : Mir ersch...</td>\n",
       "      <td>[24, 334, 336, 306, 410, 263, 429, 340, 88, 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>10 And inasmuch as they are a faithful they sh...</td>\n",
       "      <td>10 und insofern sie treu sind , werden sie bew...</td>\n",
       "      <td>[16, 15, 530, 67, 283, 293, 76, 417, 447, 949,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               english  \\\n",
       "0    iron cement is a ready for use paste which is ...   \n",
       "1    iron cement protects the ingot against the hot...   \n",
       "2    a fire restant repair cement for fire places ,...   \n",
       "3        Construction and repair of highways and ...\\n   \n",
       "4     An announcement must be commercial character .\\n   \n",
       "..                                                 ...   \n",
       "995  5 For I , the Lord , have decreed in mine ange...   \n",
       "996  6 Nevertheless , all flesh is in mine hand , a...   \n",
       "997  7 Wherefore , it is expedient that my servant ...   \n",
       "998  9 But now , verily I say , it behooveth me tha...   \n",
       "999  10 And inasmuch as they are a faithful they sh...   \n",
       "\n",
       "                                                german  \\\n",
       "0    iron cement ist eine gebrauchs ##AT##-##AT## f...   \n",
       "1    Nach der Aushrtung schtzt iron cement die Ko...   \n",
       "2    feuerfester Reparaturkitt fr Feuerungsanlagen...   \n",
       "3      Der Bau und die Reparatur der Autostraen ...\\n   \n",
       "4    die Mitteilungen sollen den geschftlichen kom...   \n",
       "..                                                 ...   \n",
       "995  5 denn ich , der Herr , habe in meinem Zorn vi...   \n",
       "996  6 Doch ist alles Fleisch in meiner Hand , und ...   \n",
       "997  7 Darum ist es ratsam , da mein Knecht Sidney...   \n",
       "998  9 aber jetzt , wahrlich , ich sage : Mir ersch...   \n",
       "999  10 und insofern sie treu sind , werden sie bew...   \n",
       "\n",
       "                                           english_ids  \n",
       "0    [330, 265, 296, 339, 301, 326, 259, 353, 445, ...  \n",
       "1    [330, 265, 296, 339, 301, 406, 288, 364, 82, 2...  \n",
       "2    [64, 285, 651, 409, 397, 490, 353, 79, 64, 330...  \n",
       "3    [34, 265, 323, 722, 682, 312, 353, 79, 64, 330...  \n",
       "4    [32, 77, 291, 77, 749, 66, 339, 301, 926, 324,...  \n",
       "..                                                 ...  \n",
       "995  [20, 355, 275, 350, 263, 277, 399, 715, 263, 5...  \n",
       "996  [21, 452, 68, 434, 475, 335, 396, 263, 520, 28...  \n",
       "997  [22, 388, 71, 393, 494, 68, 263, 441, 326, 487...  \n",
       "998  [24, 334, 336, 306, 410, 263, 429, 340, 88, 35...  \n",
       "999  [16, 15, 530, 67, 283, 293, 76, 417, 447, 949,...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vs/anaconda3/envs/keras/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "_index = _df_train.english_ids.apply(lambda ids: len(ids) > 100)\n",
    "test = _df_train[_index]\n",
    "test.english_ids = test.english_ids.apply(lambda ids: [0]*(128-len(ids)) + ids)\n",
    "test.english_ids = test.english_ids.apply(lambda ids: ids[:128])\n",
    "test_np = np.array(test.english_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Each lesson teaches</td>\n",
       "      <td>the tough keys ...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The English Pro Dict</td>\n",
       "      <td>ss professional ...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>The English Pro Dict</td>\n",
       "      <td>ss professional ...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>The dictionary works</td>\n",
       "      <td>ons and phrases ...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Today , QuarkXPress</td>\n",
       "      <td>obe Dreamweaver  .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>1  2 , How men may</td>\n",
       "      <td>el shall be saved .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>2 I am Jesus Christ</td>\n",
       "      <td>hat we may be one .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>14 And their arm sha</td>\n",
       "      <td>l I preserve them .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>18 And I have given</td>\n",
       "      <td>lant in his stead .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1  12 , The Lord ha</td>\n",
       "      <td>of the Son of Man .\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  english                english\n",
       "11   Each lesson teaches    the tough keys ...\\n\n",
       "29   The English Pro Dict  ss professional ...\\n\n",
       "39   The English Pro Dict  ss professional ...\\n\n",
       "45   The dictionary works  ons and phrases ...\\n\n",
       "50   Today , QuarkXPress   obe Dreamweaver  .\\n\n",
       "..                    ...                    ...\n",
       "965  1  2 , How men may   el shall be saved .\\n\n",
       "967  2 I am Jesus Christ   hat we may be one .\\n\n",
       "978  14 And their arm sha  l I preserve them .\\n\n",
       "980  18 And I have given   lant in his stead .\\n\n",
       "992  1  12 , The Lord ha  of the Son of Man .\\n\n",
       "\n",
       "[112 rows x 2 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([test.english.apply(lambda text: text[:20]), test.english.apply(lambda text: text[-20:])], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1024)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/wma-en-de/tokenizer/en-de-v0-t1024.tok-vocab.json') as file:\n",
    "    token_to_index = json.load(file)\n",
    "    index_to_token = {v: k for k, v in token_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " 'A': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'D': 35,\n",
       " 'E': 36,\n",
       " 'F': 37,\n",
       " 'G': 38,\n",
       " 'H': 39,\n",
       " 'I': 40,\n",
       " 'J': 41,\n",
       " 'K': 42,\n",
       " 'L': 43,\n",
       " 'M': 44,\n",
       " 'N': 45,\n",
       " 'O': 46,\n",
       " 'P': 47,\n",
       " 'Q': 48,\n",
       " 'R': 49,\n",
       " 'S': 50,\n",
       " 'T': 51,\n",
       " 'U': 52,\n",
       " 'V': 53,\n",
       " 'W': 54,\n",
       " 'X': 55,\n",
       " 'Y': 56,\n",
       " 'Z': 57,\n",
       " '[': 58,\n",
       " '\\\\': 59,\n",
       " ']': 60,\n",
       " '^': 61,\n",
       " '_': 62,\n",
       " '`': 63,\n",
       " 'a': 64,\n",
       " 'b': 65,\n",
       " 'c': 66,\n",
       " 'd': 67,\n",
       " 'e': 68,\n",
       " 'f': 69,\n",
       " 'g': 70,\n",
       " 'h': 71,\n",
       " 'i': 72,\n",
       " 'j': 73,\n",
       " 'k': 74,\n",
       " 'l': 75,\n",
       " 'm': 76,\n",
       " 'n': 77,\n",
       " 'o': 78,\n",
       " 'p': 79,\n",
       " 'q': 80,\n",
       " 'r': 81,\n",
       " 's': 82,\n",
       " 't': 83,\n",
       " 'u': 84,\n",
       " 'v': 85,\n",
       " 'w': 86,\n",
       " 'x': 87,\n",
       " 'y': 88,\n",
       " 'z': 89,\n",
       " '{': 90,\n",
       " '|': 91,\n",
       " '}': 92,\n",
       " '~': 93,\n",
       " '': 94,\n",
       " '': 95,\n",
       " '': 96,\n",
       " '': 97,\n",
       " '': 98,\n",
       " '': 99,\n",
       " '': 100,\n",
       " '': 101,\n",
       " '': 102,\n",
       " '': 103,\n",
       " '': 104,\n",
       " '': 105,\n",
       " '': 106,\n",
       " '': 107,\n",
       " '': 108,\n",
       " '': 109,\n",
       " '': 110,\n",
       " '': 111,\n",
       " '': 112,\n",
       " '': 113,\n",
       " '': 114,\n",
       " '': 115,\n",
       " '': 116,\n",
       " '': 117,\n",
       " '': 118,\n",
       " '': 119,\n",
       " '': 120,\n",
       " '': 121,\n",
       " '': 122,\n",
       " '': 123,\n",
       " '': 124,\n",
       " '': 125,\n",
       " '': 126,\n",
       " '': 127,\n",
       " '': 128,\n",
       " '': 129,\n",
       " '': 130,\n",
       " '': 131,\n",
       " '': 132,\n",
       " '': 133,\n",
       " '': 134,\n",
       " '': 135,\n",
       " '': 136,\n",
       " '': 137,\n",
       " '': 138,\n",
       " '': 139,\n",
       " '': 140,\n",
       " '': 141,\n",
       " '': 142,\n",
       " '': 143,\n",
       " '': 144,\n",
       " '': 145,\n",
       " '': 146,\n",
       " '': 147,\n",
       " '': 148,\n",
       " '': 149,\n",
       " '': 150,\n",
       " '': 151,\n",
       " '': 152,\n",
       " '': 153,\n",
       " '': 154,\n",
       " '': 155,\n",
       " '': 156,\n",
       " '': 157,\n",
       " '': 158,\n",
       " '': 159,\n",
       " '': 160,\n",
       " '': 161,\n",
       " '': 162,\n",
       " '': 163,\n",
       " '': 164,\n",
       " '': 165,\n",
       " '': 166,\n",
       " '': 167,\n",
       " '': 168,\n",
       " '': 169,\n",
       " '': 170,\n",
       " '': 171,\n",
       " '': 172,\n",
       " '': 173,\n",
       " '': 174,\n",
       " '': 175,\n",
       " '': 176,\n",
       " '': 177,\n",
       " '': 178,\n",
       " '': 179,\n",
       " '': 180,\n",
       " '': 181,\n",
       " '': 182,\n",
       " '': 183,\n",
       " '': 184,\n",
       " '': 185,\n",
       " '': 186,\n",
       " '': 187,\n",
       " '': 188,\n",
       " '': 189,\n",
       " '': 190,\n",
       " '': 191,\n",
       " '': 192,\n",
       " '': 193,\n",
       " '': 194,\n",
       " '': 195,\n",
       " '': 196,\n",
       " '': 197,\n",
       " '': 198,\n",
       " '': 199,\n",
       " '': 200,\n",
       " '': 201,\n",
       " '': 202,\n",
       " '': 203,\n",
       " '': 204,\n",
       " '': 205,\n",
       " '': 206,\n",
       " '': 207,\n",
       " '': 208,\n",
       " '': 209,\n",
       " '': 210,\n",
       " '': 211,\n",
       " '': 212,\n",
       " '': 213,\n",
       " '': 214,\n",
       " '': 215,\n",
       " '': 216,\n",
       " '': 217,\n",
       " '': 218,\n",
       " '': 219,\n",
       " '': 220,\n",
       " '': 221,\n",
       " '': 222,\n",
       " '': 223,\n",
       " '': 224,\n",
       " '': 225,\n",
       " '': 226,\n",
       " '': 227,\n",
       " '': 228,\n",
       " '': 229,\n",
       " '': 230,\n",
       " '': 231,\n",
       " '': 232,\n",
       " '': 233,\n",
       " '': 234,\n",
       " '': 235,\n",
       " '': 236,\n",
       " '': 237,\n",
       " '': 238,\n",
       " '': 239,\n",
       " '': 240,\n",
       " '': 241,\n",
       " '': 242,\n",
       " '': 243,\n",
       " '': 244,\n",
       " '': 245,\n",
       " '': 246,\n",
       " '': 247,\n",
       " '': 248,\n",
       " '': 249,\n",
       " '': 250,\n",
       " '': 251,\n",
       " '': 252,\n",
       " '': 253,\n",
       " '': 254,\n",
       " '': 255,\n",
       " 'en': 256,\n",
       " 'er': 257,\n",
       " 'in': 258,\n",
       " 'a': 259,\n",
       " 't': 260,\n",
       " 'd': 261,\n",
       " 'ch': 262,\n",
       " ',': 263,\n",
       " 'es': 264,\n",
       " 'on': 265,\n",
       " 'th': 266,\n",
       " 'un': 267,\n",
       " 's': 268,\n",
       " 'it': 269,\n",
       " 'is': 270,\n",
       " '.': 271,\n",
       " 'w': 272,\n",
       " 'at': 273,\n",
       " 'ie': 274,\n",
       " 'or': 275,\n",
       " 'an': 276,\n",
       " 'the': 277,\n",
       " '##': 278,\n",
       " 're': 279,\n",
       " 'b': 280,\n",
       " 'o': 281,\n",
       " 'al': 282,\n",
       " 'in': 283,\n",
       " 'ar': 284,\n",
       " 'f': 285,\n",
       " 'm': 286,\n",
       " 'ich': 287,\n",
       " 'te': 288,\n",
       " 'ion': 289,\n",
       " 'e': 290,\n",
       " 'an': 291,\n",
       " 'ur': 292,\n",
       " 'as': 293,\n",
       " 'le': 294,\n",
       " 'p': 295,\n",
       " 'c': 296,\n",
       " 'un': 297,\n",
       " 'h': 298,\n",
       " 'us': 299,\n",
       " 'ge': 300,\n",
       " 'ent': 301,\n",
       " 'S': 302,\n",
       " 'of': 303,\n",
       " 'ro': 304,\n",
       " '': 305,\n",
       " 'n': 306,\n",
       " 'v': 307,\n",
       " 'AT': 308,\n",
       " 'ing': 309,\n",
       " 'ung': 310,\n",
       " 'to': 311,\n",
       " 'and': 312,\n",
       " '': 313,\n",
       " 'll': 314,\n",
       " 'die': 315,\n",
       " 'ed': 316,\n",
       " 'om': 317,\n",
       " 'der': 318,\n",
       " 'und': 319,\n",
       " 'A': 320,\n",
       " 'ig': 321,\n",
       " 'ic': 322,\n",
       " 'st': 323,\n",
       " 'be': 324,\n",
       " 'E': 325,\n",
       " 'is': 326,\n",
       " 'ein': 327,\n",
       " 'P': 328,\n",
       " 'z': 329,\n",
       " 'ir': 330,\n",
       " 'M': 331,\n",
       " 'g': 332,\n",
       " 'im': 333,\n",
       " 'B': 334,\n",
       " 'el': 335,\n",
       " 'ut': 336,\n",
       " 'ra': 337,\n",
       " 'ter': 338,\n",
       " 'em': 339,\n",
       " 'il': 340,\n",
       " 'ten': 341,\n",
       " 'ot': 342,\n",
       " 'ation': 343,\n",
       " 'ol': 344,\n",
       " 'eit': 345,\n",
       " 'l': 346,\n",
       " 'der': 347,\n",
       " 'et': 348,\n",
       " 'am': 349,\n",
       " 'I': 350,\n",
       " 'den': 351,\n",
       " '##': 352,\n",
       " 're': 353,\n",
       " 'os': 354,\n",
       " 'F': 355,\n",
       " '-##': 356,\n",
       " '##-##': 357,\n",
       " 'G': 358,\n",
       " 'ies': 359,\n",
       " 'hr': 360,\n",
       " 'um': 361,\n",
       " 'K': 362,\n",
       " 'op': 363,\n",
       " 'ct': 364,\n",
       " 'k': 365,\n",
       " 'sch': 366,\n",
       " 'uf': 367,\n",
       " 'ul': 368,\n",
       " '': 369,\n",
       " 'R': 370,\n",
       " 'ber': 371,\n",
       " 'iv': 372,\n",
       " 'C': 373,\n",
       " 'lich': 374,\n",
       " 'de': 375,\n",
       " '&': 376,\n",
       " 'T': 377,\n",
       " 'for': 378,\n",
       " 'D': 379,\n",
       " 'ab': 380,\n",
       " 'H': 381,\n",
       " 'V': 382,\n",
       " 'r': 383,\n",
       " 'isch': 384,\n",
       " 'icht': 385,\n",
       " 'ier': 386,\n",
       " 'gen': 387,\n",
       " 'W': 388,\n",
       " 'on': 389,\n",
       " 'qu': 390,\n",
       " 'das': 391,\n",
       " 'zu': 392,\n",
       " 'ere': 393,\n",
       " 'den': 394,\n",
       " 'ly': 395,\n",
       " 'ess': 396,\n",
       " 'est': 397,\n",
       " 'that': 398,\n",
       " 'L': 399,\n",
       " 'and': 400,\n",
       " 'he': 401,\n",
       " 'id': 402,\n",
       " 'omm': 403,\n",
       " 'ce': 404,\n",
       " 'ers': 405,\n",
       " 'pro': 406,\n",
       " 'ein': 407,\n",
       " 'al': 408,\n",
       " 'r': 409,\n",
       " 'ow': 410,\n",
       " 'wh': 411,\n",
       " 'ach': 412,\n",
       " 'von': 413,\n",
       " 'ge': 414,\n",
       " 'ort': 415,\n",
       " 'st': 416,\n",
       " 'uch': 417,\n",
       " 'im': 418,\n",
       " '': 419,\n",
       " 'y': 420,\n",
       " 'U': 421,\n",
       " 'ith': 422,\n",
       " 'ap': 423,\n",
       " 'wir': 424,\n",
       " 'kt': 425,\n",
       " 'con': 426,\n",
       " 'if': 427,\n",
       " 'ist': 428,\n",
       " 'ver': 429,\n",
       " 'er': 430,\n",
       " 'with': 431,\n",
       " 'we': 432,\n",
       " 'auf': 433,\n",
       " 'ver': 434,\n",
       " 'fr': 435,\n",
       " 'urop': 436,\n",
       " 'ay': 437,\n",
       " '1': 438,\n",
       " 'ill': 439,\n",
       " 'des': 440,\n",
       " 'it': 441,\n",
       " 'mit': 442,\n",
       " 'are': 443,\n",
       " 've': 444,\n",
       " 'ad': 445,\n",
       " 'ts': 446,\n",
       " 'as': 447,\n",
       " 'ind': 448,\n",
       " 'de': 449,\n",
       " 'so': 450,\n",
       " 'ck': 451,\n",
       " 'N': 452,\n",
       " 'ne': 453,\n",
       " '': 454,\n",
       " 'yo': 455,\n",
       " 'iss': 456,\n",
       " 'St': 457,\n",
       " ')': 458,\n",
       " 'dies': 459,\n",
       " '2': 460,\n",
       " 'end': 461,\n",
       " '': 462,\n",
       " 'af': 463,\n",
       " 'Europ': 464,\n",
       " 'Z': 465,\n",
       " 'ate': 466,\n",
       " 'ment': 467,\n",
       " 'ik': 468,\n",
       " 'se': 469,\n",
       " 'up': 470,\n",
       " '(': 471,\n",
       " 'ist': 472,\n",
       " 'ungen': 473,\n",
       " 'The': 474,\n",
       " 'th': 475,\n",
       " 'nen': 476,\n",
       " 'ischen': 477,\n",
       " 'la': 478,\n",
       " '00': 479,\n",
       " 'men': 480,\n",
       " 'ib': 481,\n",
       " 'als': 482,\n",
       " 'Ver': 483,\n",
       " 'ust': 484,\n",
       " 'eine': 485,\n",
       " 'In': 486,\n",
       " 'ex': 487,\n",
       " 'ak': 488,\n",
       " 'pe': 489,\n",
       " 'ant': 490,\n",
       " 'dem': 491,\n",
       " 'this': 492,\n",
       " 'ne': 493,\n",
       " 'for': 494,\n",
       " 'ity': 495,\n",
       " 'or': 496,\n",
       " 'ive': 497,\n",
       " 'ien': 498,\n",
       " 'j': 499,\n",
       " 'ah': 500,\n",
       " 'ial': 501,\n",
       " 'nicht': 502,\n",
       " ':': 503,\n",
       " 'co': 504,\n",
       " 'wer': 505,\n",
       " 'art': 506,\n",
       " 'quot': 507,\n",
       " 'ug': 508,\n",
       " 'not': 509,\n",
       " 'und': 510,\n",
       " 'ha': 511,\n",
       " 'Un': 512,\n",
       " 'ann': 513,\n",
       " 'port': 514,\n",
       " 'ber': 515,\n",
       " 'ste': 516,\n",
       " 'le': 517,\n",
       " 'apos': 518,\n",
       " 'sich': 519,\n",
       " 'all': 520,\n",
       " 'n': 521,\n",
       " 'ub': 522,\n",
       " '-': 523,\n",
       " 'werden': 524,\n",
       " 'tel': 525,\n",
       " 'J': 526,\n",
       " 'ff': 527,\n",
       " 'Re': 528,\n",
       " 'by': 529,\n",
       " 'An': 530,\n",
       " 'Sie': 531,\n",
       " 'we': 532,\n",
       " 'have': 533,\n",
       " 'dass': 534,\n",
       " 'at': 535,\n",
       " 'rom': 536,\n",
       " 'igen': 537,\n",
       " 'ag': 538,\n",
       " 'es': 539,\n",
       " 'igh': 540,\n",
       " 'ass': 541,\n",
       " 'ens': 542,\n",
       " 'aus': 543,\n",
       " 'ain': 544,\n",
       " 'you': 545,\n",
       " 'uld': 546,\n",
       " 'all': 547,\n",
       " 'will': 548,\n",
       " 'com': 549,\n",
       " 'ab': 550,\n",
       " 'uns': 551,\n",
       " 'od': 552,\n",
       " 'iel': 553,\n",
       " 'ern': 554,\n",
       " 'O': 555,\n",
       " 'ber': 556,\n",
       " 'i': 557,\n",
       " 'ions': 558,\n",
       " 'ungs': 559,\n",
       " 'aus': 560,\n",
       " 'sh': 561,\n",
       " 'ure': 562,\n",
       " 'me': 563,\n",
       " 'ze': 564,\n",
       " 'ommiss': 565,\n",
       " 'which': 566,\n",
       " 'age': 567,\n",
       " 'ted': 568,\n",
       " 'was': 569,\n",
       " 'Pr': 570,\n",
       " 'auch': 571,\n",
       " 'ran': 572,\n",
       " 'ben': 573,\n",
       " 'lie': 574,\n",
       " 'hen': 575,\n",
       " 'from': 576,\n",
       " 'Europe': 577,\n",
       " 'ahr': 578,\n",
       " 'Sch': 579,\n",
       " 'Die': 580,\n",
       " 'hal': 581,\n",
       " 'du': 582,\n",
       " 'och': 583,\n",
       " 'ip': 584,\n",
       " 'um': 585,\n",
       " 'ard': 586,\n",
       " 'Pro': 587,\n",
       " 'us': 588,\n",
       " 'ould': 589,\n",
       " 'sind': 590,\n",
       " 'uss': 591,\n",
       " 'ch': 592,\n",
       " 'tern': 593,\n",
       " 'pr': 594,\n",
       " 'sch': 595,\n",
       " 'pr': 596,\n",
       " 'tr': 597,\n",
       " 'ident': 598,\n",
       " 'ommission': 599,\n",
       " 'ent': 600,\n",
       " 'og': 601,\n",
       " 'vor': 602,\n",
       " 'fer': 603,\n",
       " 'wird': 604,\n",
       " 'has': 605,\n",
       " 'esch': 606,\n",
       " 'hab': 607,\n",
       " 'ue': 608,\n",
       " 'pl': 609,\n",
       " 'mer': 610,\n",
       " 'schaf': 611,\n",
       " 'ft': 612,\n",
       " 'iz': 613,\n",
       " 'en': 614,\n",
       " 'En': 615,\n",
       " 'wor': 616,\n",
       " 'ss': 617,\n",
       " 'n': 618,\n",
       " 'ine': 619,\n",
       " 'res': 620,\n",
       " 'In': 621,\n",
       " 'Th': 622,\n",
       " 'gr': 623,\n",
       " 'ud': 624,\n",
       " 'llen': 625,\n",
       " 'ang': 626,\n",
       " 'oc': 627,\n",
       " 'ous': 628,\n",
       " 'Be': 629,\n",
       " 'ick': 630,\n",
       " 'ische': 631,\n",
       " 'wie': 632,\n",
       " 'ok': 633,\n",
       " 'da': 634,\n",
       " 'enn': 635,\n",
       " 'ast': 636,\n",
       " 'Par': 637,\n",
       " 't': 638,\n",
       " 'he': 639,\n",
       " 'can': 640,\n",
       " 'man': 641,\n",
       " 'our': 642,\n",
       " 'urch': 643,\n",
       " 'am': 644,\n",
       " 'able': 645,\n",
       " 'etz': 646,\n",
       " 'err': 647,\n",
       " 'ise': 648,\n",
       " 'me': 649,\n",
       " 'ige': 650,\n",
       " 'ire': 651,\n",
       " 'form': 652,\n",
       " 'keit': 653,\n",
       " 'hat': 654,\n",
       " 'ich': 655,\n",
       " 'sein': 656,\n",
       " 'oder': 657,\n",
       " 'Ar': 658,\n",
       " 'our': 659,\n",
       " 'ost': 660,\n",
       " 'otel': 661,\n",
       " 'ament': 662,\n",
       " 'ert': 663,\n",
       " 'ore': 664,\n",
       " 'erv': 665,\n",
       " 'cc': 666,\n",
       " 'ia': 667,\n",
       " 'ations': 668,\n",
       " 'zur': 669,\n",
       " 'European': 670,\n",
       " 'also': 671,\n",
       " 'Er': 672,\n",
       " 'ult': 673,\n",
       " 'be': 674,\n",
       " 'beit': 675,\n",
       " 'haben': 676,\n",
       " 'sie': 677,\n",
       " 'comp': 678,\n",
       " '200': 679,\n",
       " 'ad': 680,\n",
       " 'ational': 681,\n",
       " 'ction': 682,\n",
       " 'heit': 683,\n",
       " 'ical': 684,\n",
       " 'her': 685,\n",
       " '/': 686,\n",
       " 'ese': 687,\n",
       " 'ition': 688,\n",
       " 'lichen': 689,\n",
       " 'ice': 690,\n",
       " 'ated': 691,\n",
       " 'are': 692,\n",
       " 'bei': 693,\n",
       " 'do': 694,\n",
       " 'dern': 695,\n",
       " 'kn': 696,\n",
       " 'out': 697,\n",
       " 'einer': 698,\n",
       " '!': 699,\n",
       " 'ome': 700,\n",
       " 'rit': 701,\n",
       " 'olit': 702,\n",
       " '?': 703,\n",
       " 'aten': 704,\n",
       " '': 705,\n",
       " 'tz': 706,\n",
       " 'hl': 707,\n",
       " 'ther': 708,\n",
       " 'op': 709,\n",
       " 'ark': 710,\n",
       " 'f': 711,\n",
       " 'pro': 712,\n",
       " 'ect': 713,\n",
       " 'ieren': 714,\n",
       " 'ord': 715,\n",
       " 'unter': 716,\n",
       " 'se': 717,\n",
       " 'ap': 718,\n",
       " 'Mit': 719,\n",
       " 'Ge': 720,\n",
       " 'ish': 721,\n",
       " 'ru': 722,\n",
       " 'nach': 723,\n",
       " 'n': 724,\n",
       " 'qu': 725,\n",
       " 'einen': 726,\n",
       " 'part': 727,\n",
       " 'Parl': 728,\n",
       " 'Aus': 729,\n",
       " 'your': 730,\n",
       " 'tet': 731,\n",
       " 'schaft': 732,\n",
       " 'ac': 733,\n",
       " 'iert': 734,\n",
       " 'ms': 735,\n",
       " 'ht': 736,\n",
       " 'ign': 737,\n",
       " 'r': 738,\n",
       " 'te': 739,\n",
       " 'elt': 740,\n",
       " 'ight': 741,\n",
       " 'Union': 742,\n",
       " 'its': 743,\n",
       " 'vel': 744,\n",
       " 'ates': 745,\n",
       " '3': 746,\n",
       " 'ake': 747,\n",
       " 'ob': 748,\n",
       " 'oun': 749,\n",
       " 'reg': 750,\n",
       " 'tra': 751,\n",
       " 'gt': 752,\n",
       " 'gro': 753,\n",
       " '&#': 754,\n",
       " 'ite': 755,\n",
       " 'durch': 756,\n",
       " 'ro': 757,\n",
       " 'ank': 758,\n",
       " 'au': 759,\n",
       " 'ger': 760,\n",
       " 'go': 761,\n",
       " 'ian': 762,\n",
       " 'Ber': 763,\n",
       " 'yste': 764,\n",
       " 'but': 765,\n",
       " 'fen': 766,\n",
       " 'wur': 767,\n",
       " 'ember': 768,\n",
       " 'ag': 769,\n",
       " 'knnen': 770,\n",
       " 'Fra': 771,\n",
       " 'there': 772,\n",
       " 'ary': 773,\n",
       " 'ide': 774,\n",
       " 'cial': 775,\n",
       " 'sp': 776,\n",
       " 'oll': 777,\n",
       " 'liche': 778,\n",
       " 'Ein': 779,\n",
       " 's': 780,\n",
       " 'EU': 781,\n",
       " 'hre': 782,\n",
       " 'tra': 783,\n",
       " 'ste': 784,\n",
       " 'ose': 785,\n",
       " 'dar': 786,\n",
       " 'spe': 787,\n",
       " 'r': 788,\n",
       " '19': 789,\n",
       " 'ov': 790,\n",
       " 'ear': 791,\n",
       " 'anz': 792,\n",
       " 'war': 793,\n",
       " 'cht': 794,\n",
       " 'Vor': 795,\n",
       " 'We': 796,\n",
       " 'Gr': 797,\n",
       " 'Commission': 798,\n",
       " 'ual': 799,\n",
       " 'chen': 800,\n",
       " 'inc': 801,\n",
       " 'Ab': 802,\n",
       " 'sp': 803,\n",
       " 'Das': 804,\n",
       " 'would': 805,\n",
       " 'm': 806,\n",
       " 'ime': 807,\n",
       " 'eren': 808,\n",
       " 'more': 809,\n",
       " 'ance': 810,\n",
       " 'ail': 811,\n",
       " 'zum': 812,\n",
       " 'tw': 813,\n",
       " 'ile': 814,\n",
       " 'their': 815,\n",
       " 'pt': 816,\n",
       " 'einem': 817,\n",
       " 'cont': 818,\n",
       " 'rie': 819,\n",
       " 'gen': 820,\n",
       " 'ommen': 821,\n",
       " 'been': 822,\n",
       " 'bes': 823,\n",
       " 'amp': 824,\n",
       " 'reat': 825,\n",
       " 'con': 826,\n",
       " 'ar': 827,\n",
       " '..': 828,\n",
       " 'u': 829,\n",
       " 'ute': 830,\n",
       " 'glich': 831,\n",
       " 'imm': 832,\n",
       " 'ase': 833,\n",
       " 'Te': 834,\n",
       " 'We': 835,\n",
       " 'chte': 836,\n",
       " 'ame': 837,\n",
       " 'ong': 838,\n",
       " 'Europ': 839,\n",
       " 'Ich': 840,\n",
       " 'act': 841,\n",
       " 'chn': 842,\n",
       " 'one': 843,\n",
       " 'fin': 844,\n",
       " 'utz': 845,\n",
       " 'kann': 846,\n",
       " 'acc': 847,\n",
       " '': 848,\n",
       " 'ild': 849,\n",
       " '': 850,\n",
       " 'dis': 851,\n",
       " 'up': 852,\n",
       " 'ck': 853,\n",
       " 'diese': 854,\n",
       " 'per': 855,\n",
       " 'oss': 856,\n",
       " 'gra': 857,\n",
       " 'nur': 858,\n",
       " 'ss': 859,\n",
       " 'che': 860,\n",
       " 'ack': 861,\n",
       " 'coun': 862,\n",
       " 'ont': 863,\n",
       " 'ble': 864,\n",
       " 'comm': 865,\n",
       " 'ugh': 866,\n",
       " 'ents': 867,\n",
       " 'cl': 868,\n",
       " 'pl': 869,\n",
       " 'one': 870,\n",
       " 'sten': 871,\n",
       " 'should': 872,\n",
       " 'Sp': 873,\n",
       " 'De': 874,\n",
       " 'ges': 875,\n",
       " 'gan': 876,\n",
       " 'alle': 877,\n",
       " 'work': 878,\n",
       " 'bet': 879,\n",
       " 'Le': 880,\n",
       " 'ahl': 881,\n",
       " 'hn': 882,\n",
       " 'new': 883,\n",
       " 'wer': 884,\n",
       " 'ike': 885,\n",
       " 'Hotel': 886,\n",
       " ';': 887,\n",
       " 'Ch': 888,\n",
       " 'ution': 889,\n",
       " 'sup': 890,\n",
       " 'Unter': 891,\n",
       " 'out': 892,\n",
       " 'geb': 893,\n",
       " 'eine': 894,\n",
       " 'This': 895,\n",
       " 'ely': 896,\n",
       " 'Wir': 897,\n",
       " 'hmen': 898,\n",
       " 'pos': 899,\n",
       " 'sident': 900,\n",
       " 'zu': 901,\n",
       " 'Jahr': 902,\n",
       " 'auf': 903,\n",
       " 'ree': 904,\n",
       " 'land': 905,\n",
       " 'dieser': 906,\n",
       " 'hren': 907,\n",
       " 'Pres': 908,\n",
       " 'ob': 909,\n",
       " 'ierung': 910,\n",
       " 'ld': 911,\n",
       " 'It': 912,\n",
       " 'igkeit': 913,\n",
       " 'per': 914,\n",
       " 'jed': 915,\n",
       " 'ner': 916,\n",
       " 'olle': 917,\n",
       " '...': 918,\n",
       " 'countr': 919,\n",
       " 'pre': 920,\n",
       " 'reich': 921,\n",
       " 'ons': 922,\n",
       " 'very': 923,\n",
       " 'av': 924,\n",
       " 'ind': 925,\n",
       " 'must': 926,\n",
       " 'produ': 927,\n",
       " 'inter': 928,\n",
       " 'ings': 929,\n",
       " 'int': 930,\n",
       " 'ma': 931,\n",
       " 'who': 932,\n",
       " 'da': 933,\n",
       " 'noch': 934,\n",
       " 'zw': 935,\n",
       " 'other': 936,\n",
       " 'ev': 937,\n",
       " 'ktion': 938,\n",
       " 'Kommission': 939,\n",
       " '4': 940,\n",
       " 'ities': 941,\n",
       " 'bar': 942,\n",
       " 'formation': 943,\n",
       " 'hr': 944,\n",
       " 'ke': 945,\n",
       " 'pe': 946,\n",
       " 'weit': 947,\n",
       " 'need': 948,\n",
       " 'they': 949,\n",
       " '5': 950,\n",
       " 'gram': 951,\n",
       " 'ink': 952,\n",
       " 'year': 953,\n",
       " 'ces': 954,\n",
       " 'no': 955,\n",
       " 'mit': 956,\n",
       " 'wurde': 957,\n",
       " 'ors': 958,\n",
       " 'ber': 959,\n",
       " 'tell': 960,\n",
       " 'wo': 961,\n",
       " 'ments': 962,\n",
       " 'ater': 963,\n",
       " 'erm': 964,\n",
       " 'like': 965,\n",
       " 'only': 966,\n",
       " 'intern': 967,\n",
       " 'mehr': 968,\n",
       " 'mp': 969,\n",
       " 'soll': 970,\n",
       " 'Auf': 971,\n",
       " 'time': 972,\n",
       " 'fe': 973,\n",
       " 'cons': 974,\n",
       " 'President': 975,\n",
       " 'str': 976,\n",
       " 'schen': 977,\n",
       " 'gel': 978,\n",
       " 'Herr': 979,\n",
       " 'Der': 980,\n",
       " 'aber': 981,\n",
       " 'bec': 982,\n",
       " 'rans': 983,\n",
       " 'Mr': 984,\n",
       " 'Se': 985,\n",
       " 'rat': 986,\n",
       " 'about': 987,\n",
       " 'oin': 988,\n",
       " 'glie': 989,\n",
       " 'olog': 990,\n",
       " 'ystem': 991,\n",
       " 'wenn': 992,\n",
       " 'ssen': 993,\n",
       " 'Prsident': 994,\n",
       " 'polit': 995,\n",
       " 'teil': 996,\n",
       " '': 997,\n",
       " 'ible': 998,\n",
       " 'Es': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Overview",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
